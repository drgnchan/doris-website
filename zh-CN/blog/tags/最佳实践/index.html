<!doctype html>
<html lang="zh-Hans-CN" dir="ltr" class="blog-wrapper blog-tags-post-list-page plugin-blog plugin-id-default">
<head>
<meta charset="UTF-8">
<meta name="viewport" content="width=device-width,initial-scale=1,minimum-scale=1,maximum-scale=1,user-scalable=no">
<meta name="generator" content="Docusaurus v2.4.1">
<link rel="alternate" type="application/rss+xml" href="/zh-CN/blog/rss.xml" title="Apache Doris RSS Feed">
<link rel="alternate" type="application/atom+xml" href="/zh-CN/blog/atom.xml" title="Apache Doris Atom Feed">

<link rel="preconnect" href="https://www.google-analytics.com">
<link rel="preconnect" href="https://www.googletagmanager.com">
<script async src="https://www.googletagmanager.com/gtag/js?id=G-DT7W9E9722"></script>
<script>function gtag(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-DT7W9E9722",{anonymize_ip:!0})</script>






<link rel="icon" href="/zh-CN/images/logo-only.png">
<link rel="manifest" href="/zh-CN/manifest.json">
<meta name="theme-color" content="#FFFFFF">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#000">
<link rel="apple-touch-icon" href="/zh-CN/img/docusaurus.png">
<link rel="mask-icon" href="/zh-CN/img/docusaurus.svg" color="rgb(37, 194, 160)">
<meta name="msapplication-TileImage" content="/zh-CN/img/docusaurus.png">
<meta name="msapplication-TileColor" content="#000">

<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Montserrat:500">
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Noto+Sans+SC:400"><title data-rh="true">19 篇博文 含有标签「最佳实践」 - Apache Doris</title><meta data-rh="true" name="viewport" content="width=device-width,initial-scale=1"><meta data-rh="true" name="twitter:card" content="summary_large_image"><meta data-rh="true" property="og:url" content="https://doris.apache.org/zh-CN/blog/tags/最佳实践"><meta data-rh="true" name="docusaurus_locale" content="zh-CN"><meta data-rh="true" name="docsearch:language" content="zh-CN"><meta data-rh="true" property="og:title" content="19 篇博文 含有标签「最佳实践」 - Apache Doris"><meta data-rh="true" name="docusaurus_tag" content="blog_tags_posts"><meta data-rh="true" name="docsearch:docusaurus_tag" content="blog_tags_posts"><link data-rh="true" rel="icon" href="/zh-CN/images/favicon.ico"><link data-rh="true" rel="canonical" href="https://doris.apache.org/zh-CN/blog/tags/最佳实践"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/最佳实践" hreflang="en-US"><link data-rh="true" rel="alternate" href="https://doris.apache.org/zh-CN/blog/tags/最佳实践" hreflang="zh-Hans-CN"><link data-rh="true" rel="alternate" href="https://doris.apache.org/blog/tags/最佳实践" hreflang="x-default"><link rel="stylesheet" href="https://cdnd.selectdb.com/zh-CN/assets/css/styles.e40c4c70.css">
<link rel="preload" href="https://cdnd.selectdb.com/zh-CN/assets/js/runtime~main.f82a6f6f.js" as="script">
<link rel="preload" href="https://cdnd.selectdb.com/zh-CN/assets/js/main.5ce93bb9.js" as="script">
</head>
<body class="navigation-with-keyboard">
<script>!function(){function t(t){document.documentElement.setAttribute("data-theme",t)}var e=function(){var t=null;try{t=new URLSearchParams(window.location.search).get("docusaurus-theme")}catch(t){}return t}()||function(){var t=null;try{t=localStorage.getItem("theme")}catch(t){}return t}();t(null!==e?e:"light")}(),document.documentElement.setAttribute("data-announcement-bar-initially-dismissed",function(){try{return"true"===localStorage.getItem("docusaurus.announcement.dismiss")}catch(t){}return!1}())</script><div id="__docusaurus">
<div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="announcementBar_s0pr" style="background-color:#3C2FD4;color:#FFFFFF" role="banner"><div class="announcementBarPlaceholder_qxfj"></div><div class="announcementBarContent_dpRF"><a href="https://github.com/apache/doris" target="_blank" style="display: flex; width: 100%; align-items: center; justify-content: center; margin-left: 4px; text-decoration: none; color: white">Do you like Apache Doris？Give us a 🌟 on GitHub 
                        <img style="width: 1.2rem; height: 1.2rem; margin-left: 0.4rem;" src="/images/github-white-icon.svg">
                    </a></div><button type="button" class="clean-btn close announcementBarClose_iXyO" aria-label="关闭"><svg viewBox="0 0 15 15" width="14" height="14"><g stroke="currentColor" stroke-width="3.1"><path d="M.75.75l13.5 13.5M14.25.75L.75 14.25"></path></g></svg></button></div><nav aria-label="主导航" class="navbar navbar--fixed-top"><div class="navbar__inner"><div class="navbar__items"><button aria-label="切换导航栏" aria-expanded="false" class="navbar__toggle clean-btn" type="button"><svg width="30" height="30" viewBox="0 0 30 30" aria-hidden="true"><path stroke="currentColor" stroke-linecap="round" stroke-miterlimit="10" stroke-width="2" d="M4 7h22M4 15h22M4 23h22"></path></svg></button><a class="navbar__brand" href="/zh-CN/"><div class="navbar__logo"><img src="https://cdnd.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--light_HNdA"><img src="https://cdnd.selectdb.com/images/logo.svg" alt="Doris" class="themedImage_ToTc themedImage--dark_i4oU"></div><b class="navbar__title text--truncate"></b></a><a class="navbar__item navbar__link" href="/zh-CN/">首页</a><a class="navbar__item navbar__link" href="/zh-CN/docs/dev/summary/basic-summary">文档</a><a aria-current="page" class="navbar__item navbar__link navbar__link--active" href="/zh-CN/blog">Blogs</a><a class="navbar__item navbar__link" href="/zh-CN/community/team">社区</a><a class="navbar__item navbar__link" href="/zh-CN/users">User Stories</a></div><div class="navbar__items navbar__items--right"><div class="versions">版本<!-- -->:<div class="navbar__item dropdown dropdown--hoverable dropdown--right"><a class="navbar__link" aria-haspopup="true" aria-expanded="false" role="button" href="/zh-CN/docs/dev/get-starting/">dev</a><ul class="dropdown__menu"><li><a class="dropdown__link" href="/zh-CN/docs/dev/get-starting/">dev</a></li><li><a class="dropdown__link" href="/zh-CN/docs/1.2/get-starting/">1.2</a></li></ul></div></div><div class="locale-box"><a href="/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link">EN</a><a href="/zh-CN/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link dropdown__link--active">中文</a></div><a class="navbar__item navbar__link header-right-button-primary navbar-download-mobile" href="/zh-CN/download">下载</a><div class="navbar-search searchBox_ZlJk"><div class="navbar__search searchBarContainer_PzyC"><input placeholder="搜索" aria-label="Search" class="navbar__search-input"><div class="loadingRing__K5d searchBarLoadingRing_e2f0"><div></div><div></div><div></div><div></div></div><div class="searchHintContainer_m7ml"><kbd class="searchHint_zuPL">ctrl</kbd><kbd class="searchHint_zuPL">K</kbd></div></div></div><span class="github-btn desktop header-right-button-github"><span class="github-btn github-btn-large"><a class="gh-btn" href="//github.com/apache/doris/" target="_blank"><span class="gh-ico" aria-hidden="true"></span><span class="gh-text">Star</span></a><a class="gh-count" target="_blank" href="//github.com/apache/doris/stargazers/"></a></span></span><a class="header-right-button-primary navbar-download-desktop" href="/zh-CN/download">下载</a></div></div><div role="presentation" class="navbar-sidebar__backdrop"></div></nav><div id="__docusaurus_skipToContent_fallback" class="main-wrapper mainWrapper_z2l0"><div role="region" aria-label="跳到主要内容"><a class="skipToContent_fXgn" href="#__docusaurus_skipToContent_fallback">跳到主要内容</a></div><div class="main-wrapper"><div class="container margin-vert--lg blog-container"><div class="row"><main class="col col--9 col--offset-1" itemscope="" itemtype="http://schema.org/Blog"><header class="margin-bottom--xl"><h1>19 篇博文 含有标签「最佳实践」</h1><a href="/zh-CN/blog/tags">查看所有标签</a></header><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Moka">查询提速 20 倍，Apache Doris 在 Moka BI  SaaS 服务场景下的应用实践</a></h2><div class="blog-info"><time datetime="2023-07-10T00:00:00.000Z" itemprop="datePublished">2023年7月10日</time><span class="split-line"></span><span class="authors"><span class="s-author">张宝铭</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong> MOKA 主要有两大业务线 MOKA 招聘（智能化招聘管理系统）和 MOKA People（智能化人力资源管理系统），MOKA BI 通过全方位数据统计和可灵活配置的实时报表，赋能于智能化招聘管理系统和人力资源管理系统。为了提供更完备的数据支持，助力企业提升招聘竞争力，MOKA 引入性能强悍的 <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a> 对早期架构进行升级转型，成就了 Moka BI 强大的性能与优秀的用户体验。</p><p>作者<strong>｜</strong>Moka 数据架构师 张宝铭</p><h1>业务需求</h1><p>MOKA 主要有两大业务线 MOKA 招聘（智能化招聘管理系统）和 MOKA People（智能化人力资源管理系统）。</p><ul><li>MOKA 招聘系统覆盖社招、校招、内推、猎头管理等场景，让 HR 获得更高效的招聘体验，更便捷的协作体验，让管理者获得招聘数据洞见，让招聘降本增效的同时，树立企业在候选人心目中的专业形象。</li><li>MOKA People 覆盖企业所需要的组织人事、假期考勤、薪酬、绩效、审批等高频业务场景，打通从招聘到人力资源管理的全流程，为 HR 工作提效赋能。通过多维度数据洞见，助力管理者高效科学决策。全生态对接，更加注重全员体验，是一款工作体验更愉悦的人力资源管理系统。</li></ul><p>而 MOKA BI 通过全方位数据统计和可灵活配置的实时报表，赋能于智能化招聘管理系统和人力资源管理系统。通过 PC 端和移动端的多样化报表展示，为企业改善招聘业务提供数据支持，全面提升招聘竞争力，从而助力科学决策。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ca6bfd5e11ea4e9a92d6012601ee549c~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h1>MOKA BI 早期架构</h1><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a9785a9867a3432786ffe0a866c940e7~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>Moka BI 数仓早期架构是类 Lambda 架构，实时处理和离线处理并存。</p><ul><li>实时部分数据主要来源为结构化的数据，Canal 采集 MySQL 或 DBLE（基于 MySQL 的分布式中间件）的 Binlog 输出至 Kafka 中；未建模的数据按照公司分库，存储在业务 DBLE 中，通过 Flink 进行实时建模，将计算后的数据实时写入业务 DBLE 库，通过 DBLE 提供报表查询能力，支持数据大屏和实时报表统计。</li><li>离线部分涵盖了实时部分数据，其结构化数据来源于 DBLE 的 Binlog，明细数据在 Hbase 中实时更新，并映射成 Hive 表，非结构化数据通过 ETL 流程，存储至 Hive 中，通过 Spark 进行进行离线部分建模计算，离线数仓 ADS 层数据输出至 MySQL 和 Redis 支持离线报表统计，明细数据又为指标预测和搜索等外部应用提供数据支持。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="现状与问题"><strong>现状与问题</strong><a href="#现状与问题" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在早期数仓架构中，为了实现实时建模以及实时报表查询功能，就必须要求底层数据库能够承载业务数据的频繁插入、更新及删除操作，并要求支持标准 SQL，因此当时我们选择 DBLE 作为数据存储、建模、查询的底层库。早期 Moka BI 灰度期用户较少，业务数据量以及报表的使用量都比较低，DBLE 尚能满足业务需求，但随着 Moka BI 逐渐面向所有用户开放，DBLE 逐渐无法适应 BI 报表的查询分析性能要求，同时实时与离线架构分离、存储成本高且数据不易维护，亟需进行升级转型。</p><h1>技术选型</h1><p>为匹配业务飞速增长的要求、满足更复杂的查询需求，我们决定引入一款性能突出的 OLAP 引擎对 Moka BI 进行升级改造。同时出于多样化分析场景的考虑，我们希望其能够支撑更广泛的应用场景。调研的主要方向包括 报表的实时查询能力、数据的更新能力、标准的查询 SQL 以及数据库的可维护性、扩展性、稳定性等。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7690c66e83594d178ba224b924d1cddf~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>确定调研方向后，我们首先对 Greenplum 展开了调研，其特点主要是数据加载和批量 DML 处理快，但受限于主从双层架构设计、存在性能瓶颈，且并发能力很有限、性能随着并发量增加而快速下降，同时其使用的是 PG 语法、不支持 MySQL 语法，在进行引擎切换时成本较高，因此在基本功能调研结束后便不再考虑使用。</p><p>随后我们对 ClickHouse 进行了调研，ClickHouse 在单表查询场景下性能表现非常优异的，但是在多表 Join 场景中性能表现不尽如人意，另外 ClickHouse 缺少数据实时更新和删除的能力，仅适用于批量删除或修改数据，同时 ClickHouse 对 SQL 的支持也比较有限，使用起来需要一定的学习成本。</p><p>紧接着我们对近几年势如破竹的 Apache Doris 进行了调研，在调研中发现，Doris 支持实时导入，同时也支持数据的实时更新与删除，可以实现 Exactly-Once 语义；其次，在实时查询方面，Doris 可以实现秒级查询，且在多表 Join 能力的支持上更加强劲；除此之外，Doris 简单易用，部署只需两个进程，不依赖其他系统，兼容 MySQL 协议，并且使用标准 SQL ，可快速上手，部署及学习成本投入均比较低。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="benchmark"><strong>Benchmark</strong><a href="#benchmark" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在初步调研的基础之上，我们进一步将 Apache Doris 、Clickhouse 与当下使用的 DBLE 在查询性能上进行了多轮测试对比，查询耗时如下：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f3c3bf23f8874ae4af1a9fea364bc06a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><ul><li><strong>多表 Join</strong>：随着 SQL Join 数量的增多，Doris 和 ClickHouse 性能表现差距越来越大，Doris 的查询延迟相对比较稳定，最长耗时仅为 3.2s；而 ClickHouse 的查询延迟呈现指数增长，最长耗时甚至达到 17.8s，二者性能最高相差 5 倍，DBLE 的查询性能则远不如这两款产品。</li><li><strong>慢查询：</strong> 在线上慢查询 SQL 的对比测试中，Doris 的性能同样非常稳定，不同的 SQL 查询基本都能在 1s 内返回查询结果，ClickHouse 与之对比查询延迟波动较大、性能表现很不稳定，二者相同 SQL 性能差距最大超过 10 余倍。</li></ul><p><strong>通过以上调研对比，可以看出 Apache Doris 不管是在基本功能上、还是查询性能上表现都更胜一筹，因此我们将目标锁定了 Doris，并决定尽快引入 Apache Doris 作为 Moka</strong> <strong>BI</strong> <strong>新一代</strong> <strong>数仓</strong> <strong>架构的查询引擎。</strong></p><h1>新版架构</h1><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/20a0be39810942a78bb1d66018744c84~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>在引入 Doris 之后，Moka BI 数仓架构的主要变化是将 OLAP 和 OLTP 进行分离，即使用 DBLE 支持数据的实时建模，数据来源于 Moka 系统的业务数据，包含了结构化和半结构化的数据，通过 Flink 读取 DBLE Binlog，完成数据去重、合并后写入 Kafka，Doris 通过 Routine Load 读取 Kafka 完成数据写入，此时 DBLE 仅作为数据建模合成使用，由 Doris 提供报表查询能力。</p><p>基于 Doris 列存储、高并发、高性能等特性，Moka BI 报表采用自助方式构建完成，支撑客户根据需求灵活配置行、列、筛选的场景。与传统报表按需求定制开发方式对比，这种自助式报表构建非常灵活，平台开发与需求开发完全独立，需求完成速度得到极大的提升。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5de38af04e544c0ca59e43dfe9c7f015~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>数据导入方面，数据通过 Routine Load 定期批量导入到 Doris 数据仓库中，保证了数据的准实时同步。通过对系统数据收集与建模，及时向客户提供最新的业务数据，以帮助客户快速了解招聘情况，并做出有效的调整。</p><p>数据更新方面，Doris 在大数据量（单表几十亿）的场景下，表现出了突出的数据更新和删除能力，Moka BI 读取的是业务库的 Binlog 数据，其中有大量的更新以及删除操作，Doris 可以通过 Routine Load 的 Delete 配置实现实时删除，根据 Key 实现幂等性写入，配合 Flink 可以做到真正的 Exactly-Once。在架构中增加了 Routine Load 后，数仓可以实现 1 分钟级别的准实时 <strong>，</strong> 同时结合 Routine load + Kafka 可以实现流量的削峰，保证集群稳定，并且可以通过重置 Kafka 偏移量来实现间数据重写，通过 Kafka 实现多点消费等。</p><p>数据查询方面，充分利用 Doris 的多表 Join 能力，使得系统能够实现实时查询。我们将不同的数据表按照关联字段进行连接，形成一个完整的数据集，基于数据集可进行各种数据分析和可视化操作，同时可高效应对任意条件组合的查询场景以及需要灵活定制需求的查询分析场景，<strong>在某些报表中，需要 Join 的表可能达到几十张，Doris 强大的 Join 性能，使 Moka</strong> <strong>BI</strong> <strong>的报表查询可以达到秒级响应。</strong></p><p>运维管理方面，Doris 部署运维简单方便，不依赖第三方组件，无损弹性扩缩容，自动数据均衡，集群高可用。Doris 集群仅有 FE 和 BE 两个组件，不依赖 Zookeeper 等组件即可实现高可用，部署、运维方便，相比传统的 Hadoop 组件，非常友好，支持弹性扩容，只需简单配置即可实现无损扩容，并且可以自动负载数据到扩容的节点，大大降低了我们引入新技术栈的难度和运维压力。</p><h1>调优实践</h1><p>新架构实际的落地使用中，我们总结了一些调优的经验，在此分享给大家。</p><p>在 Moka BI 报表查询权限场景中，同样配置的报表，<strong>有权限认证</strong>时查询速度比<strong>没有权限认证</strong>时慢 30% 左右，甚至出现查询超时，而<strong>超管权限</strong>查询时则正常，这一现象在数据量较大的客户报表中尤为明显。</p><p>人力资源管理业务的数据权限有着极为严格和精细的管控需求，除了 SaaS 业务自身对于不同租户间的数据隔离要求外，还需要针对业务人员的身份角色、管理部门范畴以及被管理人员的信息敏感程度对可见数据的范围进行进一步细分，因此在 Moka BI 权限功能模块的设计之时就考虑并实现了极为灵活的自定义配置化方案。例如 HRBP 与 PayRoll、HRIS 等角色的可见字段不同、不同职级或部门但角色一致用户的可见数据区间不同，同时针对部分敏感的人员信息还需要做数据过滤，或者出于管理授权的需求临时开通某一权限，甚至以上权限要求还会进行多重的交叉组合，以保证每一用户可查看的数据、报表、信息均被限制在权限范围以内。</p><p>因此当用户需要对数据报表进行查询时，会先在 Moka BI 的权限管理模块进行多重验证，验证信息会通过<code> in </code>的方式拼接在查询 SQL 中并传递给 OLAP 系统。随着客户业务体量的增大，对于权限管控的要求越精细、最终所产生的 SQL 就越复杂，部分业务规模比较大的客户报表会出现上千甚至更多的权限限制，因此造成 OLAP 系统的 id 过滤时间变长，导致报表查询延迟增加，给大客户造成了体验不佳。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3c1e3f9eb7674afab066f166a4142ce0~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>解决方案：</strong></p><p>为适配该业务场景，我们通过查看官方的文档发现 Doris Bloom Filter 索引的特性可以很好的解决该问题</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/550798e56e704e3e9dd927e0f9847e44~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>Doris BloomFilter 索引使用场景：</p><ul><li>BloomFilter 适用于非前缀过滤。</li><li>查询会根据该列高频过滤，而且查询条件大多是<code> in </code>和<code> = </code>过滤。</li><li>不同于 Bitmap，BloomFilter 适用于高基数列，比如 UserID。因为如果创建在低基数的列上，比如 “性别” 列，则每个 Block 几乎都会包含所有取值，导致 BloomFilter 索引失去意义。</li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0568e0172e3f46ea9c251cade3093063~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>经过验证，可以通过上方对比报表看到，<strong>将相关 ID 字段增加 BloomFilter 索引后</strong> <strong>，权限验证场景查询速度提升约 30% ，有权限验证的报表超时的问题也得到了改善。</strong></p><h1>收益与总结</h1><p><strong>目前 Moka</strong> <strong>BI</strong> <strong>Doris 有两个集群，</strong> <strong>共 40 台服务器，</strong> <strong>数仓</strong> <strong>共维护了 400 多张表</strong> <strong>，其中 50 多张表数据量超过 1 亿，总数据量为 T</strong> <strong>B</strong> <strong>级别。</strong></p><p>引入 Apache Doris 改造了新的数据仓库之后，满足了日益增长的分析需求以及对数据实时性的要求，总体收益包含以下几点：</p><ol><li><strong>高性能数据查询：</strong> Doris 基于列存储技术，能够快速处理大量的数据，并支持高并发的在线查询，解决了关系型数据库无法支持的复杂查询问题，复杂 SQL 查询的速度上升了一个数据量级。</li><li><strong>数据仓库</strong> <strong>的可扩展性：</strong> Doris 采用分布式集群架构，可以通过增加节点来线性提升存储和查询瓶颈，打破了关系型数据库数据单点限制问题，查询性能得以显著提升。</li><li><strong>更广泛的应用：</strong> 基于 Doris 构建了统一的数据查询平台，应用不再局限于报表服务，对于离线的查询也有很好的支撑，可以说 Doris 的引入是构建数仓一体化的前奏。</li><li><strong>实现自助式分析：</strong> 基于 Doris 强大的查询能力，我们引入了全新的报表构建方式，通过用户自助构建报表方式，能够快速满足用户的各种灵活需求。</li></ol><p>在使用 Doris 的两年多时间里，Moka BI 与 Apache Doris 共同成长、共同进步，可以说 Doris 成就了 Moka BI 强大的性能与优秀的用户体验；也正是 Moka BI 特殊的使用场景，也丰富了 Doris 的优化方向，我们提的很多 Issue 与建议，经过版本更新迭代后使其更具竞争力。在未来的时间里，Moka BI 也会紧跟社区脚步，不断优化、回馈社区，希望 <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a> 和 <a href="https://cn.selectdb.com/" target="_blank" rel="noopener noreferrer">SelectDB</a> 发展越来越好、越来越强大。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Tianyancha">秒级数据写入，毫秒查询响应，天眼查基于 Apache Doris 构建统一实时数仓</a></h2><div class="blog-info"><time datetime="2023-07-01T00:00:00.000Z" itemprop="datePublished">2023年7月1日</time><span class="split-line"></span><span class="authors"><span class="s-author">王涛</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong> 随着天眼查近年来对产品的持续深耕和迭代，用户数量也在不断攀升，业务的突破更加依赖于数据赋能，精细化的用户/客户运营也成为提升体验、促进消费的重要动力。在这样的背景下正式引入 Apache Doris 对数仓架构进行升级改造，实现了数据门户的统一，大大缩短了数据处理链路，数据导入速率提升 75 %，500 万及以下人群圈选可以实现毫秒级响应，收获了公司内部数据部门、业务方的一致好评。</p><p><strong>作者：</strong> 王涛，天眼查实时计算负责人</p><h1>业务需求</h1><p>天眼查的数据仓库主要服务于三个业务场景，每个场景都有其特点和需求，具体如下：</p><ol><li><strong>亿级用户人群圈选：</strong> 人群圈选场景中目前有 100+ 人群包，我们需要根据 SQL 条件圈选人群包，来支持人群包的交并差、人群包实时圈选和人群包更新通知下游等需求。例如：圈选出下单未支付超过 5 分钟的用户，我们通过用户标签可以直观掌握用户支付状态，为运营 &amp; 营销团队提供更精细化的人群管理服务，从而提高转化率。</li><li><strong>多元活动支撑的精准营销：</strong> 该场景目前支持了 1000 多个指标，可支持即席查询，根据活动效果及时调整运营策略。例如在“开工季”活动中，需要为数据分析 &amp; 运营团队提供数据支持，从而生成可视化的活动驾驶舱。</li><li><strong>高并发的 C 端分析数据：</strong> 该场景承载了 3 亿+实体（多种维度）的数据体量，同时要求实时更新，以供用户进行数据分析。</li></ol><h1>原有架构及痛点</h1><p>为满足各业务场景提出的需求，我们开始搭建第一代数据仓库，即原有数仓：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3d532df25bc948cb847107a149b9079f~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>在原有数仓架构中， Hive 作为数据计算层，MySQL、ES、PG 作为数据存储层，我们简单介绍一下架构的运行原理：</p><ul><li><strong>数据源层和数据接入层：</strong> MySQL 通过 Canal 将 BinLog 接入 Kafka、埋点日志通过 Flume 接入 Kafka，最后由 DataX 把 Kafka 中的数据接入数据计算层 Hive 中；</li><li><strong>数据计算层：</strong> 该层使用 Hive 中的传统的数仓模型，并利用海豚调度使数据通过 ODS -&gt; DWD -&gt; DWS 分层，最后通过 DataX 将 T+1 把数据导入到数据存储层的 MySQL 和 ES 中。</li><li><strong>数据存储层：</strong> MySQL 主要为 DataBank、Tableau、C 端提供分析数据，ES 用于存储用户画像数据，PG 用于人群包的存储（PG 安装的插件具有 Bitmap 交并差功能），ES、PG 两者均服务于 DMP人群圈选系统。</li></ul><p><strong>问题与挑战：</strong></p><p>依托于原有架构的投入使用，初步解决了业务方的需求，但随着天眼查近年来对产品的持续深耕和迭代，用户数量也在不断攀升，业务的突破更加依赖于数据赋能。精细化的用户/客户运营也成为提升体验、促进消费的重要动力。在这样的背景下，原有架构的缺点逐渐暴露：</p><ol><li>开发流程冗长：体现在数据处理链路上，比如当面对一个简单的开发需求，需要先拉取数据，再经过 Hive 计算，然后通过 T+1更新导入数据等，数据处理链路较长且复杂，非常影响开发效率。</li><li>不支持即席查询：体现在报表服务和人群圈选场景中，所用的指标无法根据条件直接查询，必须提前进行定义和开发。</li><li>T+1 更新延迟高：T+1 数据时效性已经无法提供精确的线索，主要体现在报表和人群圈选场景上。</li><li>运维难度高：原有架构具有多条数据处理链路、多组件耦合的特点，运维和管理难度都很高。</li></ol><h1>理想架构</h1><p>基于以上问题，我们决定对架构进行升级改进，在正式升级之前，我们希望未来的架构可以做到以下几点：</p><ul><li>原架构涉及 MySQL 、PG、ES 等多个组件，并为不同应用提供服务；我们希望未来的架构可以兼容 MySQL 协议，实现低成本替换、无缝衔接以上组件。</li><li>支持即席查询且性能优异，即席查询能够给业务方提供更灵活的表达方式，业务方可以从多个角度、多个维度对数据进行查询和分析，更好地发现数据的规律和趋势，帮助业务方更精准备地做出决策。</li><li>支持实时聚合，以减轻开发负担并保证计算结果的准确性。</li><li>统一数据出口，原架构中数据出口不唯一，我们希望未来的架构能更统一数据出口，缩短链路维护成本，提升数据的可复用性。</li><li>支持高并发， C 端的实时分析数据需要较高的并发能力，我们希望未来的架构可以高并发性能优异。</li></ul><h1>技术选型</h1><p>考虑到和需求的匹配度，我们重点对 OLAP 引擎进行了调研，并快速定位到 ClickHouse 和 <a href="https://doris.apache.org/zh-CN/" target="_blank" rel="noopener noreferrer">Apache Doris</a> 这两款产品，在深入调研中发现 Doris 在以下几个方面优势明显，更符合我们的诉求：</p><ul><li>标准 SQL：ClickHouse 对标准 SQL 支持有限，使用中需要对多表 Join 语法进行改写；而 Doris 兼容 MySQL 协议，支持标准 SQL ，可以直接运行，同时 Doris 的 Join 性能远优于 ClickHouse。</li><li>降本增效：Doris 部署简单，只有 FE 和 BE 两个组件，不依赖其他系统；生态内导数功能较为完备，可针对数据源/数据格式选择导入方式；还可以直接使用命令行操作弹性伸缩，无需额外投入人力；运维简单，问题排查难度低。相比之下，ClickHouse 需要投入较多的开发人力来实现类似的功能，使用难度高；同时 ClickHouse 运维难度很高，需要研发一个运维系统来支持处理大部分的日常运维工作。</li><li>并发能力：ClickHouse 的并发能力较弱是一个潜在风险，而 Doris 并发能力更占优势，并且刚刚发布的 2.0 版本支持了<a href="https://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247516978&amp;idx=1&amp;sn=eb3f1f74eedd2306ca0180b8076fe773&amp;chksm=cf2f8d35f85804238fd680c18b7ab2bc4c53d62adfa271cb31811bd6139404cc8d2222b9d561&amp;token=699376670&amp;lang=zh_CN#rd" target="_blank" rel="noopener noreferrer">更高并发的点查</a>。</li><li>导入事务：ClickHouse 的数据导入没有事务支持，无法实现 Exactly Once 语义，如导数失败需要删除重导，流程比较复杂；而 Doris 导入数据支持事务，可以保证一批次内的数据原子生效，不会出现部分数据写入的情况，降低了判断的成本。</li><li>丰富的使用场景：ClickHouse 支持场景单一，Doris 支持场景更加丰富，用户基于 Doris 可以构建用户行为分析、AB 实验平台、日志检索分析、用户画像分析、订单分析等应用。</li><li>丰富的数据模型：Doris 提供了Unique、Duplicate、Aggregate 三种数据模型，可以针对不同场景灵活应用不同的数据模型。</li><li>社区响应速度快：Doris 社区的响应速度是其独有特色，SelectDB 为社区组建了一直完备的社区支持团队，社区的快速响应让我们少走了很多歪路，帮助我们解决了许多问题。</li></ul><h1>新数仓架构</h1><p>经过对 Doris 进行综合评估，我们最终决定采用 Doris 对原有架构进行升级优化，并在架构层级进行了压缩。新的架构图如下所示：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c053e0f2491c44c5a2ef253f7496f449~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>在新架构中，数据源层和数据接入层与原有架构保持一致，<strong>主要变化是将 Doris 作为新架构的数据服务层，统一了原有架构中的数据计算层和存储层，这样实现了数据门户的统一，大大缩短了数据处理链路，解决了开发流程冗长的问题。</strong> 同时，基于 Doris 的高性能，实现了即席查询能力，提高了数据查询效率。另外，Flink 与 Doris 的结合实现了实时数据快速写入，解决了 T+1 数据更新延迟较高的问题。除此之外，借助于 Doris 精简的架构，大幅降低了架构维护的难度。</p><p><strong>数据流图</strong></p><p>缩短数据处理链路直接或间接地带来了许多收益。接下来，我们将具体介绍引入 Doris 后的数据流图。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/090c5467c81e43a0b68da227cab59dad~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>总体而言，数据源由 MySQL 和日志文件组成，数据在 Kafka 中进行分层操作（ODS、DWD、DWS），Apache Doris 作为数据终点统一进行存储和计算。应用层包含 C 端、Tableau 和 DMP 系统，通过网关服务从 Doris 中获取相应的数据。</p><p>具体来看，MySQL 通过 Canal 把 Binlog 接入 Kafka，日志文件通过 Flume 接入 Kafka 作为 ODS 层。然后经过 Flink SQL 进行清洗、关联维表，形成 DWD 层的宽表，并生成聚合表。为了节省空间，我们将 ODS 层存储在 Kafka 中，DWD 层和 DWS 层主要与 Doris 进行交互。DWD 层的数据一般通过 Flink SQL 写入 Doris。针对不同的场景，我们应用了不同的数据模型进行数据导入。MySQL 数据使用 Unique 模型，日志数据使用 Duplicate 模型，DWS 层采用 Aggregate 模型，可进行实时聚合，从而减少开发成本。</p><h1>应用场景优化</h1><p>在应用新的架构之后，我们必须对业务场景的数据处理流程进行优化以匹配新架构，从而达到最佳应用效果。接下来我们以人群圈选、C端分析数据及精准营销线索为主要场景，分享相关场景流程优化的实践与经验。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="人群圈选">人群圈选<a href="#人群圈选" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/d7dd9fa07aa24b9f8368afa24efcd736~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p><strong>原流程（左）中</strong>，业务人员在画像平台页面上利用表的元数据创建人群圈选任务，任务创建后进行人群 ID 分配，写入到 PG 画像表和 MySQL 任务表中。接着根据任务条件定时在 ES 中查询结果，获取结果后更新任务表的状态，并把 Bitmap 人群包写入 PG。利用 PG 插件提供的 Bitmap 交并差能力操作人群包，最后下游运营介质从 PG 取相应人群包。</p><p>然而，该流程处理方式非常复杂，ES 和 PG 中的表无法复用，造成成本高、效益低。同时，原流程中的数据为 T+1 更新，标签必须提前进行定义及计算，这非常影响查询效率。</p><p><strong>现流程（右）中</strong>，业务人员在画像平台创建人群圈选任务，后台分配人群 ID，并将其写入 MySQL 任务表中。首次圈选时，根据任务条件在 Doris 中进行即席查询，获取结果后对任务表状态进行更新，并将人群包写入 Doris。后续根据时间进行微批轮询，利用 Doris Bitmap 函数提供的交并差功能与上一次的人群包做差集，如果有人群包更新会主动通知下游。</p><p>引入 Doris 后，原有流程的问题得到了解决，新流程以 Doris 为核心构建了人群圈选服务，支持人群包实时更新，新标签无需提前定义，可通过条件配置自助生成，减少了开发时间。新流程表达方式更加灵活，为人群包 AB 实验提供了便捷的条件。流程中采用 Doris 统一了明细数据和人群包的存储介质，实现业务聚焦，无需处理多组件数据之间的读写问题，达到了降本增效的终极目标。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="c端分析数据及精准营销线索场景">C端分析数据及精准营销线索场景<a href="#c端分析数据及精准营销线索场景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d94e74cae5748f7944a61b4f6d85a53~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p><strong>原流程：</strong> 在原流程中，如果业务提出新需求，需要先发起需求变更，再经过评审、排期开发，然后开始对 Hive 中的数据模型进行开发并进行测试，测试完成后进行数仓上线，配置 T+1 调度任务写入 MySQL，最后 C端和精准营销系统对 MySQL 数据进行读取。原流程链路复杂，主要体现在流程长、成本高、上线周期长。</p><p><strong>现流程：</strong> 当前明细数据已经在 Doris 上线，当业务方发起需求变更时，只需要拉取元数据管理平台元数据信息，配置查询条件，审批完成后即可上线，上线 SQL 可直接在 Doris 中进行即席查询。相比原流程，现在的流程大幅缩短了需求变更流程，只需进行低代码配置，成功降低了开发成本，缩短了上线周期。</p><h1>优化经验</h1><p>为了规避风险，许多公司的人群包<code>user_id</code>是随机生成的，这些<code>user_id</code>相差很大且是非连续的。然而，使用非连续的<code>user_id</code>进行人群圈选时，会导致 Bitmap 生成速度较慢。因此，我们生成了映射表，并生成了连续稠密的<code>user_id</code>。当使用连续 <code>user_id</code> 圈选人群时，<strong>速度较之前提升了 70%</strong> 。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0fcb33643d97446e997f8e3961f58de9~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>用户 ID 映射表样例数据：从图可知原始用户 ID 由多位数字组合，并且 ID 很稀疏（用户 ID 间相差很大），而连续用户 ID 则 从1开始，且 ID 很稠密。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd03622a2587491993f0d0bf9a50eb07~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p><strong>案例展示：</strong></p><ol><li>用户 ID 映射表：</li></ol><p>用户 ID 映射表将用户 ID 作为唯一键模型，而连续用户 ID 则通过用户 ID 来生成，一般从 1 开始，严格保持单调递增。需要注意的是，因为该表使用频繁，因此将 <code>in_memory</code> 设置为<code>true</code>，直接将其缓存在内存中：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6e1b14d76edd4d29b1bb8c3fc31a2ef4~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><ol start="2"><li>人群包表</li></ol><p>人群包表是以用户标签作聚合键的模型，假设以 <code>user_id</code> 大于 0、小于 2000000 作为圈选条件，使用原始 <code>user_id</code> 进行圈选耗费的时间远远远大于连续稠密 <code>user_id</code> 圈选所耗时间。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ef541a3299d642d9a9cd2cd7b353880c~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>如下图所示，左侧使用 <code>tyc_user_id</code>圈选生成人群包响应时间：1843ms，右侧使用使<code>tyc_user_id_continuous</code>圈选生成人群包响应时间：543ms。消耗时间大幅缩短</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0b309ad36a774c09ae087e372f4605b7~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/33672f62dca344dfb49863eb24cccb4a~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h1>规模与收益：</h1><p>引入 Doris 后，我们已经搭建了 2 个集群，承载的数据规模正随着迁移的推进而持续增大。目前，<strong>我们已经处理的数据总量已经达到了数十TB，单日新增数据量已经达到了 数亿条</strong>，而数据体量还在持续增长中。此外，我们在 Doris 上运行的指标和人群包数量已经超过了 500，分别涵盖了商查、搜索、运营、用户和营收五大类指标。</p><p>Doris 的引入满足了业务上的新需求，解决了原有架构的痛点问题，具体表现为以下几点：</p><ul><li><strong>降本增效：</strong> Doris 统一了数据的门户，实现了存储和计算的统一，提高了数据/表的复用率，降低了资源消耗。同时，新架构优化了数据到 MySQL、ES 的流程，开发效率得到有效提升。</li><li><strong>导入速率提升：</strong> 原有数据流程中，数据处理流程过长，数据的导入速度随着业务体量的增长和数据量的不断上升而急剧下降。引入 Doris 后，我们依赖 Broker Load 优秀的写入能力，使得<strong>导入速率提升了 75%以上</strong>。</li><li><strong>响应速度</strong>：Doris 的使用提高了各业务场景中的查询响应速度。例如，在人群圈选场景中，对于 <strong>500 万及以下的人群包进行圈选时，能够做到毫秒级响应</strong>。</li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9359e2a027ff47c992c4efc3a96dac94~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h1>未来规划</h1><p>正如前文所讲，Apache Doris 的引入解决了许多架构及业务上的难题，初见成效，同时也收获了公司内部数据部门、业务方的一致好评，未来我们将继续探索，基于 Doris 展开更深度的应用，不久的将来，我们将重点推进以下几个方面工作：</p><ul><li>离线指标实时化：将更多的指标从离线转为实时，提供更及时的数据服务。</li><li>搭建数据血缘系统：将代码中的血缘关系重新定义为可视，全面构建数据血缘关系，为问题排查、链路报警等提供有效支持。</li><li>探索批流一体路线：从使用者的角度思考设计，实现语义开发层的统一，使数据开发更便捷、更低门槛、更高效率。</li></ul><p>在此特别感谢 <a href="https://cn.selectdb.com/" target="_blank" rel="noopener noreferrer">SelectDB 团队</a>，作为基于 <a href="https://doris.apache.org/zh-CN/" target="_blank" rel="noopener noreferrer">Apache Doris</a> 的商业化公司，为社区投入了大量的研发和用户支持力量，在使用过程中遇到任何问题都能及时响应，为我们降低了许多试错成本。未来，我们也会更积极参与社区贡献及活动中来，与社区共同进步和成长，欢迎大家选择和使用 Doris，相信 Doris 一定不会让你失望。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/360">日增百亿数据，查询结果秒出， Apache Doris 在 360商业化的统一 OLAP 应用实践</a></h2><div class="blog-info"><time datetime="2023-06-01T00:00:00.000Z" itemprop="datePublished">2023年6月1日</time><span class="split-line"></span><span class="authors"><span class="s-author">360商业化数据团队</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读</strong>：360商业化为助力业务团队更好推进商业化增长，实时数仓共经历了三种模式的演进，分别是 Storm + Druid + MySQL 模式、Flink + Druid + TIDB 的模式 以及 Flink + Doris 的模式，基于 <a href="https://doris.apache.org/" target="_blank" rel="noopener noreferrer">Apache Doris</a> 的新一代架构的成功落地使得 360商业化团队完成了实时数仓在 OLAP 引擎上的统一，成功实现广泛实时场景下的秒级查询响应。本文将为大家进行详细介绍演进过程以及新一代实时数仓在广告业务场景中的具体落地实践。</p><p>作者｜360商业化数据团队 窦和雨、王新新</p><p>360 公司致力于成为互联网和安全服务提供商，是互联网免费安全的倡导者，先后推出 360安全卫士、360手机卫士、360安全浏览器等安全产品以及 360导航、360搜索等用户产品。</p><p>360商业化依托 360产品庞大的用户覆盖能力和超强的用户粘性，通过专业数据处理和算法实现广告精准投放，助力数十万中小企业和 KA 企业实现价值增长。360商业化数据团队主要是对整个广告投放链路中所产生的数据进行计算处理，为产品运营团队提供策略调整的分析数据，为算法团队提供模型训练的优化数据，为广告主提供广告投放的效果数据。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务场景">业务场景<a href="#业务场景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在正式介绍 Apache Doris 在 360 商业化的应用之前，我们先对广告业务中的典型使用场景进行简要介绍：</p><ul><li><strong>实时大盘：</strong> 实时大盘场景是我们对外呈现数据的关键载体，需要从多个维度监控商业化大盘的指标情况，包括流量指标、消费指标、转化指标和变现指标，因此其对数据的准确性要求非常高（保证数据不丢不重），同时对数据的时效性和稳定性要求也很高，要求实现秒级延迟、分钟级数据恢复。</li><li><strong>广告账户的实时消费数据场景：</strong> 通过监控账户粒度下的多维度指标数据，及时发现账户的消费变化，便于产品团队根据实时消费情况推动运营团队对账户预算进行调整。在该场景下数据一旦出现问题，就可能导致账户预算的错误调整，从而影响广告的投放，这对公司和广告主将造成不可估量的损失，因此在该场景中，同样对数据准确性有很高的要求。目前在该场景下遇到的困难是如何在数据量比较大、查询交叉的粒度比较细的前提下，实现秒级别查询响应。</li><li><strong>AB 实验平台：</strong> 在广告业务中，算法和策略同学会针对不同的场景进行实验，在该场景下，具有报表维度不固定、多种维度灵活组合、数据分析比较复杂、数据量较大等特点，这就需要可以在百万级 QPS 下保证数据写入存储引擎的性能，因此我们需要针对业务场景进行特定的模型设计和处理上的优化，提高实时数据处理的性能以及数据查询分析的效率，只有这样才能满足算法和策略同学对实验报表的查询分析需求。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时数仓演进">实时数仓演进<a href="#实时数仓演进" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>为提升各场景下数据服务的效率，助力相关业务团队更好推进商业化增长，截至目前实时数仓共经历了三种模式的演进，分别是 Storm + Druid + MySQL 模式、Flink + Druid + TIDB 的模式 以及 Flink + Doris 的模式，本文将为大家进行详细介绍实时数仓演进过程以及新一代实时数仓在广告业务场景中的具体落地。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第一代架构">第一代架构<a href="#第一代架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>该阶段的实时数仓是基于 Storm + Druid + MySQL 来构建的，Storm 为实时处理引擎，数据经 Storm 处理后，将数据写入 Druid ，利用 Druid 的预聚合能力对写入数据进行聚合。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/be75fc93fa7c43299e366246f89297d4~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>架构痛点：</strong></p><p>最初我们试图依靠该架构解决业务上所有的实时问题，经由 Druid 统一对外提供数据查询服务，但是在实际的落地过程中我们发现 Druid 是无法满足某些分页查询和 Join 场景的，为解决该问题，我们只能利用 MySQL 定时任务的方式将数据定时从 Druid 写入 MySQL 中（类似于将 MySQL 作为 Druid 的物化视图），再通过 Druid + MySQL 的模式对外提供服务。通过这种方式暂时可以满足某些场景需求，但随着业务规模的逐步扩大，当面对更大规模数据下的查询分析需求时，该架构已难以为继，架构的缺陷也越发明显：</p><ul><li>面对数据量的持续增长，数据仓库压力空前剧增，已无法满足实时数据的时效性要求。</li><li>MySQL 的分库分表维护难度高、投入成本大，且 MySQL 表之间的数据一致性无法保障。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第二代架构">第二代架构<a href="#第二代架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/44160b89e28546d6871fcabddc2eacc4~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>基于第一套架构存在的问题，我们进行了首次升级，这次升级的主要变化是将 Storm 替换成新的实时数据处理引擎 Flink ，Flink 相较于 Storm 不仅在许多语义和功能上进行了扩展，还对数据的一致性做了保证，这些特性使得报表的时效性大幅提升；其次我们使用 TiDB 替换了 MySQL ，利用 TIDB 分布式的特性，一定程度上解决了 MySQL 分库分表难以维护的问题（TiDB 在一定程度上比 MySQL 能够承载更大数据量，可以拆分更少表）。在升级完成后，我们按照不同业务场景的需求，将 Flink 处理完的数据分别写入 Druid 和 TiDB ，由 Druid 和 TIDB 对外提供数据查询服务。</p><p><strong>架构痛点：</strong></p><p>虽然该阶段的实时数仓架构有效提升了数据的时效性、降低了 MySQL 分库分表维护的难度，但在一段时间的使用之后又暴露出了新的问题，也迫使我们进行了第二次升级：</p><ul><li>Flink + TIDB 无法实现端到端的一致性，原因是当其面对大规模的数据时，开启事务将对 TiDB 写入性能造成很大的影响，该场景下 TiDB 的事务形同虚设，心有余而力不足。</li><li>Druid 不支持标准 SQL ，使用有一定的门槛，相关团队使用数据时十分不便，这也直接导致了工作效率的下降。</li><li>维护成本较高，需要维护两套引擎和两套查询逻辑，极大增加了维护和开发成本的投入。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新一代实时数仓架构">新一代实时数仓架构<a href="#新一代实时数仓架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>第二次升级我们引入 Apache Doris 结合 Flink 构建了新一代实时数仓架构，借鉴离线数仓分层理念对实时数仓进行分层构建，并统一 Apache Doris 作为数仓 OLAP 引擎，由 Doris 统一对外提供服务。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0b2dbffd2e3640b893d4a94d73eff253~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们的数据主要源自于维表物料数据和业务打点日志。维表物料数据会定时全量同步到 Redis 或者 Aerospike （类似于 Redis 的 KV 存储）中，通过 Binlog 变更进行增量同步。业务数据由各个团队将日志收集到 Kafka，内部称为 ODS 原始数据（ODS 原始数据不做任何处理），我们对 ODS 层的数据进行归一化处理，包括字段命名、字段类型等，并对一些无效字段进行删减，并根据业务场景拆分生成 DWD 层数据，DWD 层的数据通过业务逻辑加工以及关联 Redis 中维表数据或者多流 Join，最后生成面向具体业务的大宽表（即 DWT 层数据），我们将 DWT 层数据经过聚合、经由 Stream Load 写入 Doris 中，由 Doris 对外提供数据查询服务。在离线数仓部分，同样也有一些场景需要每日将加工完的 DWS 数据经由 Broker Load 写入到 Doris 集群中，并利用 Doris 进行查询加速，以提升我们对外提供服务的效率。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="选择-doris-的原因">选择 Doris 的原因<a href="#选择-doris-的原因" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>基于 Apache Doris 高性能、极简易用、实时统一等诸多特性，助力 360商业化成功构建了新一代实时数仓架构，本次升级不仅提升了实时数据的复用性、实现了 OLAP 引擎的统一，而且满足了各大业务场景严苛的数据查询分析需求，使得整体实时数据流程架构变得简单，大大降低了其维护和使用的成本。我们选择 Doris 作为统一 OLAP 引擎的重要原因大致可归结为以下几点：</p><ul><li><strong>物化视图：</strong> Doris 的物化视图与广告业务场景的特点契合度非常高，比如广告业务中大部分报表的查询维度相对比较固定，利用物化视图的特性可以提升查询的效率，同时 Doris 可以保证物化视图和底层数据的一致性，该特性可帮助我们降低维护成本的投入。</li><li><strong>数据一致性：</strong> Doris 提供了 Stream Load Label 机制，我们可通过事务的方式与 Flink 二阶段提交进行结合，以保证幂等写入数据，另外我们通过自研 Flink Sink Doris 组件，实现了数据的端到端的一致性，保证了数据的准确性。</li><li><strong>SQL 协议兼容</strong>：Doris 兼容 MySQL 协议，支持标准 SQL，这无论是对于开发同学，还是数据分析、产品同学，都可以实现无成本衔接，相关同学直接使用 SQL 就可以进行查询，使用门槛很低，为公司节省了大量培训和使用成本，同时也提升了工作效率。</li><li><strong>优秀的查询性能：</strong> Apache Doris 已全面实现向量化查询引擎，使 Doris 的 OLAP 性能表现更加强悍，在多种查询场景下都有非常明显的性能提升，可极大优化了报表的询速度。同时依托列式存储引擎、现代的 MPP 架构、预聚合物化视图、数据索引的实现，在低延迟和高吞吐查询上，都达到了极速性能</li><li><strong>运维难度低：</strong> Doris 对于集群和和数据副本管理上做了很多自动化工作，这些投入使得集群运维起来非常的简单，近乎于实现零门槛运维。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="在-ab-实验平台的具体落地">在 AB 实验平台的具体落地<a href="#在-ab-实验平台的具体落地" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>Apache Doris 目前广泛应用于 360商业化内部的多个业务场景。比如在实时大盘场景中，我们利用 Doris 的 Aggregate 模型对请求、曝光、点击、转化等多个实时流进行事实表的 Join ；依靠 Doris 事务特性保证数据的一致性；通过多个物化视图，提前根据报表维度聚合数据、提升查询速度，由于物化视图和 Base 表的一致关系由 Doris 来维护保证，这也极大的降低了使用复杂度。比如在账户实时消费场景中，我们主要借助 Doris 优秀的查询优化器，通过 Join 来计算同环比......</p><p><strong>接下来仅以 AB 实验平台这一典型业务场景为例，详尽的为大家介绍 Doris 在该场景下的落地实践，在上述所举场景中的应用将不再赘述。</strong></p><p>AB 实验在广告场景中的应用非常广泛，是衡量设计、算法、模型、策略对产品指标提升的重要工具，也是精细化运营的重要手段，我们可以通过 AB实验平台对迭代方案进行测试，并结合数据进行分析和验证，从而优化产品方案、提升广告效果。</p><p>在文章开头也有简单介绍，AB 实验场景所承载的业务相对比较复杂，这里再详细说明一下：</p><ul><li>各维度之间组合灵活度很高，例如需要对从 DSP 到流量类型再到广告位置等十几个维度进行分析，完成从请求、竞价、曝光、点击、转化等几十个指标的完整流量漏斗。</li><li>数据量巨大，日均流量可以达到<strong>百亿级别</strong>，峰值可达<strong>百万OPS</strong>（Operations Per Second），一条流量可能包含<strong>几十个实验标签 ID</strong>。</li></ul><p>基于以上特点，我们在 AB实验场景中一方面需要保证数据算的快、数据延迟低、用户查询数据快，另一方面也要保证数据的准确性，保障数据不丢不重。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2ef64a9fbc274f1bb33a864c813b39f6~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据落地">数据落地<a href="#数据落地" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>当面对一条流量可能包含几十个实验标签 ID 的情况时，从分析角度出发，只需要选中一个实验标签和一个对照实验标签进行分析；而如果通过<code>like</code>的方式在几十个实验标签中去匹配选中的实验标签，实现效率就会非常低。</p><p>最初我们期望从数据入口处将实验标签打散，将一条包含 20 个实验标签的流量拆分为 20 条只包含一个实验标签的流量，再导入 Doris 的聚合模型中进行数据分析。而在这个过程中我们遇到一个明显的问题，当数据被打散之后会膨胀数十倍，百亿级数据将膨胀为千亿级数据，即便 Doris 聚合模型会对数据再次压缩，但这个过程会对集群造成极大的压力。因此我们放弃该实现方式，开始尝试将压力分摊一部分到计算引擎，这里需要注意的是，如果将数据直接在 Flink 中打散，当 Job 全局 Hash 的窗口来 Merge 数据时，膨胀数十倍的数据也会带来几十倍的网络和 CPU 消耗。</p><p>接着我们开始第三次尝试，这次尝试我们考虑在 Flink 端将数据拆分后立刻进行 Local Merge，在同一个算子的内存中开一个窗口，先将拆分的数据进行一层聚合，再通过 Job 全局 Hash 窗口进行第二层聚合，因为 Chain 在一起的两个算子在同一个线程内，因此可以大幅降低膨胀后数据在不同算子之间传输的网络消耗。该方式<strong>通过两层窗口的聚合，再结合 Doris 的聚合模型，有效降低了数据的膨胀程度</strong>，其次我们也同步推动实业务方定期清理已下线的实验，减少计算资源的浪费。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/02bfff5bf0224fa7a7bb21879f9c6b12~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>考虑到 AB实验分析场景的特点，我们将实验 ID 作为 Doris 的第一个排序字段，利用前缀索引可以很快定位到目标查询的数据。另外根据常用的维度组合建立物化视图，进一步缩小查询的数据量，<strong>Doris 物化视图基本能够覆盖 80% 的查询场景</strong>，我们会定期分析查询 SQL 来调整物化视图。<strong>最终我们通过模型的设计、前缀索引的应用，结合物化视图能力，使大部分实验查询结果能够实现秒级返回。</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据一致性保障">数据一致性保障<a href="#数据一致性保障" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>数据的准确性是 AB实验平台的基础，当算法团队呕心沥血优化的模型使广告效果提升了几个百分点，却因数据丢失看不出实验效果，这样的结果确实无法令人接受，同时这也是我们内部不允许出现的问题。那么我们该如何避免数据丢失、保障数据的一致性呢？</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="自研-flink-sink-doris-组件"><strong>自研 Flink Sink Doris 组件</strong><a href="#自研-flink-sink-doris-组件" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>我们内部已有一套 Flink Stream API 脚手架，因此借助 Doris 的幂等写特性和 Flink 的二阶段提交特性，自研了 Sink To Doris 组件，保证了数据端到端的一致性，并在此基础上新增了异常情况的数据保障机制。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fcba3984e6e641049b1b343ed8e021b5~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>在 Doris 0.14 版本中（初期使用的版本），我们一般通过“同一个 Label ID 只会被写入一次”的机制来保证数据的一致性；在 Doris 1.0 版本之后，通过 “Doris 的事务结合 Flink 二阶段提交”的机制来保证数据的一致性。这里详细分享使用 Doris 1.0 版本之后，通过 “Doris 的事务结合 Flink 二阶段提交”机制保证数据的一致性的原理与实现。</p><blockquote><p>在 Flink 中做到数据端到端的一致性有两种方式，一种为通过至少一次结合幂等写，一种为通过恰好一次的二阶段事务。</p></blockquote><p>如右图所示，我们首先在数据写入阶段先将数据写入本地文件，一阶段过程中将数据预提交到 Doris，并保存事务 ID 到状态，如果 Checkpoint 失败，则手动放弃 Doris 事务；如果 Checkpoint 成功，则在二阶段进行事务提交。对于二阶段提交重试多次仍然失败的数据，将提供数据以及事务 ID 保存到 HDFS 的选项，通过 Broker Load 进行手动恢复。为了避免单次提交数据量过大，而导致 Stream Load 时长超过 Flink Checkpoint 时间的情况，我们提供了将单次 Checkpoint 拆分为多个事务的选项。<strong>最终成功通过二阶段提交的机制实现了对数据一致性的保障。</strong></p><p><strong>应用展示</strong></p><p>下图为 Sink To Doris 的具体应用，整体工具屏蔽了 API 调用以及拓扑流的组装，只需要通过简单的配置即可完成 Stream Load 到 Doris 的数据写入 。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5b599cbb96234d4b8e6783128af0c31a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bb38729f3f73404da1b5f4fa50fcd527~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="集群监控">集群监控<a href="#集群监控" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在集群监控层面，我们采用了社区提供的监控模板，从集群指标监控、主机指标监控、数据处理监控三个方面出发来搭建 Doris 监控体系。其中集群指标监控和主机指标监控主要根据社区监控说明文档进行监控，以便我们查看集群整体运行的情况。除社区提供的模板之外，我们还新增了有关 Stream Load 的监控指标，比如对当前 Stream Load 数量以及写入数据量的监控，如下图所示：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ebe459f1fa0a4a87bacf0f8209eaf836~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>除此之外，我们对数据写入 Doris 的时长以及写入的速度也比较关注，根据自身业务的需求，我们对任务写入数据速度、处理数据耗时等数据处理相关指标进行监控，帮助我们及时发现数据写入和读取的异常情况，借助公司内部的报警平台进行监控告警，报警方式支持电话、短信、推推、邮件等</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/aef04fc519a64920b7ae4feea0c84cc8~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结与规划">总结与规划<a href="#总结与规划" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>目前 Apache Doris 主要应用于广告业务场景，<strong>已有数十台集群机器，覆盖近 70% 的实时数据分析场景，实现了全量离线实验平台以及部分离线 DWS 层数据查询加速。当前日均新增数据规模可以达到百亿级别，在大部分实时场景中，其查询延迟在 1s 内</strong>。同时，Apache Doris 的成功落地使得我们完成了实时数仓在 OLAP 引擎上的统一。Doris 优异的分析性能及简单易用的特点，也使得数仓架构更加简洁。</p><p>未来我们将对 Doris 集群进行扩展，根据业务优先级进行资源隔离，完善资源管理机制，并计划将 Doris 应用到 360商业化内部更广泛的业务场景中，充分发挥 Doris 在 OLAP 场景的优势。最后我们将更加深入的参与到 Doris 社区中来，积极回馈社区，与 Doris 并肩同行，共同进步！</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Midland Realty">Apache Doris 在美联物业的数据仓库应用实践，助力传统行业数字化革新</a></h2><div class="blog-info"><time datetime="2023-05-12T00:00:00.000Z" itemprop="datePublished">2023年5月12日</time><span class="split-line"></span><span class="authors"><span class="s-author">谢帮桂</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong> 传统行业面对数字化转型往往会遇到很多困难，比如缺乏数据管理体系、数据需求开发流程冗长、烟囱式开发、过于依赖纸质化办公等，美联物业也有遇到类似的问题。本文主要介绍美联物业基于 <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a> 在数据体系方面的建设，以及对数据仓库搭建经验进行的分享和介绍，旨在为数据量不大的传统企业提供一些数仓思路，实现数据驱动业务，低成本、高效的进行数仓改造。</p><p>作者｜美联物业数仓负责人 谢帮桂</p><p>美联物业属于香港美联集团成员，于 1973 年成立，并于 1995 年在香港联合交易所挂牌上市(香港联交所编号:1200)，2008 年美联工商铺于主板上市（香港联交所编号:459）， 成为拥有两家上市公司的地产代理企业。拥有 40 余载房地产销售行业经验，业务涵盖中、小型住宅、豪宅及工商铺，提供移民顾问、金融、测量、按揭转介等服务，业务遍布中国香港地区、中国澳门地区和中国内地等多个重要城市。</p><p>本文主要介绍关于美联物业在数据体系方面的建设，以及对数据仓库搭建经验进行的分享和介绍，旨在为数据量不大的传统企业提供一些数仓思路，实现数据驱动业务，低成本、高效的进行数仓改造。</p><p><em>考虑隐私政策，本文不涉及公司任何具体业务数据。</em></p><h1>业务背景</h1><p>美联物业早在十多年前就已深入各城市开展房地产中介业务，数据体系的建设和发展与大多数传统服务型公司类似，经历过几个阶段时期，如下图所示。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/99156394583e4c289c81872b1ebbace5~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们的数据来源于大大小小的子业务系统和部门手工报表数据等，存在历史存量数据庞大，数据结构多样复杂，数据质量差等普遍性问题。此外，早期业务逻辑处理多数是使用关系型数据库 SQL Server 的存储过程来实现，当业务流程稍作变更，就需要投入大量精力排查存储过程并进行修改，使用及维护成本都比较高。</p><p><strong>基于此背景，我们面临的挑战可以大致归纳为以下几点：</strong></p><ul><li>缺乏数据管理体系，统计口径统一，已有数据无法降本复用。多部门、多系统、多字段，命名随意、表违反范式结构混乱；对同一业务来源数据无法做到多份报表复用，反复在不同报表编写同一套计算逻辑。</li><li>海量数据下性能不足，查询响应慢。历史大多数业务数据存储在关系型数据库中，分表分库已无法做到上亿数据秒级分析查询。</li><li>数据需求开发流程冗长、烟囱式开发。每当业务部门提出一个数据需求，数据开发就需要在多个系统之间进行数据兼容编写存储过程，从而导致存储过程的可移植性和可读性都非常差。</li><li>部门之间严重依赖文本文档处理工作，效率低下。由于长期的手工统计，用户已形成习惯，导致对信息系统的信任程度也比较低。</li></ul><h1>早期架构</h1><p>针对上述的⼏个需求，我们在平台建设的初期选⽤了 Hadoop、Hive、Spark 构建最初的离线数仓架构，也是比较普遍、常见的架构，运作原理不进行过多赘述。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a80f8ff750a04761b6d97eb1fdfc0250~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们数据体系主要服务对象以内部员工为主，如房产经纪人、后勤人员、行政人事、计算机部门，房产经纪在全国范围内分布广泛，也是我们的主要服务对象。当前数据体系还无需面向 C 端用户，因此在数据计算和资源方面的压力并不大，早期基于 Hadoop 的架构可以满足一部分基本的需求。但是随着业务的不断发展、内部人员对于数据分析的复杂性、分析的效率也越来越高，该架构的弊端日益越发的明显，主要体现为以下几点：</p><ul><li>过于笨重：传统公司的计算量和数据量并不大，使用 Hadoop 过于浪费。</li><li>效率低下：T+1 的调度时效和脚本，动辄需要花费 1 小时的计算时间导入导出，效率低、影响数据的开发工作。</li><li>维护成本高：组件过多，排查故障链路过长，运维成本也很高，且部门同事之间熟悉各个组件需要大量学习和沟通成本。</li></ul><h1>新数仓架构</h1><p>基于上述业务需求及痛点，我们开始了架构升级，并希望在这次升级中实现几个目标：</p><ul><li>初步建立数据管理体系，搭建数据仓库。</li><li>搭建报表平台和报表快速开发流程体系。</li><li>实现数据需求能够快速反应和交付（1小时内），查询延迟不超过 10s。</li><li>最小成本原则构建架构，支持滚动扩容。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="技术选型">技术选型<a href="#技术选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>经过调研了解以及朋友推荐，我们了解到了 Apache Doris ，并很快与社区取得了联系，Apache Doris 的几大优势吸引了我们：</p><p><strong>足够简单</strong></p><p>美联物业及大部分传统公司的数据人员除了需要完成数据开发工作之外，还需要兼顾运维和架构规划的工作。因此我们选择数仓组件的第一原则就是&quot;简单&quot;，简单主要包括两个方面：</p><ul><li>使用简单：Apache Doris 兼容 MySQL 协议，支持标准 SQL，有利于开发效率和共识统一，此外，Doris 的 ETL 编写脚本主要使用 SQL进行开发，使用 MySQL 协议登陆使用，兼容多数 MySQL 语法，提供丰富的数据分析函数，省去了 UDF 开发工作。</li><li>架构简单：Doris 的组件架构由 FE+BE 两类进程组成，不依赖其他系统，升级扩容非常方便，故障排查链路非常清晰，有利于运维成本的降低。</li></ul><p><strong>极速性能</strong></p><p>Doris 依托于列式存储引擎、自动分区分桶、向量计算、多方面 Join 优化和物化视图等功能的实现，可以覆盖众多场景的查询优化，海量数据也能可以保证低延迟查询，实现分钟级或秒级响应。</p><p><strong>极低成本</strong></p><p>降本提效已经成为现如今企业发展的常态，免费的开源软件就比较满足我们的条件，另外基于 Doris 极简的架构、语言的兼容、丰富的生态等，为我们节省了不少的资源和人力的投入。并且 Doris 支持 PB 级别的存储和分析，对于存量历史数据较大、增量数据较少的公司来说，仅用 5-8 个节点就足以支撑上线使用。</p><p><strong>社区活跃</strong></p><p>截止目前，Apache Doris 已开源数年，并已支持全国超 1500 企业生产使用，其健壮性、稳定性不可否认。另外社区非常活跃，SelectDB 为社区组建了专职的技术支持团队，任何问题均能快速反馈，提供无偿技术支持，使用起来没有后顾之忧。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="运行架构"><strong>运行架构</strong><a href="#运行架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/35e83ee5515648779a8baaff2cc11d07~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>在对 Apache Doris 进一步测试验证之后，我们完全摒弃了之前使用 Hadoop、Hive、Spark 体系建立的数仓，决定基于 Doris 对架构进行重构，以 Apache Doris 作为数仓主体进行开发：</p><ul><li>数据集成：利用 DataX、Flink CDC 和 Apache Doris 的 Multi Catalog 功能等进行数据集成。</li><li>数据管理：利用 Apache Dolphinscheduler 进行脚本开发的生命周期管理、多租户人员的权限管理、数据质量监察等。</li><li>监控告警：采用 Grafana + Prometheus + Loki 进行监控告警，Doris 的各项监控指标可以在上面运行，解决了对组件资源和日志的监控问题。</li><li>数据服务：使用帆软 Report 为用户提供数据查询和分析服务，帆软支持表单制作和数据填报等功能，支持自助取数和自助分析。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据模型"><strong>数据模型</strong><a href="#数据模型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><strong>1）纵向分域</strong></p><p>房地产中介行业的大数据主题大致如下，一般会根据这些主题进行数仓建模。建模主题域核心围绕&quot;企业用户&quot;、&quot;客户&quot;、&quot;房源&quot;、&quot;组织&quot;等几个业务实体展开，进行维度表和事实表的创建。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c7ff29e7e87144959328feb348eeb645~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们从前线到后勤，对业务数据总线进行了梳理，旨在整理业务实体和业务活动相关数据，如多个系统之间存在同一个业务实体，应统一为一个字段。梳理业务总线有助于掌握公司整体数据结构，便于维度建模等工作。</p><p>下图为我们简单的梳理部分房地产中介行业的业务总线：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2efc7d4692664627a8bdd2efb54a8ef7~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>2）横向分层</strong></p><p>数据分层是最常见的 5 层结构主要是利用 Apache Doris + Apache DolphinScheduler 进行层级数据之间 DAG 脚本调度。</p><p><strong>存储策略：</strong> 我们在 8 点到 24 点之间采用增量策略，0 点到 8 点执行全量策略。采用增量 + 全量的方式是为了在ODS 表因为记录的历史状态字段变更或者 CDC 出现数据未完全同步的情况下，可以及时进行全量补数修正。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/17ff0792a5c140b8b8afc798561c1ad1~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>3）增量策略</strong></p><ol><li>where &gt;= &quot;业务时间-1天或-1小时&quot;</li></ol><p>增量的 SQL 语句不使用 <code>where=&quot;业务时间当天&quot;</code>的原因是为了避免数据漂移情况发生，换言之，调度脚本之间存在时间差，如 23:58:00 执行了脚本，脚本的执行周期是 10 分钟/次，但是源库最后一条数据 23:59:00 才进来，这时候 <code>where=&quot;业务时间当天&quot; </code>就会将该数据漏掉。</p><ol start="2"><li>每次跑增量脚本前获取表中最大的主键 ID 存入辅助表，<code>where &gt;= &quot;辅助表记录ID&quot;</code></li></ol><p>如果 Doris 表使用的是 Unique Key 模型，且恰好为组合主键，当主键组合在源表发生了变化，这时候 <code>where &gt;=&quot; 业务时间-1天&quot;</code>会记录该变化，把主键发生变化的数据 Load 进来，从而造成数据重复。而使用这种自增策略可有效避免该情况发生，且自增策略只适用于源表自带业务自增主键的情况。</p><ol start="3"><li>表分区</li></ol><p>如面对日志表等基于时间的自增数据，且历史数据和状态基本不会变更，数据量非常大，全量或快照计算压力非常大的场景，这种场景需要对 Doris 表进行建表分区，每次增量进行分区替换操作即可，同时需要注意数据漂移情况。</p><p><strong>4）全量策略</strong></p><ol><li>Truncate Table 清空表插入</li></ol><p>先清空表格后再把源表数据全量导入，该方式适用于数据量较小的表格和凌晨没有用户使用系统的场景。</p><ol start="2"><li><code>ALTER TABLE tbl1 REPLACE WITH TABLE tbl2 </code>表替换</li></ol><p>这种方式是一种原子操作，适合数据量大的全量表。每次执行脚本前先 Create 一张数据结构相同的临时表，把全量数据 Load 到临时表，再执行表替换操作，可以进行无缝衔接。</p><h1><strong>应用实践</strong></h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务模型">业务模型<a href="#业务模型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3e414110d34a460a869c9c0a8cf5a23c~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><ul><li>业务模型是分钟级调度 ETL</li><li>初次部署建议配置：8 节点 2FE * 8BE 混合部署</li><li>节点配置：32C <em> 60GB </em> 2TB SSD</li><li>对于存量数据 TB 级、增量数据 GB 级的场景完全够用，如有需要可以进行滚动扩容。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="具体应用"><strong>具体应用</strong><a href="#具体应用" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ol><li>离线数据和日志数据集成利用 DataX 进行增量和全量调度，Datax 支持 CSV 格式和多种关系型数据库的Redear，而 Doris 在很早之前就提供了 DataX Doris writer 连接器。</li></ol><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/71127fce1de54a518aed418af70faafe~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><ol start="2"><li>实时统计部分借助了 Flink CDC 对源表进行实时同步，利用 Doris 的物化视图或者 Aggregate 模型表进行实时指标的汇总处理，因我们只有部分指标需要实时处理，不希望产生过多的数据库连接和 Flink Job，因此我们使用 Dinky 的多源合并和整库同步功能，也可以自己简单实现一个Flink DataStream 多源合并任务，只通过一个 Job 可对多个 CDC 源表进行维护。值得一提的是， Flink CDC 和 Apache Doris 新版本支持 Schema Change 实时同步，在成本允许的前提下，可完全使用 CDC 的方式对 ODS 层进行改造。</li></ol><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">EXECUTE CDCSOURCE demo_doris WITH (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;connector&#x27; = &#x27;mysql-cdc&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;hostname&#x27; = &#x27;127.0.0.1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;port&#x27; = &#x27;3306&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;username&#x27; = &#x27;root&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;password&#x27; = &#x27;123456&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;checkpoint&#x27; = &#x27;10000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;scan.startup.mode&#x27; = &#x27;initial&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;parallelism&#x27; = &#x27;1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;table-name&#x27; = &#x27;ods.ods_*,ods.ods_*&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.connector&#x27; = &#x27;doris&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.fenodes&#x27; = &#x27;127.0.0.1:8030&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.username&#x27; = &#x27;root&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.password&#x27; = &#x27;123456&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.doris.batch.size&#x27; = &#x27;1000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.max-retries&#x27; = &#x27;1&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.batch.interval&#x27; = &#x27;60000&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.db&#x27; = &#x27;test&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.properties.format&#x27; =&#x27;json&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.properties.read_json_by_line&#x27; =&#x27;true&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.table.identifier&#x27; = &#x27;${schemaName}.${tableName}&#x27;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  &#x27;sink.sink.label-prefix&#x27; = &#x27;${schemaName}_${tableName}_1&#x27;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">);</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="3"><li>脚本语言采用 Shell + SQL 或纯 SQL 的形式，我们在 Apache DolphinScheduler 上进行脚本生命周期管理和发布，如 ODS 层，可以编写通用的 DataX Job 文件，通过传参的方式将 DataX Job 文件传参执行源表导入，无需在每一个源表编写不同的DataX Job ，支持统一配置参数和代码内容，维护起来非常方便。另外我们在 DolphinsSheduler 上对 Doris 的 ETL 脚本进行管理，还可以进行版本控制，能有效控制生产环境错误的发生，进行及时回滚。</li></ol><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/4ceb0780a1764df881c4ac6e9a3c7530~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/bfcfa5c5caf444ecbdf442685ef7065c~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><ol start="4"><li>发布 ETL 脚本后导入数据，可直接在帆软 Report 进行页面制作，基于登陆账号来控制页面权限，如需控制行级别、字段级别权限，可以制作全局字典，利用 SQL 方式进行控制。Doris 完全支持对账号的库表权限控制，这一点和 MySQL 的设置完全一样，使用起来非常便捷。</li></ol><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b8e4112e2e714a4cb721f733e22b0c65~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>除以上之外，在容灾恢复、集群监控、数据安全等方面也有应用，比如利用 Doris 备份实现容灾恢复、Grafana+Loki 对集群进行指标规则告警、Supervisor 对节点组件进行守护进程监控，开启 Doris 审计日志对执行 SQL 效率进行监控等，因篇幅限制，此处不进行详细说明。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="优化经验">优化经验<a href="#优化经验" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ol><li><strong>数据导入</strong></li></ol><p>我们使用 DataX 进行离线数据导入，DataX 采用的是 Stream Load 方式导入，该方式可以通过参数控制导入批次流量，DataX 导入不需要借助计算引擎，开箱即用的特点非常方便。另外，Stream Load 导入是同步返回结果的，其他导入方式一般是异步返回结果，针对我们的架构来说，在 Dolphinscheduler上执行异步导入数据会误以为该脚本已经执行成功，影响其正常运行。如采用其他异步导入方式，建议在 Shell 脚本中 执行<code> show load</code> 再利用正则过滤状态进行判断。</p><ol start="2"><li><strong>数据模型</strong></li></ol><p>我们所有层级的表模型大部分采用 Unique Key 模型，该模型可有效保证数据脚本的结果幂等性，Unique Key 模型可以完美解决上游数据重复的问题，大家可以根据业务模式来选择不同的模型建表。</p><ol start="3"><li><strong>外部数据源读取</strong></li></ol><p>Catalog 方式可以使用 JDBC 外表连接，还可以对 Doris 生产集群数据进行读取，便于生产数据直接 Load 进测试服务器进行测试。另外，新版支持多数据源的 Catalog，可以基于 Catalog 对 ODS 层进行改造，无需使用 DataX 对ODS 层进行导入。</p><ol start="4"><li><strong>查询优化</strong></li></ol><p>尽量把非字符类型（如 int 类型、where 条件）中最常用的字段放在前排 36 个字节内，在点查表过程中可以快速过滤这些字段（毫秒级别），可以充分利用该特性进行数据表输出。</p><ol start="5"><li><strong>数据字典</strong></li></ol><p>利用 Doris 自带的 <code>information_schema</code> 元数据制作简单的数据字典，这在还未建立数据治理体系前非常重要，当部门人数较多的时候，沟通成本成为发展过程中最大的“拦路虎”，利用数据字典可快速对表格和字段的全局查找和释义，最低成本形成数仓人员的数据规范，减少人员沟通成本，提高开发效率。</p><h1>架构收益</h1><ul><li>自动取数导数：数据仓库的明细表可以定时进行取数、导数，自助组合维度进行分析。</li><li>效率提升：T+1 的离线时效从小时计降低至分钟级</li><li>查询延迟降低：面对上亿行数据的表，利用 Doris 在索引和点查方面的能力，实现即席查询 1 秒内响应，复杂查询 5 秒内响应。</li><li>运维成本降低：从数据集成到数据服务，只需维护少数组件就可以实现整体链路高效管理。</li><li>数据管理体系：Doris 数仓的搭建，使得数据管理体系初步形成，数据资产得以规范化的沉淀。</li><li>资源节省：只用了少数服务器，快速搭建起一套数据仓库，成功实现降本赋能。同时 Doris 超高的压缩比，将数据压缩了 70%，相较于 Hadoop 来说，存储资源的消耗大幅降低。</li></ul><h1>总结与规划</h1><p>目前我们已经完成数仓建设的初期目标，未来我们有计划基于 Apache Doris 进行中台化的改造，同时 Apache Doris在用户画像和人群圈选场景的能力十分强悍，支持 Bitmap 等格式进行导入和转换，提供了丰富的 Bitmap 分析函数等，后续我们也将利用这部分能力进行客户群体分析，加快数字化转型。</p><p>最后，感谢 Apache Doris 社区和 SelectDB 团队对美联物业的快速响应和无偿支持，希望 Doris 发展越来越好，也希望更多的企业可以尝试使用 Apache Doris。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Douyu">赋能直播行业精细化运营，斗鱼基于 Apache Doris 的应用实践</a></h2><div class="blog-info"><time datetime="2023-05-05T00:00:00.000Z" itemprop="datePublished">2023年5月5日</time><span class="split-line"></span><span class="authors"><span class="s-author">韩同阳</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong> 斗鱼是一家弹幕式直播分享网站，为用户提供视频直播和赛事直播服务。随着斗鱼直播、视频等业务的高速发展，用户增长和营收两大主营业务线对精细化运营的需求越发地迫切，各个细分业务场景对用户的差异化分析诉求也越发的强烈。为更好满足业务需求，斗鱼在 2022 年引入了 <a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">Apache Doris</a> 构建了一套比较相对完整的实时数仓架构，并在该基础上成功构建了标签平台以及多维分析平台，在此期间积累了一些建设及实践经验通过本文分享给大家。</p><p>作者<strong>｜</strong>斗鱼资深大数据工程师、OLAP 平台负责人 韩同阳</p><p>斗鱼是一家弹幕式直播分享网站，为用户提供视频直播和赛事直播服务。斗鱼以游戏直播为主，也涵盖了娱乐、综艺、体育、户外等多种直播内容。随着斗鱼直播、视频等业务的高速发展，用户增长和营收两大主营业务线对精细化运营的需求越发地迫切，各个细分业务场景对用户的差异化分析诉求也越发的强烈，例如增长业务线需要在各个活动（赛事、专题、拉新、招募等）中针对不同人群进行差异化投放，营收业务线需要根据差异化投放的效果及时调整投放策略。  </p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f85dc99bc9fd48cd9db1bd93faf51e23~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>根据业务场景的诉求和精细化运营的要求，我们从金字塔自下而上来看，需求大致可以分为以下几点：</p><ul><li>分析需求更加复杂、精细化，不再满足简单的聚合分析；数据时效性要求更高，不满足于 T+1 的分析效率，期望实现近实时、实时的分析效率。</li><li>业务场景多，细分业务场景既存在独立性、又存在交叉性，例如：针对某款游戏进行专题活动投放（主播、用户），进行人群圈选、AB 实验等，需要标签/用户画像平台支持。</li><li>多维数据分析的诉求强烈，需要精细化运营的数据产品支持。</li></ul><p><strong>为更好解决上述需求，我们的初步目标是：</strong></p><ul><li>构建离线/实时数仓，斗鱼的离线数仓体系已成熟，希望此基础上构建一套实时数仓体系；</li><li>基于离线/实时数仓构建通用的标签中台（用户画像平台），为业务场景提供人群圈选、AB实验等服务；</li><li>在标签平台的基础上构建适用于特定业务场景的多维分析和精细化运营的数据产品。</li></ul><p>在目标驱动下，斗鱼在原有架构的基础上进行升级改造、<strong>引入 <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a> 构建了实时数仓体系，并在该基础上成功构建了标签平台以及多维分析平台</strong>，在此期间积累了一些建设及实践经验通过本文分享给大家。</p><h1>原有实时数仓架构</h1><p>斗鱼从 2018 年开始探索实时数仓的建设，并尝试在某些垂直业务领域应用，但受制于人力的配置及流计算组件发展的成熟度，直到 2020 年第一版实时数据架构才构建完成，架构图如下图所示：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/894c5aa438e64fb88dc8865a1a725720~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>原有实时数仓架构是一个典型的 Lambda 架构，上方链路为离线数仓架构，下方链路为实时数据仓架构。鉴于当时离线数仓体系已经非常成熟，使用 Lambda 架构足够支撑实时分析需求，但随着业务的高速发展和数据需求的不断提升，<strong>原有架构凸显出几个问题</strong>：</p><ul><li>在实际的流式作业开发中，缺乏对实时数据源的管理，在极端情况下接近于烟囱式接入实时数据流，无法关注数据是否有重复接入，也无法辨别数据是否可以复用。</li><li>离线、实时数仓完全割裂，实时数仓没有进行数仓分层，无法像离线数仓按层复用，只能面向业务定制化开发。</li><li>数据仓库数据服务于业务平台需要多次中转，且涉及到多个技术组件，ToB 应用亟需引入 OLAP 引擎缓解压力。</li><li>计算引擎和存储引擎涉及技术栈多，学习成本和运维难度也很大，无法进行合理有效管理。</li></ul><h1>新实时数仓架构</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="技术选型"><strong>技术选型</strong><a href="#技术选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>带着以上的问题，我们希望引入一款成熟的、在业内有大规模落地经验的 OLAP 引擎来帮助我们解决原有架构的痛点。我们希望该 OLAP 引擎不仅要具备传统 OLAP 的优势（即 Data Analytics），还能更好地支持数据服务（Data Serving）场景，比如标签数据需要明细级的查询、实时业务数据需要支持点更新、高并发以及大数据量的复杂 Join 。除此之外，我们希望该 OLAP 引擎可以便捷、低成本的的集成到 Lambda 架构下的离线/实时数仓架构中。<strong>立足于此，我们在技术选型时对比了市面上的几款 OLAP 引擎，如下图所示</strong>：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/eecaaaf523ba4fefb77cf2ebc1871357~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>根据对选型的要求，<strong>我们发现 Apache Doris 可以很好地满足当前业务场景及诉求，同时也兼顾了低成本的要求，因此决定引入 Doris 进行升级尝试</strong>。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="架构设计"><strong>架构设计</strong><a href="#架构设计" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们在 2022 年引入了 Apache Doris ，并基于 Apache Doris 构建了一套比较相对完整的实时数仓架构，如下图所示。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c4b304cb38ff4582b3292c52af2ba280~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>总的来说，引入 Doris 后为整体架构带来几大变化：</p><ul><li>统一了计算平台（玄武计算），底层引擎支持 Flink、Spark 等组件，接入层支持统一 SQL 和 JAR 包接入。</li><li>引入 Doris 后，我们将实时数仓分为 ODS、DWD、DWS、ADS 层，部分中间层实时数据直接使用 Doris 进行存储；</li><li>构建了基于 Doris 的 HOLAP 多维分析平台，直接服务于业务；简化了原来需要通过 Hive 进行预计算的加工链路，逐步替换使用难度和运维难度相对较高的 ClickHouse；</li><li>下游应用的数据存储从之前的 MySQL 和 HBase 更换为 Doris，可以在数据集市和大宽表的数据服务场景下直接查询 Doris。</li><li>支持混合 IDC（自建和云厂商）。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="overwrite-语义实现">Overwrite 语义实现<a href="#overwrite-语义实现" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Apache Doris 支持原子替换表和分区，我们在计算平台（玄武平台）整合 Doris Spark Connector 时进行了定制，且在 Connector 配置参数上进行扩展、增加了“Overwrite”模式。</p><p>当 Spark 作业提交后会调用 Doris 的接口，获取表的 Schema 信息和分区信息。</p><ul><li>如果为非分区表：先创建目标表对应的临时表，将数据导入到临时表中，导入后进行原子替换，如导入失败则清理临时表；</li><li>如果是动态分区表：先创建目标分区对应的临时分区，将数据导入临时分区，导入后进行原子替换，如导入失败则清理临时分区；</li><li>如果是非动态分区：需要扩展 Doris Spark Connector 参数配置分区表达式，配完成后先创建正式目标分区、再创建其临时分区，将数据导入到临时分区中，导入后进行原子替换，如导入失败则清理临时分区。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="架构收益">架构收益<a href="#架构收益" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>通过架构升级及二次开发，我们获得了 3 个明显的收益：</p><ul><li>构建了规范、完善、计算统一的实时数仓平台</li><li>构建了统一混合 OLAP 平台，既支持 MOLAP，又支持 ROLAP，大部分多维分析需求均由该平台实现。</li><li>面对大批量数据导入的场景，任务吞入率和成功率提升了 50%。</li></ul><h1>Doris 在标签中台的应用</h1><p>标签中台（也称用户画像平台）是斗鱼进行精准运营的重要平台之一，承担了各业务线人群圈选、规则匹配、A/B 实验、活动投放等需求。接下来看下 Doris 在标签中台是如何应用的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="原标签中台架构">原标签中台架构<a href="#原标签中台架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a139e7f853d246f2a482877f6695dfc9~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>上图为斗鱼原来的标签中台架构，离线标签在数仓中加工完成后合入宽表，将最终数据写入 HBase 中，实时标签使用 Flink 加工，加工完直接写入到 HBase 中。</p><p>终端 APP 在使用标签中台时，主要解决两种业务需求：</p><ul><li>人群圈选，即通过标签和规则找到符合条件的人。</li><li>规则匹配，即当有一个用户，找出该用户在指定的业务场景下符合哪些已配置的规则，也可以理解是“人群圈选“的逆方向。</li></ul><p>在应对这两种场需求中，原标签中台架构出现了两个问题：</p><ul><li>实时标签链路：Flink 计算长周期实时指标时稳定性较差且耗费资源较高，任务挂掉之后由于数据周期较长，导致 Checkpoint 恢复很慢；</li><li>人群圈选：Spark 人群圈选效率较低，特别是在实时标签的时效性上。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新标签中台架构">新标签中台架构<a href="#新标签中台架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/264534a90f5a4b0db757bbd97cbf0bba~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>引入 Apache Doris 之后，我们对标签中台架构的进行了改进，主要改进集中在实时链路和标签数据存储这两个部分：</p><ul><li>实时标签链路：仍然是通过实时数据源到 Kafka 中，通过 Flink 进行实时加工；不同的是，我们将一部分加工逻辑迁移到 Doris 中进行计算，长周期实时指标的计算从单一的 Flink 计算转移到了 Flink + Doris 中进行；</li><li>标签数据存储：从 HBase 改成了 Doris，利用 Doris 聚合模型的部分更新特性，将离线标签和实时标签加工完之后直接写入到 Doris 中。</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="1-离线实时标签混合圈人"><strong>1. 离线/实时标签混合圈人</strong><a href="#1-离线实时标签混合圈人" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ea2b504bfb3a425c8191f9c5b49695cd~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><ul><li>简化存储：原存储在 HBase 中的大宽表，改为在 Doris 中分区存储，其中离线标签 T+1 更新，实时标签 T 更新、T+1 采用离线数据覆盖矫正。</li><li>查询简化：<strong>面对人群圈选场景</strong>，无需利用 Spark 引擎，可直接在标签中台查询服务层，将圈选规则配置解析成 SQL 在 Doris 中执行、并获得最终的人群，大大提高了人群圈选的效率。<strong>面对规则匹配场景</strong>，使用 Redis 缓存 Doris 中的热点数据，以降低响应时间。</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="2-长周期实时标签计算原链路"><strong>2. 长周期实时标签计算原链路</strong><a href="#2-长周期实时标签计算原链路" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>长周期实时标签：计算实时标签时所需的数据周期较长，部分标签还需要采用历史数据（离线）合并实时数据流一起进行计算的场景。</p><p><strong>使用前：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/c160d25ef8f84729b04da093f0ee98e6~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>从原来的计算链路中可知，计算长周期的实时标签时会涉及到维度补充、历史数据 Merge，在经过几步加工最终将数据写入到 HBase 中。</p><p>在实际使用中发现，在这个过程中 Merge 离线聚合的数据会使链路变得很复杂，往往一个实时标签需要多个任务参与才能完成计算；另外聚合逻辑复杂的实时标签一般需要多次聚合计算，任意一个中间聚合资源分配不够或者不合理，都有可能出现反压或资源浪费的问题，从而使整个任务调试起来特别困难，同时链路过长运维管理也很麻烦，稳定性也比较低。</p><p><strong>使用后：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/156b5b5531ac4202943554b23e8455a6~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>我们在长周期指标计算实时链路中加入了 Apache Doris，在  Flink 中只做维度补充和轻度加工汇总，只关注短的实时数据流，对于需要 Merge 的离线数据，Merge 的计算逻辑转移到 Doris 中进行计算，另外 Doris 中的轻度汇总/明细数据有助于问题排查，同时任务稳定性也能提升。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="使用收益">使用收益<a href="#使用收益" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>目前标签中台底层有近 4 亿+条用户标签，每个用户标签 300+，已有 1W+ 用户规则人群，每天定时更新的人群数量达到 5K+。标签中台引入 Apache Doris 之后，<strong>单个人群平均圈选时间实现了分钟级到秒级的跨越</strong>，实时标签任务稳定性有所提高，实时标签任务的产出时间相较于之前约有 <strong>40% 的提升</strong>，资源使用成本大大降低。</p><h1>Doris 在多维数据分析平台的应用</h1><p>除以上所述应用及收益之外，Apache Doris 也助力内部多维数据分析平台——斗鱼 360 取得了较大的发展，受益于 Apache Doris 的 Rollup、物化视图以及向量化执行引擎，使原来需要预计算的场景可以直接导入明细数据到 Doris 中，简化了业务数据开发流程，提升了分析效率；Doris 兼容 MySQL 协议，并具有独立简单的分布式架构，使得业务开发人员入门使用也更容易，缩短了业务开发周期，有效降低了开发成本；同时我们原来基于 ClickHouse 的查询目前也全部切换到了 Doris 中进行。</p><p>目前我们用于多维分析场景的 Doris 集群共有两个，节点规模约 120 个，存储数据量达 90~100 TB，每天增量写入到 Doris 的数据约 900GB，其中查询 QPS 在 120 左右，Apache Doris 应对起来毫不费力，轻松自如。</p><p><em>因文章篇幅限制，该部分应用不再赘述，后续有机会与大家进行详细分享。</em>  </p><h1>未来展望</h1><p>未来随着 Apache Doris 在斗鱼更广泛业务场景的落地，我们将在可视化运维、问题快速定位排查等方面进行更多实践和深耕。我们关注到， Apache Doris 1.2.0 版本已经对 Multi Catalog 功能进行了支持，我们也计划对其进行探索、解锁更多使用场景，同时也期待即将发布 Apache Doris 2.x 版本的行列混存功能，更好的支持 Data Serving 场景。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/HYXJ">杭银消金基于 Apache Doris  的统一数据查询网关改造</a></h2><div class="blog-info"><time datetime="2023-04-20T00:00:00.000Z" itemprop="datePublished">2023年4月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">杭银消金大数据团队</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong> 随着业务量快速增长，数据规模的不断扩大，杭银消金早期的大数据平台在应对实时性更强、复杂度更高的的业务需求时存在瓶颈。为了更好的应对未来的数据规模增长，杭银消金于 2022 年 10 月正式引入 Apache Doris 1.2 对现有的风控数据集市进行了升级改造，利用 Multi Catalog 功能统一了 ES、Hive、GP 等数据源出口，实现了联邦查询，为未来统一数据查询网关奠定了基础；同时，基于 <a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">Apache Doris</a> 高性能、简单易用、部署成本低等诸多优势，也使得各大业务场景的查询分析响应实现了从分钟级到秒级的跨越。</p><p>杭银消费金融股份有限公司，成立于 2015 年 12 月，是杭州银行牵头组建的浙江省首家持牌消费金融公司，经过这几年的发展，在 2022 年底资产规模突破 400 亿，服务客户数超千万。公司秉承“数字普惠金融”初心，坚持服务传统金融覆盖不充分的、具有消费信贷需求的客户群体，以“<strong>数据、场景、风控、技术</strong>”为核心，依托大数据、人工智能、云计算等互联网科技，为全国消费者提供专业、高效、便捷、可信赖的金融服务。</p><h1><strong>业务需求</strong></h1><p>杭银消金业务模式是线上业务结合线下业务的双引擎驱动模式。为更好的服务用户，运用数据驱动实现精细化管理，基于当前业务模式衍生出了四大类的业务数据需求：</p><ul><li>预警类：实现业务流量监控，主要是对信贷流程的用户数量与金额进行实时监控，出现问题自动告警。</li><li>分析类：支持查询统计与临时取数，对信贷各环节进行分析，对审批、授信、支用等环节的用户数量与额度情况查询分析。</li><li>看板类：打造业务实时驾驶舱与 T+1 业务看板，提供内部管理层与运营部门使用，更好辅助管理进行决策。</li><li>建模类：支持多维模型变量的建模，通过算法模型回溯用户的金融表现，提升审批、授信、支用等环节的模型能力。</li></ul><h1>数据架构 1.0</h1><p>为满足以上需求，我们采用 Greenplum + CDH 融合的架构体系创建了大数据平台 1.0 ，如下图所示，大数据平台的数据源均来自于业务系统，我们可以从数据源的 3 个流向出发，了解大数据平台的组成及分工：</p><ul><li>业务系统的核心系统数据通过 CloudCanal 实时同步进入 Greenplum 数仓进行数据实时分析，为 BI 报表，数据大屏等应用提供服务，部分数据进入风控集市 Hive 中，提供查询分析和建模服务。</li><li>业务系统的实时数据推送到 Kafka 消息队列，经 Flink 实时消费写入 ES，通过风控变量提供数据服务，而 ES 中的部分数据也可以流入 Hive 中，进行相关分析处理。</li><li>业务系统的风控数据会落在 MongoDB，经过离线同步进入风控集市 Hive，Hive 数仓支撑了查询平台和建模平台，提供风控分析和建模服务。</li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/669b73289fd8400cbc067b2b11f887d9~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>我们将 ES 和</strong> <strong>Hive</strong> <strong>共同组成了风控数据集市</strong>，从上述介绍也可知，四大类的业务需求基本都是由风控数据集市来满足的，因此我们后续的改造升级主要基于风控数据集市来进行。在这之前，我们先了解一下风控数据集市 1.0 是如何来运转的。</p><p><strong>风控数据集市 1.0</strong></p><p>风控数据集市原有架构是基于 CDH 搭建的，由实时写入和离线统计分析两部分组成，整个架构包含了 ES、Hive、Greenplum 等核心组件，风控数据集市的数据源主要有三种：通过 Greenplum 数仓同步的业务系统数据、通过 MongoDB 同步的风控决策数据，以及通过 ES 写入的实时风控变量数据。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/1a7f283f3e514b6c8c1b7672ce520a45~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>实时流数据：</strong> 采用了 Kafka + Flink + ES 的实时流处理方式，利用 Flink 对 Kafka 的实时数据进行清洗，实时写入ES，并对部分结果进行汇总计算，通过接口提供给风控决策使用。</p><p><strong>离线风控数据：</strong> 采用基于 CDH 的方案实现，通过 Sqoop 离线同步核心数仓 GP 上的数据，结合实时数据与落在 MongoDB 上的三方数据，经数据清洗后统一汇总到 Hive 数仓进行日常的跑批与查询分析。</p><p><strong>需求满足情况：</strong></p><p>在大数据平台 1.0 的的支持下，我们的业务需求得到了初步的实现：</p><ul><li>预警类：基于 ES + Hive 的外表查询，实现了实时业务流量监控；</li><li>分析类：基于 Hive 实现数据查询分析和临时取数；</li><li>看板类：基于 Tableau +Hive 搭建了业务管理驾驶舱以及T+1 业务看板；</li><li>建模类：基于 Spark+Hive 实现了多维模型变量的建模分析；</li></ul><p>受限于 Hive 的执行效率，以上需求均在分钟级别返回结果，仅可以满足我们最基本的诉求，而面对秒级甚至毫秒级的分析场景，Hive 则稍显吃力。</p><p><strong>存在的问题：</strong></p><ul><li><strong>单表宽度过大，影响查询性能</strong>。风控数据集市的下游业务主要以规则引擎与实时风控服务为主，因规则引擎的特殊性，公司在数据变量衍生方面资源投入较多，某些维度上的衍生变量会达到几千甚至上万的规模，这将导致 Hive 中存储的数据表字段非常多，部分经常使用的大宽表字段数量甚至超过上千，过宽的大宽表非常影响实际使用中查询性能。</li><li><strong>数据规模庞大，维护成本高。</strong> 目前 Hive 上的风控数据集市已经有存量数据在百 T 以上，面对如此庞大的数据规模，使用外表的方式进行维护成本非常高，数据的接入也成为一大难题。</li><li><strong>接口服务不稳定。</strong> 由风控数据集市离线跑批产生的变量指标还兼顾为其他业务应用提供数据服务的职责，目前 Hive 离线跑批后的结果会定时推送到 ES 集群（每天更新的数据集比较庞大，接口调用具有时效性），推送时会因为 IO 过高触发 ES 集群的 GC 抖动，导致接口服务不稳定。</li></ul><p>除此之外，风控分析师与建模人员一般通过 Hive &amp; Spark 方式进行数据分析建模，这导致随着业务规模的进一步增大，T+1 跑批与日常分析的效率越来越低，风控数据集市改造升级的需求越发强烈。</p><h1>技术选型</h1><p>基于业务对架构提出的更高要求，我们期望引入一款强劲的 OLAP 引擎来改善架构，因此我们于 2022 年 9 月份对 ClickHouse 和 Apache Doris 进行了调研，调研中发现 Apache Doris 具有高性能、简单易用、实现成本低等诸多优势，而且 Apache Doris 1.2 版本非常符合我们的诉求，原因如下：</p><p><strong>宽表查询性能优异</strong>：从官方公布的测试结果来看，1.2 Preview 版本在 SSB-Flat 宽表场景上相对 1.1.3 版本整体性能提升了近 4 倍、相对于 0.15.0 版本性能提升了近 10 倍，在 TPC-H 多表关联场景上较 1.1.3 版本上有近 3 倍的提升、较 0.15.0 版本性能提升了 11 倍以上，多个场景性能得到飞跃性提升。</p><p><strong>便捷的数据接入框架以及联邦数据分析能力：</strong> Apache Doris 1.2 版本推出的 Multi Catalog 功能可以构建完善可扩展的数据源连接框架，<strong>便于快速接入多类数据源，提供基于各种异构数据源的联邦查询和写入能力。</strong> 目前 Multi-Catalog 已经支持了 Hive、Iceberg、Hudi 等数据湖以及 MySQL、Elasticsearch、Greenplum 等数据库，全面覆盖了我们现有的组件栈，基于此能力有希望通过 Apache Doris 来打造统一数据查询网关。</p><p><strong>生态丰富：</strong> 支持 Spark Doris Connector、Flink Doris Connector，方便离线与实时数据的处理，缩短了数据处理链路耗费的时间。</p><p><strong>社区活跃：</strong> Apache Doris 社区非常活跃，响应迅速，并且 SelectDB 为社区提供了一支专职的工程师团队，为用户提供技术支持服务。</p><h1>数据架构 2.0</h1><p><strong>风控数据集市 2.0</strong></p><p>基于对 Apache Doris 的初步的了解与验证，22 年 10 月在社区的支持下我们正式引入 Apache Doris 1.2.0 Preview 版本作为风控数据集市的核心组件，Apache Doris 的 Multi Catalog 功能助力大数据平台统一了 ES、Hive、Greenplum 等数据源出口，通过 Hive Catalog 和 ES Catalog 实现了对 Hive &amp; ES 等多数据源的联邦查询，并且支持 Spark-Doris-Connector，可以实现数据 Hive 与 Doris 的双向流动，与现有建模分析体系完美集成，在短期内实现了性能的快速提升。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/7f0597006e834176b50f230c58860e9b~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>大数据平台 2.0</strong></p><p>风控数据集市调整优化之后，大数据平台架构也相应的发生了变化，如下图所示，仅通过 Doris 一个组件即可为数据服务、分析平台、建模平台提供数据服务。</p><p>在最初进行联调适配的时候，Doris 社区和 SelectDB 支持团队针对我们提出的问题和疑惑一直保持高效的反馈效率，给于积极的帮助和支持，快速帮助我们解决在生产上遇到的问题。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2abdee1b58a144528a3cd7d52ee948bc~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>需求实现情况：</strong></p><p>在大数据平台 2.0 的加持下，业务需求实现的方式也发生了变更，主要变化如下所示</p><ul><li>预警类：基于 ES Catalog+ Doris 实现了对实时数据的查询分析。在架构 1.0 中，实时数据落在 ES 集群上，通过 Hive 外表进行查询分析，查询结果以分钟级别返回；而在 Doris 1.2 集成之后， 使用 ES Catalog 访问 ES，可以实现对 ES 数据秒级统计分析。</li><li>分析类：基于 Hive Catalog + Doris 实现了对现有风控数据集市的快速查询。目前 Hive 数据集市存量表在两万张左右，如果通过直接创建 Hive 外部表的方式，表结构映射关系的维护难度与数据同步成本使这一方式几乎不可能实现。而 Doris 1.2 的 Multi Catalog 功能则完美解决了这个问题，只需要创建一个 Hive Catalog，就能对现有风控数据集市进行查询分析，既能提升查询性能，还减少了日常查询分析对跑批任务的资源影响。</li><li>看板类：基于 Tableau + Doris 聚合展示业务实时驾驶舱和 T+1 业务看板，最初使用 Hive 时，报表查询需要几分钟才能返回结果，而 Apache Doris 则是秒级甚至是毫秒级的响应速度。</li><li>建模类：基于 Spark+Doris 进行聚合建模。利用 Doris1.2 的 Spark-Doris-Connector功 能，实现了 Hive 与 Doris 数据双向同步，满足了 Spark 建模平台的功能复用。同时增加了 Doris 数据源，基础数据查询分析的效率得到了明显提升，建模分析能力的也得到了增强。</li></ul><p>在 Apache Doris 引入之后，以上四个业务场景的查询耗时基本都实现了从分钟级到秒级响应的跨越，性能提升十分巨大。</p><p><strong>生产环境集群监控</strong></p><p>为了快速验证新版本的效果，我们在生产环境上搭建了两个集群，目前生产集群的配置是 4 个 FE + 8个 BE，单个节点是配置为 64 核+ 256G+4T，备用集群为 4 个 FE + 4 个 BE 的配置，单个节点配置保持一致。</p><p>集群监控如下图所示：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/dfbf810e93e346d2bbde7d80fddb3f59~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>可以看出，Apache Doris 1.2 的查询效率非常高，原计划至少要上 10 个节点，而在实际使用下来，我们发现当前主要使用的场景均是以 Catalog 的方式查询，因此集群规模可以相对较小就可以快速上线，也不会破坏当前的系统架构，兼容性非常好。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据集成方案">数据集成方案<a href="#数据集成方案" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>前段时间，Apache Doris 1.2.2 版本已经发布，为了更好的支撑应用服务，我们使用 Apache Doris 1.2.2 与 DolphinScheduler 3.1.4 调度器、SeaTunnel 2.1.3 数据同步平台等开源软件实现了集成，以便于数据定时从 Hive 抽取到 Doris 中。整体的数据集成方案如下：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8b029412dbb143369935fdcaa71bcc19~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>在当前的硬件配置下，数据同步采用的是 DolphinScheduler 的 Shell 脚本模式，定时调起 SeaTunnel 的脚本，数据同步任务的配置文件如下：</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain"> env{</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">app</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">name </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hive2doris-template&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">instances </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">cores </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">5</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">executor</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">memory </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;20g&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">spark {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  spark</span><span class="token punctuation" style="color:#393A34">.</span><span class="token keyword" style="color:#00009f">sql</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">catalogImplementation </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;hive&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">source {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  hive {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    pre_sql </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;select * from ods.demo_tbl where dt=&#x27;2023-03-09&#x27;&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    result_table_name </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;ods_demo_tbl&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">transform {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"> </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">sink {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">  doris {</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      fenodes </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;192.168.0.10:8030,192.168.0.11:8030,192.168.0.12:8030,192.168.0.13:8030&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token keyword" style="color:#00009f">user</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> root</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      password </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;XXX&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token keyword" style="color:#00009f">database</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ods</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token keyword" style="color:#00009f">table</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> ods_demo_tbl</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      batch_size </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">500000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      max_retries </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">1</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      </span><span class="token keyword" style="color:#00009f">interval</span><span class="token plain"> </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token number" style="color:#36acaa">10000</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">      doris</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">column_separator </span><span class="token operator" style="color:#393A34">=</span><span class="token plain"> </span><span class="token string" style="color:#e3116c">&quot;\t&quot;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    }</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">}</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p><strong>该方案成功实施后，资源占用、计算内存占用有了明显的降低，查询性能、导入性能有了大幅提升：</strong></p><ol><li>存储成本降低</li></ol><p>使用前：Hive 原始表包含 500 个字段，单个分区数据量为 1.5 亿/天，在 HDFS 上占用约 810G 的空间。</p><p>使用后：我们通过 SeaTunnel 调起 Spark on YARN 的方式进行数据同步，可以在 <strong>40 分钟左右</strong>完成数据同步，同步后数据占用 <strong>270G 空间，存储资源仅占之前的 1/3</strong>。</p><ol start="2"><li>计算内存占用降低，性能提升显著</li></ol><p>使用前：上述表在 Hive 上进行 Group By 时，占用 YARN 资源 720 核 1.44T 内存，需要 <strong>162 秒</strong>才可返回结果；</p><p>使用后：</p><ul><li>通过 Doris 调用 Hive Catalog 进行聚合查询，在设置 <code>set exec_mem_limit=16G</code> 情况下用时 <strong>58.531 秒，查询耗时较之前减少了近 2/3；</strong></li><li>在同等条件下，在 Doris 中执行相同的的操作可以在 <strong>0.828 秒</strong>就能返回查询结果，性能增幅巨大。</li></ul><p>具体效果如下：</p><p>（1）Hive 查询语句，用时 162 秒。</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">select</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">product_no   </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> ods</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">demo_tbl </span><span class="token keyword" style="color:#00009f">where</span><span class="token plain"> dt</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;2023-03-09&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">group</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">by</span><span class="token plain"> product_no</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>（2）Doris 上 Hive Catalog 查询语句，用时 58.531 秒。</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">set</span><span class="token plain"> exec_mem_limit</span><span class="token operator" style="color:#393A34">=</span><span class="token number" style="color:#36acaa">16</span><span class="token plain">G；</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">select</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">product_no   </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> hive</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">ods</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">demo_tbl </span><span class="token keyword" style="color:#00009f">where</span><span class="token plain"> dt</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;2023-03-09&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">group</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">by</span><span class="token plain"> product_no</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>（3）Doris 上本地表查询语句，<strong>仅用时0.828秒</strong>。</p><div class="language-sql codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-sql codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token keyword" style="color:#00009f">select</span><span class="token plain"> </span><span class="token function" style="color:#d73a49">count</span><span class="token punctuation" style="color:#393A34">(</span><span class="token operator" style="color:#393A34">*</span><span class="token punctuation" style="color:#393A34">)</span><span class="token punctuation" style="color:#393A34">,</span><span class="token plain">product_no   </span><span class="token keyword" style="color:#00009f">FROM</span><span class="token plain"> ods</span><span class="token punctuation" style="color:#393A34">.</span><span class="token plain">demo_tbl </span><span class="token keyword" style="color:#00009f">where</span><span class="token plain"> dt</span><span class="token operator" style="color:#393A34">=</span><span class="token string" style="color:#e3116c">&#x27;2023-03-09&#x27;</span><span class="token plain"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain"></span><span class="token keyword" style="color:#00009f">group</span><span class="token plain"> </span><span class="token keyword" style="color:#00009f">by</span><span class="token plain"> product_no</span><span class="token punctuation" style="color:#393A34">;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="3"><li>导入性能提升</li></ol><p>使用前：Hive 原始表包含 40 个字段，单个分区数据量 11 亿/天，在 HDFS 上占用约 806G 的空间</p><p>使用后：通过 SeaTunnel 调起 Spark on YARN 方式进行数据同步，可以在 11 分钟左右完成数据同步，即 <strong>1 分钟同步约一亿条数据</strong>，同步后占用 378G 空间。</p><p>可以看出，在数据导入性能的提升的同时，资源也有了较大的节省，主要得益于对以下几个参数进行了调整：</p><p><code>push_write_mbytes_per_sec</code>：BE 磁盘写入限速，300M</p><p><code>push_worker_count_high_priority:</code> 同时执行的 push 任务个数，15</p><p><code>push_worker_count_normal_priority</code>: 同时执行的 push 任务个数，15</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0e9b5d64898843afb26b60a3d5dfb705~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="架构收益"><strong>架构收益</strong><a href="#架构收益" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><strong>（1）统一数据源出口，查询效率显著提升</strong></p><p>风控数据集市采用的是异构存储的方式来存储数据，Apache Doris 的 Multi Catalog 功能成功统一了 ES、Hive、GP 等数据源出口，实现了联邦查询。 同时，Doris 本身具有存储能力，可支持其他数据源中的数据通过外表插入内容的方式快速进行数据同步，真正实现了数据门户。此外，Apache Doris 可支持聚合查询，在向量化引擎的加持下，查询效率得到显著提升。</p><p><strong>（2）</strong> <strong>Hive</strong> <strong>任务拆分，提升集群资源利用率</strong></p><p>我们将原有的 Hive 跑批任务跟日常的查询统计进行了隔离，以提升集群资源的利用效率。目前 YARN 集群上的任务数量是几千的规模，跑批任务占比约 60%，临时查询分析占比 40%，由于资源限制导致日常跑批任务经常会因为资源等待而延误，临时分析也因资源未及时分配而导致任务无法完成。当部署了 Doris 1.2 之后，对资源进行了划分，完全摆脱 YARN 集群的资源限制，跑批与日常的查询统计均有了明显的改善，<strong>基本可以在秒级得到分析结果</strong>，同时也减轻了数据分析师的工作压力，提升了用户对平台的满意度。</p><p><strong>（3）提升了数据接口的稳定性，数据写入性能大幅提升</strong></p><p>之前数据接口是基于 ES 集群的，当进行大批量离线数据推送时会导致 ES 集群的 GC 抖动，影响了接口稳定性，经过调整之后，我们将接口服务的数据集存储在 Doris 上，Doris 节点并未出现抖动，实现数据快速写入，成功提升了接口的稳定性，同时 Doris 查询在数据写入时影响较小，数据写入性能较之前也有了非常大的提升，<strong>千万级别的数据可在十分钟内推送成功</strong>。</p><p><strong>（4）Doris 生态丰富，迁移方便成本较低。</strong></p><p>Spark-Doris-Connector 在过渡期为我们减轻了不少的压力，当数据在 Hive 与 Doris 共存时，部分 Doris 分析结果通过 Spark 回写到 Hive 非常方便，当 Spark 调用 Doris 时只需要进行简单改造就能完成原有脚本的复用，迁移方便、成本较低。</p><p><strong>（5）支持横向热部署，集群扩容、运维简单。</strong></p><p>Apache Doris 支持横向热部署，集群扩容方便，节点重启可以在在秒级实现，可实现无缝对接，减少了该过程对业务的影响； 在架构 1.0 中，当 Hive 集群与 GP 集群需要扩容更新时，配置修改后一般需要较长时间集群才可恢复，用户感知比较明显。而 Doris 很好的解决了这个问题，实现用户无感知扩容，也降低了集群运维的投入。</p><h1><strong>未来与展望</strong></h1><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6fe9b6019532451e814e1f709a23d510~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>当前在架构 2.0 中的 Doris 集群在大数据平台中的角色更倾向于查询优化，大部分数据还集中维护在 Hive 集群上，未来我们计划在升级架构 3.0 的时候，完成以下改造：</p><ul><li>实时全量数据接入：利用 Flink 将所有的实时数据直接接入 Doris，不再经过 ES 存储；</li><li>数据集数据完整性：利用 Doris 构建实时数据集市的原始层，利用 FlinkCDC 等同步工具将业务库 MySQL与决策过程中产生的 MongoDB 数据实时同步到 Doris，最大限度将现有数据都接入 Doris 的统一平台，保证数据集数据完整性。</li><li>离线跑批任务迁移：将现有 Hive&amp;Spark 中大部分跑批任务迁移至 Doris，提升跑批效率；</li><li>统一查询分析出口：将所有的查询分析统一集中到 Doris，完全统一数据出口，实现统一数据查询网关，使数据的管理更加规范化；</li><li>强化集群稳定扩容：引入可视化运维管理工具对集群进行维护和管理，使 Doris 集群能够更加稳定支撑业务扩展。</li></ul><h1>总结与致谢</h1><p>Apache Doris1.2 是社区在版本迭代中的重大升级，借助 Multi Catalog 等优异功能能让 Doris 在 Hadoop 相关的大数据体系中快速落地，实现联邦查询；同时可以将日常跑批与统计分析进行解耦，有效提升大数据平台的的查询性能。</p><p>作为第一批 Apache Doris1.2 的用户，我们深感荣幸，同时也十分感谢 Doris 团队的全力配合和付出，可以让 Apache Doris 快速落地、上线生产，并为后续的迭代优化提供了可能。</p><p>Apache Doris 1.2 值得大力推荐，希望大家都能从中受益，祝愿 Apache Doris 生态越来越繁荣，越来越好！</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Tencent Music">从 ClickHouse 到 Apache Doris，腾讯音乐内容库数据平台架构演进实践</a></h2><div class="blog-info"><time datetime="2023-03-07T00:00:00.000Z" itemprop="datePublished">2023年3月7日</time><span class="split-line"></span><span class="authors"><span class="s-author">张俊 &amp; 代凯</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><hr><p>作者：腾讯音乐内容库数据平台 张俊、代凯</p><p>腾讯音乐娱乐集团（简称“腾讯音乐娱乐”）是中国在线音乐娱乐服务开拓者，提供在线音乐和以音乐为核心的社交娱乐两大服务。腾讯音乐娱乐在中国有着广泛的用户基础，拥有目前国内市场知名的四大移动音乐产品：QQ音乐、酷狗音乐、酷我音乐和全民K歌，总月活用户数超过8亿。  </p><h1>业务需求</h1><p>腾讯音乐娱乐拥有海量的内容曲库，包括录制音乐、现场音乐、音频和视频等多种形式。通过技术和数据的赋能，腾讯音乐娱乐持续创新产品，为用户带来更好的产品体验，提高用户参与度，也为音乐人和合作伙伴在音乐的制作、发行和销售方面提供更大的支持。</p><p>在业务运营过程中我们需要对包括歌曲、词曲、专辑、艺人在内的内容对象进行全方位分析，高效为业务赋能，<strong>内容库数据平台旨在集成各数据源的数据，整合形成内容数据资产（以指标和标签体系为载体），为应用层提供库存盘点、分群画像、指标分析、标签圈选等内容分析服务。</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cef94d5e9327496e9e0d2aff45816986~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><h1>数据架构演进</h1><p>TDW 是腾讯最大的离线数据处理平台，公司内大多数业务的产品报表、运营分析、数据挖掘等的存储和计算都是在TDW中进行，内容库数据平台的数据加工链路同样是在腾讯数据仓库 TDW 上构建的。截止目前，内容库数据平台的数据架构已经从 1.0 演进到了 4.0 ，<strong>经历了分析引擎从 ClickHouse 到 Apache Doris 的替换、经历了数据架构语义层的初步引入到深度应用</strong>，有效提高了数据时效性、降低了运维成本、解决了数据管理割裂等问题，收益显著。接下来将为大家分享腾讯音乐内容库数据平台的数据架构演进历程与实践思考。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据架构-10">数据架构 1.0<a href="#数据架构-10" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6c7d269667b8450c8b1bfda3372b34e5~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>如图所示为数据架构 1.0 架构图，分为数仓层、加速层、应用层三部分，数据架构 1.0 是一个相对主流的架构，简单介绍一下各层的作用及工作原理：</p><ul><li>数仓层：通过 ODS-DWD-DWS 三层将数据整合为不同主题的标签和指标体系， DWM 集市层围绕内容对象构建大宽表，从不同主题域 DWS 表中抽取字段。</li><li>加速层：在数仓中构建的大宽表导入到加速层中，Clickhouse 作为分析引擎，Elasticsearch 作为搜索/圈选引擎。</li><li>应用层：根据场景创建 DataSet，作为逻辑视图从大宽表选取所需的标签与指标，同时可以二次定义衍生的标签与指标。</li></ul><p><strong>存在的问题：</strong></p><ul><li>数仓层：不支持部分列更新，当上游任一来源表产生延迟，均会造成大宽表延迟，进而导致数据时效性下降。</li><li>加速层：不同的标签跟指标特性不同、更新频率也各不相同。由于 ClickHouse 目前更擅长处理宽表场景，无区别将所有数据导入大宽表生成天的分区将造成存储资源的浪费，维护成本也将随之升高。</li><li>应用层：ClickHouse 采用的是计算和存储节点强耦合的架构，架构复杂，组件依赖严重，牵一发而动全身，容易出现集群稳定性问题，对于我们来说，同时维护 ClickHouse 和 Elasticsearch 两套引擎的连接与查询，成本和难度都比较高。</li></ul><p>除此之外，ClickHouse 由国外开源，交流具有一定的语言学习成本，遇到问题无法准确反馈、无法快速获得解决，与社区沟通上的阻塞也是促进我们进行架构升级的因素之一。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据架构-20">数据架构 2.0<a href="#数据架构-20" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/782a9cd3ff4d4f76afea166262065466~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>基于架构 1.0 存在的问题和 ClickHouse 的局限性，我们尝试对架构进行优化升级，<strong>将分析引擎 ClickHouse 切换为 Doris</strong>，Doris 具有以下的优势：</p><p><strong>Apache Doris 的优势：</strong></p><ul><li>Doris 架构极简易用，部署只需两个进程，不依赖其他系统，运维简单；兼容 MySQL 协议，并且使用标准 SQL。</li><li>支持丰富的数据模型，可满足多种数据更新方式，支持部分列更新。</li><li>支持对 Hive、Iceberg、Hudi 等数据湖和 MySQL、Elasticsearch 等数据库的联邦查询分析。</li><li>导入方式多样，支持从 HDFS/S3 等远端存储批量导入，也支持读取 MySQL Binlog 以及订阅消息队列 Kafka 中的数据，还可以通过 Flink Connector 实时/批次同步数据源（MySQL,Oracle,PostgreSQL 等）到 Doris。</li><li>社区目前 Apache Doris 社区活跃、技术交流更多，SelectDB 针对社区有专职的技术支持团队，在使用过程中遇到问题均能快速得到响应解决。</li></ul><p><strong>同时我们也利用 Doris 的特性，解决了架构 1.0 中较为突出的问题。</strong></p><ul><li>数仓层：Apache Doris 的 Aggregate 数据模型可支持部分列实时更新，因此我们去掉了 DWM 集市层的构建，直接增量到 Doris / ES 中构建宽表，<strong>解决了架构 1.0 中上游数据更新延迟导致整个宽表延迟的问题，进而提升了数据的时效性</strong>。数据（指标、标签等）通过 Spark 统一离线加载到 Kafka 中，使用 Flink 将数据增量更新到 Doris 和 ES 中（利用 Flink 实现进一步的聚合，减轻了 Doris 和 ES 的更新压力）。</li><li>加速层：该层主要将大宽表拆为小宽表，根据更新频率配置不同的分区策略，减小数据冗余带来的存储压力，提高查询吞吐量。<strong>Doris 具备多表查询和联邦查询性能特性，可以利用多表关联特性实现组合查询。</strong></li><li>应用层：DataSet 统一指向 Doris，<strong>Doris 支持外表查询</strong>，利用该特性可对 ES 引擎直接查询。</li></ul><p><strong>架构 2.0 存在的问题：</strong></p><ul><li>DataSet 灵活度较高，数据分析师可对指标和标签自由组合和定义，但是不同的分析师对同一数据的定义不尽相同、定义口径不一致，导致指标和标签缺乏统一管理，这使得数据管理和使用的难度都变高。</li><li>Dataset 与物理位置绑定，应用层无法进行透明优化，如果 Doris 引擎出现负载较高的情况，无法通过降低用户查询避免集群负载过高报错的问题。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据架构-30">数据架构 3.0<a href="#数据架构-30" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>针对指标和标签定义口径不统一，数据使用和管理难度较高的问题，我们继续对架构进行升级。数据架构 3.0 主要的变化是引入了专门的语义层，语义层的主要作用是将技术语言转换为业务部门更容易理解的概念，目的是将标签 (tag)与指标(metric)变为“一等公民”，作为数据定义与管理的基本对象。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a7703a5716b24315b61828c86ec26fa9~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p><strong>引入语义层的优势有：</strong></p><ul><li>对于技术来说，应用层不再需要创建 DataSet，从语义层可直接获取特定内容对象的标签集 (tagset)和指标集(metricset) 来发起查询。</li><li>对于数据分析师来说，可统一在语义层定义和创建衍生的指标和标签，解决了定义口径不一致、管理和使用难度较高的问题。</li><li>对于业务来说，无需耗费过长时间考虑什么场景应选择哪个数据集使用，语义层对标签和指标透明统一的定义提升了工作效率、降低了使用成本。</li></ul><p><strong>存在的问题：</strong></p><p>从架构图可知，标签和指标等数据均处于下游位置，虽然标签与指标在语义层被显式定义，但仍然无法影响上游链路，数仓层有自己的语义逻辑，加速层有自己的导入配置，<strong>这样就造成了数据管理机制的割裂</strong>。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据架构-40">数据架构 4.0<a href="#数据架构-40" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在数据架构 3.0 的基础上，我们对语义层进行更深层次的应用，在数据架构 4.0 中，我们将语义层变为架构的中枢节点，目标是对所有的指标和标签统一定义，从计算-加速-查询实现中心化、标准化管理，解决数据管理机制割裂的问题。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2866d01339914cf7960b82fe83e870dc~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>语义层作为架构中枢节点所带来的变化：</p><ul><li>数仓层：语义层接收 SQL 触发计算或查询任务。数仓从 DWD 到 DWS 的计算逻辑将在语义层中进行定义，且以单个指标和标签的形式进行定义，之后由语义层来发送命令，生成 SQL 命令给数仓层执行计算。</li><li>加速层：从语义层接收配置、触发导入任务，比如加速哪些指标与标签均由语义层指导。</li><li>应用层：向语义层发起逻辑查询，由语义层选择引擎，生成物理 SQL。</li></ul><p><strong>架构优势：</strong></p><ul><li>可以形成统一视图，对于核心指标和标签的定义进行统一查看及管理。</li><li>应用层与物理引擎完成解耦，可进一步对更加灵活易用的架构进行探索：如何对相关指标和标签进行加速，如何在时效性和集群的稳定性之间平衡等。</li></ul><p><strong>存在的问题：</strong></p><p>因为当前架构是对单个标签和指标进行了定义，因此如何在查询计算时自动生成一个准确有效的 SQL 语句是非常有难度的。如果你有相关的经验，期待有机会可以一起探索交流。</p><h1>优化经验</h1><p>从上文已知，为更好地实现业务需求，数据架构演进到 4.0 版本，其中 <strong>Apache Doris 作为分析加速场景的解决方案在整个系统中发挥着重要的作用</strong>。接下来将从场景需求、数据导入、查询优化以及成本优化四个方面出发，分享基于 Doris 的读写优化经验，希望给读者带来一些参考。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="场景需求">场景需求<a href="#场景需求" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b03fed188c0c4e8d8761b2e10f52954e~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>目前我们有 800+ 标签， 1300+ 指标，对应 TDW 中有 80 + Source 表，单个标签、指标的最大基数达到了 2 亿+。我们希望将这些数据从 TDW 加速到 Doris 中完成标签画像和指标的分析。<strong>从业务的角度，需要满足以下要求：</strong></p><ul><li><strong>实时可用</strong>：标签/指标导入以后，需实现数据尽快可用。不仅要支持常规离线导入 T+1 ，同时也要支持实时打标场景。</li><li><strong>部分更新</strong>：因每个 Source 表由各自 ETL 任务产出对应的数据，其产出时间不一致，并且每个表只涉及部分指标或标签，不同数据查询对时效性要求也不同，因此架构需要支持部分列更新。</li><li><strong>性能高效</strong>：具备高效的写入能力，且在圈选、洞察、报表等场景可以实现秒级响应。</li><li><strong>控制成本</strong>：在满足业务需求的前提下，最大程度地降低成本；支持冷热数据精细化管理，支持标签灵活上下架。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据导入方案">数据导入方案<a href="#数据导入方案" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cacb43aa7815479c835d5401bd943ce9~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>为了减轻 Doris 写入压力，我们考虑在数据写入 Doris 之前，尽量将数据生成宽表，再写入到 Doris 中。<strong>针对宽表的生成，我们有两个实现思路</strong>：第一个是在 TDW 数仓中生成宽表；第二个是 Flink 中生成宽表。我们对这两个实现思路进行了实践对比，最终决定选择第二个实现思路，原因如下：</p><p>在 TDW 中生成宽表，虽然链路简单，但是弊端也比较明显。</p><ul><li>存储成本较高， TDW 除了要维护离散的 80 +个 Source 表外，还需维护 1 个大宽表、2 份冗余的数据。</li><li>实时性比较差，由于每个 Source 表产出的时间不一样，往往会因为某些延迟比较大的 Source 表导致整个数据链路延迟增大。</li><li>开发成本较高，该方案只能作为离线方式，若想实现实时方式则需要投入开发资源进行额外的开发。</li></ul><p>而<strong>在 Flink 中生成宽表，链路简单、成本低也容易实现</strong>，主要流程是：首先用 Spark 将相关 Source 表最新数据离线导入到 Kafka 中， 接着使用 Flink 来消费 Kafka，并通过主键 ID 构建出一张大宽表，最后将大宽表导入到 Doris 中。如下图所示，来自数仓 N 个表中 ID=1 的 5 条数据，经过 Flink 处理以后，只有一条 ID=1 的数据写入 Doris 中，大大减少 Doris 写入压力。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a06412b91a9d47da872a0a9796ffe257~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>通过以上导入优化方案，<strong>极大地降低了存储成本</strong>， TDW 无需维护两份冗余的数据，Kafka 也只需保存最新待导入的数据。同时该方案<strong>整体实时性更好且可控</strong>，并且大宽表聚合在 Flink 中执行，可灵活加入各种 ETL 逻辑，离线和实时可对多个开发逻辑进行复用，<strong>灵活度较高</strong>。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据模型选择"><strong>数据模型选择</strong><a href="#数据模型选择" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>目前我们生产环境所使用的版本为 Apache Doris 1.1.3，我们对其所支持的 Unique 主键模型、Aggregate 聚合模型和 Duplicate 明细模型进行了对比 ，相较于 Unique 模型和 Duplicate 模型，<strong>Aggregate 聚合模型满足我们部分列更新的场景需求</strong>：</p><p>Aggregate 聚合模型可以支持多种预聚合模式，可以通过<code>REPLACE_IF_NOT_NULL</code>的方式实现部分列更新。数据写入过程中，Doris 会将多次写入的数据进行聚合，最终用户查询时，返回一份聚合后的完整且正确的数据。</p><p>另外两种数据模型适用的场景，这里也进行简单的介绍：</p><ul><li>Unique 模型适用于需要保证 Key 唯一性场景，同一个主键 ID 多次导入之后，会以 append 的方式进行行级数据更新，仅保留最后一次导入的数据。在与社区进行沟通后，确定<strong>后续版本 Unique 模型也将支持部分列更新</strong>。</li><li>Duplicate 模型区别于 Aggregate 和 Unique 模型，数据完全按照导入的明细数据进行存储，不会有任何预聚合或去重操作，即使两行数据完全相同也都会保留，因此 Duplicate 模型适用于既没有聚合需求，又没有主键唯一性约束的原始数据存储。</li></ul><p>确定数据模型之后，我们在建表时如何对列进行命名呢？可以直接使用指标或者是标签的名称吗？</p><p>在使用场景中通常会有以下几个需求：</p><ul><li>为了更好地表达数据的意义，业务方会有少量修改标签、指标名称的需求。</li><li>随着业务需求的变动，标签经常存在上架、下架的情况。</li><li>实时新增的标签和指标，用户希望数据尽快可用。</li></ul><p>Doris 1.1.3 是不支持对列名进行修改的，如果直接使用指标/标签名称作为列名，则无法满足上述标签或指标更名的需求。而对于上下架标签的需求，如果直接以 drop/add column 的方式实现，则会涉及数据文件的更改，该操作耗时耗力，甚至会影响线上查询的性能。</p><p>那么，有没有更轻量级的方式来满足需求呢？接下来将为大家分享相关解决方案及收益：****</p><ul><li><strong>为了实现少量标签、指标名称修改</strong>，我们用 MySQL 表存储相应的元数据，包括名称、全局唯一的 ID 和上下架状态等信息，比如标签歌曲名称<code>song_name</code>的  ID 为 4，在 Doris 中存储命名为 a4，用户使用更具有业务含义<code>song_name</code>进行查询。在查询 Doris 前，我们会在查询层将 SQL 改写成具体的列名 a4。这样名称的修改只是修改其元数据，底层 Doris 的表结构可以保持不变。</li><li><strong>为了实现标签灵活上下架</strong>，我们通过统计标签的使用情况来分析标签的价值，将低价值的标签进入下架流程。下架指的是对元信息进行状态标注，在下架标签重新上架之前，不会继续导入其数据，元信息中数据可用时间也不会发生变化。</li><li><strong>对于实时新增标签/指标</strong>，我们基于名称 ID 的映射在 Doris 表中预先创建适量 ID 列，当标签/指标完成元信息录入后，直接将预留的 ID 分配给新录入的标签/指标，避免在查询高峰期因新增标签/指标所引起的 Schema Change 开销对集群产生的影响。经测试，用户在元信息录入后 10 分钟内就可以使用相应的数据。</li></ul><p>值得关注的是，<strong>在社区近期发布的 1.2.0 版本中，增加了 Light Schema Change 功能， 对于增减列的操作不需要修改数据文件，只需要修改 FE 中的元数据，从而可以实现毫秒级的 Schame Change 操作</strong>。同时开启 Light Schema Change 功能的数据表也可以支持列名的修改，这与我们的需求十分匹配，后续我们也会及时升级到最新版本。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="写入优化"><strong>写入优化</strong><a href="#写入优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>接着我们在数据写入方面也进行了调整优化，这里几点小经验与大家分享：</p><ul><li>Flink 预聚合：通过主键 ID 预聚合，减少写入压力。（前文已说明，此处不再赘述）</li><li>写入 Batch 大小自适应变更：为了不占用过多 Flink 资源，我们实现了从同一个 Kafka Topic 中消费数据写入到不同 Doris 表中的功能，并且可以根据数据的大小自动调整写入的批次，尽量做到攒批低频写入。</li><li>Doris 写入调优：针对- 235 报错进行相关参数的调优。比如设置合理的分区和分桶（Tablet 建议1-10G），同时结合场景对 Compaction 参数调优：</li></ul><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">max_XXXX_compaction_thread</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">max_cumulative_compaction_num_singleton_deltas</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ul><li>优化 BE 提交逻辑：定期缓存 BE 列表，按批次随机提交到 BE 节点，细化负载均衡粒度。</li></ul><blockquote><p>优化背景：在写入时发现某一个 BE负载会远远高于其他的 BE，甚至出现 OOM。结合源码发现：作业启动后会获取一次 BE 地址列表，从中随机选出一个 BE 作为 Coordinator 协调者，该节点主要负责接收数据、并分发到其他的 BE 节点，除非作业异常报错，否则该节点不会发生切换。</p><p>对于少量 Flink 作业大数据场景会导致选中的 BE 节点负载较高，因此我们尝试对 BE 提交逻辑进行优化，设置每 1 小时缓存一次 BE 列表，每写入一个批次都随机从 BE 缓存列表中获取一个进行提交，这样负载均衡的粒度就从 job 级别细化到每次提交的批次，使得 BE 间负载更加的均衡，这部分实现我们已经贡献到社区，欢迎大家一起使用并反馈。</p><ul><li><a href="https://github.com/apache/doris-spark-connector/pull/59" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris-spark-connector/pull/59</a></li><li><a href="https://github.com/apache/doris-spark-connector/pull/60" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris-spark-connector/pull/60</a></li><li><a href="https://github.com/apache/doris-spark-connector/pull/61" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris-spark-connector/pull/61</a></li></ul></blockquote><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cf32f571d41b473580c9affc6b1eb012~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><p>通过以上数据导入的优化措施，使得整体导入链路更加稳定，每日离线<strong>导入时长下降了 75%</strong> ，数据版本累积情况也有所改善，其中 cumu compaction 的<strong>合并分数更是从 600+直降到 100 左右</strong>，优化效果十分明显。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3a9e49997121435a887ce81265f2c9e7~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="查询优化">查询优化<a href="#查询优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>目前我们的场景指标数据是以分区表的形式存储在 Doris 中， ES 保留一份全量的标签数据。在我们的使用场景中，标签圈选的使用率很高，大约有 60% 的使用场景中用到了标签圈选，在标签圈选场景中，通常需要满足以下几个要求：</p><ul><li>用户圈选逻辑比较复杂，数据架构需要支持同时有上百个标签做圈选过滤条件。</li><li>大部分圈选场景只需要最新标签数据，但是在指标查询时需要支持历史的数据的查询。</li><li>基于圈选结果，需要进行指标数据的聚合分析。</li><li>基于圈选结果，需要支持标签和指标的明细查询。</li></ul><p>经过调研，我们最终采用了 <strong>Doris on ES 的解决方案</strong>来实现以上要求，将 Doris 的分布式查询规划能力和 ES 的全文检索能力相结合。Doris on ES 主要查询模式如下所示：</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">SELECT tag, agg(metric) </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   FROM Doris </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   WHERE id in (select id from Es where tagFilter)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">   GROUP BY tag</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>在 ES 中圈选查询出的 ID 数据，以子查询方式在 Doris 中进行指标分析。</p><p>我们在实践中发现，查询时长跟圈选的群体大小相关。如果从 ES 中圈选的群体规模超过 100 万时，查询时长会达到 60 秒，圈选群体再次增大甚至会出现超时报错。经排查分析，主要的耗时包括两方面：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a599a1c478dd4fe49572958b9e7582e2~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><ul><li>BE 从 ES 中拉取数据（默认一次拉取 1024 行），对于 100 万以上的群体，网络 IO 开销会很大。</li><li>BE 数据拉取完成以后，需要和本地的指标表做 Join，一般以 SHUFFLE/BROADCAST 的方式，成本较高。</li></ul><p><strong>针对这两点，我们进行了以下优化：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5d617b0bd683410b9189a4e3129dede8~tplv-k3u1fbpfcp-zoom-1.image" alt="图片" class="img_ev3q"></p><ul><li>增加了查询会话变量<code>es_optimize</code>，以开启优化开关；</li><li>数据写入 ES 时，新增 BK 列用来存储主键 ID Hash 后的分桶序号，算法和 Doris 的分桶算法相同（CRC32）；</li><li>BE 生成 Bucket Join 执行计划，将分桶序号下发到 BE ScanNode 节点，并下推到 ES；</li><li>ES 对查询出的数据进行 Bitmap 压缩，并将数据的多批次获取优化为一次获取，减少网络 IO 开销；</li><li>Doris BE 只拉取和本地 Doris 指标表相关 Bucket 的数据，直接进行本地 Join，避免 Doris BE 间数据再 Shuffle 的过程。</li></ul><p>通过以上优化措施，<strong>百万分群圈选洞察查询时间从最初的 60 秒缩短到 3.7 秒</strong>，性能显著提升！</p><p>经过与社区沟通交流，<strong>Apache Doris 从 2.0.0 版本开始，将支持倒排索引</strong>。可进行文本类型的全文检索；支持中文、英文分词；支持文本、数值日期类型的等值和范围过滤；倒排索引对数组类型也提供了支持，多个过滤条件可以任意进行 AND OR NOT 逻辑组合。由于高性能的向量化实现和面向 AP 数据库的精简优化，<strong>Doris 的倒排索引相较于 ES 会有 3~5 倍性价比提升</strong>，即将在 2 月底发布的 2.0 preview 版本中可用于功能评估和性能测试，相信在这个场景使用后会有进一步的性能提升。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="成本优化">成本优化<a href="#成本优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在当前大环境下，降本提效成为了企业的热门话题，<strong>如何在保证服务质量的同时降低成本开销</strong>，是我们一直在思考的问题。在我们的场景中，<strong>成本优化主要得益于 Doris 自身优秀的能力</strong>，这里为大家分享两点：</p><p><strong>1、冷热数据进行精细化管理。</strong></p><ul><li>利用 Doris TTL 机制，在 Doris 中只存储近一年的数据，更早的数据放到存储代价更低的 TDW 中；</li><li>支持分区级副本设置，3 个月以内的数据高频使用，分区设置为 3 副本 ；3-6 个月数据分区调整为 2 副本；6 个月之前的数据分区调整为1 副本；</li><li>支持数据转冷， 在 SSD 中仅存储最近 7 天的数据，并将 7 天之前的数据转存到到 HDD 中，以降低存储成本；</li><li>标签上下线，将低价值标签和指标下线处理后，后续数据不再写入，减少写入和存储代价。</li></ul><p><strong>2、降低数据链路成本。</strong></p><p>Doris 架构非常简单，只有FE 和 BE 两类进程，不依赖其他组件，并通过一致性协议来保证服务的高可用和数据的高可靠，自动故障修复，运维起来比较容易;</p><ul><li>高度兼容 MySQL 语法，支持标准 SQL，极大降低开发人员接入使用成本；</li><li>支持多种联邦查询方式，支持对 Hive、MySQL、Elasticsearch 、Iceberg 等组件的联邦查询分析，降低多数据源查询复杂度。</li></ul><p>通过以上的方式，<strong>使得存储成本降低 42%，开发与时间成本降低了 40%</strong> ，成功实现降本提效，后续我们将继续探索！</p><h1>未来规划</h1><p>未来我们还将继续进行迭代和优化，我们计划在以下几个方向进行探索：</p><ul><li>实现自动识别冷热数据，用 Apache Doris 存储热数据，Iceberg 存储冷数据，利用 Doris 湖仓一体化能力简化查询。</li><li>对高频出现的标签/指标组合，通过 Doris 的物化视图进行预计算，提升查询的性能。</li><li>探索 Doris 应用于数仓计算任务，利用物化视图简化代码逻辑，并提升核心数据的时效性。</li></ul><p>最后，感谢 Apache Doris 社区和 <a href="https://selectdb.com" target="_blank" rel="noopener noreferrer">SelectDB</a> 的同学，感谢其快速响应和积极支持，未来我们也会持续将相关成果贡献到社区，希望 Apache Doris 飞速发展，越来越好！</p><p><strong># 相关链接：</strong></p><p><strong>SelectDB 官网</strong>：</p><p><a href="https://selectdb.com" target="_blank" rel="noopener noreferrer">https://selectdb.com</a> </p><p><strong>Apache Doris 官网</strong>：</p><p><a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p><strong>Apache Doris Github</strong>：</p><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Duyansoft">复杂查询响应速度提升10+倍，度言软件基于 Apache Doris 实时数仓建设实践</a></h2><div class="blog-info"><time datetime="2023-02-27T00:00:00.000Z" itemprop="datePublished">2023年2月27日</time><span class="split-line"></span><span class="authors"><span class="s-author">杭州度言软件大数据团队</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p>作者 | 杭州度言软件大数据团队</p><p>编辑整理：SelectDB</p><p>杭州度言软件有限公司（度言软件）成立于2014年，是信贷不良资产处置技术服务供应商，以“智能科技赋能不良资产处置，推动贷后行业合规高效发展”为使命，运用云通讯、大数据、人工智能等智能科技为信贷不良资产处置业务赋能，提供贷后管理通信能力支撑，实现了催收作业的智能化管理，客户群体为银行、消费金融公司、AMC 等金融机构和为这些机构提供人力资源外包服务的相关公司，目前已拥有 2000 多家企业客户。</p><p>度言软件以围绕信贷不良资产案件高效流转管理为目的核心，从机构管理、团队管理、坐席管理、外呼作业、调解法诉等环节入手，帮助客户构建数智化的业务管理体系，以数字化系统提升管理效能、以智能化工具赋能处置作业，打通金融机构、外包服务公司、司法系统等多方的业务系统，并按照监管要求<strong>对外呼行为、录音文件、沟通记录等案件相关数据进行记录、汇集、稽核、统计和分析，具有海量账号同时登陆、数据请求高并发、多来源数据汇总接入的特点要求。</strong></p><h1>业务需求</h1><p>2021 年之前，度言软件旗下产品的数据需求主要由传统数据库 MySQL，MongoDB，ElasticSearch 为主的技术架构来实现，近两年随着业务不断发展带而来数据量的高速增长，传统的数仓技术架构已初显瓶颈，难以满足客户日益丰富多样化的数据及分析需求。为了给客户提供更优质的服务体验，度言软件亟需对现有的技术架构做出优化和重构。</p><h1>早期架构及痛点</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数仓架构-10-版本">数仓架构 1.0 版本<a href="#数仓架构-10-版本" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>初创期间，由于公司业务量相对较少，主要还是以 OLTP 业务和少量的业务报表服务为主，并且对于数据分析方面的需求也很少，仅通过传统的数据库基本就能满足早期的业务数据需求。<strong>数仓</strong> <strong>架构 1.0 如下图所示：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b90115e4f1124a7480f095c113412dce~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数仓架构-20-版本">数仓架构 2.0 版本<a href="#数仓架构-20-版本" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>随着公司业务的不断扩展，数据体量也出现极速增长的态势，业务侧对于数据分析方面的需求也逐渐多了起来，为此我们成立了专门的大数据团队，为搭建新的数仓及业务数据分析需求进行服务。<strong>如下图所示为数仓架构 2.0</strong> ：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8556d98d920445ffa9290843aff41e4e~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>数仓架构 2.0 版本是基于 MaxCompute + Hologres/MySQL 来搭建的。数据来源主要有 MySQL 和 MongoDB 的业务数据以及埋点日志数据；数据首先采集到数据总线 DataHub 中，后经过 DataHub 直接导入到 MaxCompute，MaxCompute 作为一个离线数仓，我们将其进行了传统的数仓分层；数据的加工处理和分析计算主要在离线数仓中进行，并将计算好的结果导出到 MySQL中，来对接 QuickBI 展示报表。此外，我们还尝试了将 Hologres 用作实时数仓，作为 MongoDB 的替换方案，用于业务上的查询系统。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="早期架构存在的问题">早期架构存在的问题：<a href="#早期架构存在的问题" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ol><li><strong>响应速度较慢</strong>。MySQL 对于大表的聚合计算并不友好，响应速度较慢。产品侧要求数据查询响应时间在 5 秒以内，虽然我们也基于 MySQL 进行了许多优化，但优化效果十分有限，仍无法达到 5s 的响应要求；我们甚至尝试了直接用 MaxCompute 对接 QuickBI，希望基于 MaxCompute 的查询加速和 QuickBI 的缓存来帮助我们解决问题，然而结果并不理想。</li><li><strong>维系维度表成本高、难度大。</strong> 离线数仓在数据同步的时效性上并不占优势(每 5 分钟进行一次批量同步)，因此不太适合数据频繁更新和删除的场景，同时也给维度表的维护带来了额外的工作量。在数据更新和删除场景中，我们需要定期通过过滤和去重来保持数据的一致性，而事实上，大多时候需要报表数据实时关联维度表，这让我们直接放弃了在离线数仓中维系维度表的方案。</li><li><strong>不支持高并发点查询场景。</strong> 原实时数仓虽然可以满足我们对数据分析的部分性能要求，但其对高并发的点查场景并不太友好，不管是采用列式存储还是行级存储建表，优化之后的响应时长在 500 毫秒左右，综合来看性价比不算太高。</li></ol><h1>解决思路</h1><p>为了解决上述问题，我们希望理想数仓能具有如下特性：</p><ol><li>实现一站式实时数仓，能同时满足多种不同业务数据需求，大大简化大数据架构体系；</li><li>可同时支持 OLAP，Ad-hoc 和高 QPS 点查场景；</li><li>数据接入友好，写入即可见，对数据增删改和聚合等都有较好的支持；</li><li>架构简单，运维部署和维护简单，有较好的监控体系。</li></ol><h1>技术选型</h1><p>在 2022 年 3 月份，我们对市场上主流的的几款即席查询数据库进行了调研，调研中我们发现每个产品只能满足某 1 个或几个特定的使用场景，没有一个产品可以完全满足所有的选型要求，而同时采用多个产品，将大大增加开发运维成本，同时也会使架构变得更加庞大复杂，这并不符合我们对理想数仓的要求。</p><p>正在这时，我们从开源社区、技术媒体等渠道了解到了<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer"> Apache Doris</a> ，经初步调研，我们发现 Doris 基本可以满足我们对理想数仓的所有要求。接着我们对 Doris 展开了深入调研，并使我们最终决定使用 Doris：Doris 架构非常简单，只有 FE 和 BE 两类进程，这两类进程都可以进行横向扩展，单集群可以支持到数百台机器、数十 PB 的存储容量，并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了分布式系统的运维成本，同时也不需要依赖于 Hadoop，避免了我们需要投入成本来额外部署 Hadoop 集群。</p><h1>基于 Doris 的新数仓架构设计</h1><p>最初使用 Doris 的初衷是替换部分 MySQL 数据量较大的报表，基于 MySQL 的查询约需要几十秒的响应时间，在替换为 Doris 后，查询性能有了显著提升，几秒内即可返回结果，最长的 SQL 执行时间大概在 8 秒左右，速度相较于之前提升了8 倍。Doris 的初步应用就给了我们一个意外的惊喜，因此我们决定使用 Doris 完全替换掉早期数仓中的 MySQL，在这使用过程中，我们又发现 Doris 远比我们想象的要强大，目前已将客户报表及公司内部运营决策数据全部迁移至 Apache Doris，并计划用 Apache Doris 来替换 MongoDB 和 ElasticSearch，用于业务上高 QPS 的点查场景。<strong>以下是基于 Doris 的新数仓架构设计及使用情况：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9ee756d4fef24d07875b14855ff91f63~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据建模"><strong>数据建模：</strong><a href="#数据建模" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们在业务上使用最多的是 Unique 模型和 Aggregate 模型，这两种模型基本能够满足业务需求。</p><ul><li>Unique 模型主要用于维度表和业务表(原始表)的接入，确保数据导入过程中的一致性。</li><li>Aggregate主要用于报表数据的导入，Aggregate 分为 Key (维度列) 和 Value（指标列），Value 列支持四种聚合方式：<code>sum</code> ,<code>replace</code>,<code>max</code>,<code>min</code>。当前主要以<code>replace</code> 聚合方式为主，方便统计数据重复导入和修正结果，后续也会尝试更多的方式来充分发挥 Doris 的性能。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据接入"><strong>数据接入：</strong><a href="#数据接入" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li>使用 Flink-Doris-Connector 进行实时导入：主要用于业务数据的导入，基于MySQL 的 Binlog 日志写入到 Kafka 后，通过 Flink 解析加工后准实时写入 Doris。</li><li>使用 DataX 进行离线导入：主要用于对接离线数仓已计算后的报表数据。</li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据开发"><strong>数据开发：</strong><a href="#数据开发" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>目前 Doris 主要以提供终端查询为主，复杂的 SQL 开发任务运行比较少，为尽可能减少 Doris 在数据加工方面的资源消耗，当前仅有报表集群在凌晨执行少量的 SQL 任务，主要以 Doris SQL 通过 insert into 的方式来导入。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据管理"><strong>数据管理：</strong><a href="#数据管理" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Doris 支持将当前数据以文件的形式通过 Broker 备份到远端存储系统中，后可以通过恢复命令从远端存储系统中将数据恢复到任意 Doris 集群。通过该功能，Doris 可支持定期对数据进行快照备份，也可以通过该功能在不同集群间进行数据迁移。我们也会定期将集群数据备份到阿里云 OSS 上，作为备用数据恢复方案。另外，在这期间我们对 Doris 集群进行了一次拆分，将报表集群和业务上的高并发查询集群分开，采用了 Doris 的数据备份和迁移功能。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="监控和报警"><strong>监控和报警：</strong><a href="#监控和报警" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们使用官网推荐的 Prometheus 和 Grafana 进行监控项的采集和展示，Doris 本身提供了丰富的监控指标和标准监控模版，可以非常便捷地对 Doris 集群使用情况进行监控和报警。</p><p>另外，为了进一步对慢 SQL 进行优化，我们还部署了审计日志插件，对于特定用户和特定的慢 SQL 进行优化和资源限制。如下是我们日常使用中的一些指标：</p><p><strong>慢 SQL 查询：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/96037a4c50314c0dbfefd6c63855fac8~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>对于一些长文本 SQL，无法完全展示，可以进一步查看<code>fe.audit.log</code>。</p><p><strong>主要查询统计指标：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/f0e838af4538449aa3fed993d3ea982f~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h1>优化实践：</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="提高并发">提高并发<a href="#提高并发" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们考虑在资源给定的情况下，如何最大程度地提高并发。刚开始引入 Doris 集群的时候，我们尝试对复杂 SQL 进行压测（SQL 层面优化已完成），但始终无法达到预期的压测效果。后来我们尝试<strong>调低</strong> <code>parallel_fragment_exec_instance_num</code> <strong>的值</strong>，顺利完成了压测目标。</p><blockquote><p><strong>需要说明的是：</strong></p><p><code>parallel_fragment_exec_instance_num </code>指的是 Scan Node 在每个 BE 节点上执行实例的个数，相当于在整个查询计划执行过程中的并发度，调高该参数可以提升查询效率，但同时也会增加更多机器资源的消耗。因此在资源有限且查询耗时满足业务需求的情况下，通过调低参数来节省单个 SQL 的资源消耗，有助于提高并发表现。另外，我们通过 Doris 社区了解到，在即将发布的新版本中将实现参数自动设置，无需进行手动调整。</p></blockquote><p>如下图，我们可以看到，在参数设置为 16 的时候，异常率高达 82.84%，并且期间还出现了 BE 宕机重启，将参数调低至 8 后，异常率也同步降低到了 27.66%。最后我们将参数设置为最小值 1 后，异常率为 0，查询响应也能达到预期目标。</p><p>说明：测试环境已重新取数，配置较低，数据仅用来说明 <code>parallel_fragment_exec_instance_num</code> 变动所带来的效果。</p><p>当参数调整为 1：<code>parallel_fragment_exec_instance_num = 1</code></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/b34b8b88a4454d8c903bc682d14b8248~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>当参数调整为 8：<code>parallel_fragment_exec_instance_num = 8</code></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/082a039e65404dbfbd836b5eaa55f45e~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/fda87b9110ac45e09091051138bd6b22~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>当参数调整为 16：<code>parallel_fragment_exec_instance_num = 16</code></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3ec04a8265644ff0a2440c9499998b89~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="be-内存问题">BE 内存问题<a href="#be-内存问题" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>最初我们使用的是 Doris 0.15 的版本。由于刚开始处于测试阶段，Doris 集群配置比较低或参数配置的不合理，当某些 SQL 扫描数据量较大时，就可能因为内存问题导致 BE 宕机。</p><p>在向社区咨询后，了解到 Supervisor 是用 Python 开发的一套通用的进程管理程序，能将一个普通的命令行进程变为后台 Daemon，并监控进程状态，异常退出时能自动重启，因此我们参照官网给出的例子直接用 Supervisor 对 Doris 的 FE 和 BE 进程进行管理。</p><p>但是在运行了一段时间后，新的问题又出现了(已升级到 1.1.0 版本)。Doris 的 BE 节点内存一直在缓慢上升状态，并且达到了设置的最大内存参数 80% 后仍在继续上涨。后经分析发现 BE 存在内存泄漏问题，因此设置的参数并未生效。为此，我们将 Supervisor 切换为 Systemd 来守护 FE、BE 进程，限制整个节点的内存使用上限。不过在 Doris 1.1.3 推出之后，此问题已得到彻底的修复。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="资源占用">资源占用<a href="#资源占用" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在迁移完新集群后，我们发现通过 Flink-Doris-Connector 数据导入占用非常高的集群资源，虽然设置了按批次写入(每 3s 写入一次 )，以限制数据的导入频率，但集群资源的占用仍未得到较大改善。因此我们在集群资源和数据实时可见性方面做了权衡，介于我们对数据实时性的要求相对不是太高，所以我们将每 3s 写入一次改为每 10s 或 20s 写入一次，调整写入时间后，集群的 CPU 资源占用下降明显，得到改善。</p><h1>应用现状</h1><p>目前度言软件有 2 个 Doris 集群，1 个集群用作线上业务的查询系统，主要是应对高 QPS 的点查场景，我们将原先基于业务库 MySQL 和 MongoDB 的查询迁移至 Doris，一方面减少了业务库的读写压力，同时也提高了用户查询服务的使用体验。在 Doris 中，最复杂的查询在 1 秒以内即可响应，响应速度提升了十几倍；另外 1 个集群主要用作即席查询(AD-Hoc Query)和报表分析，具体包括公司内部使用的所有报表和一些临时查询、客户报表、数据实时大屏。</p><p>总而言之，使用 Doris 替换了部分业务使用场景后，用户在产品上的使用体验有了进一步得到提升，页面打开更加流畅，原本因为查询慢而不能实现的功能，当前已经实现并上线使用。同时在资源成本上也减轻了 MySQL 和 MongoDB 数据库的压力，不需要频繁进行升配和磁盘升级。</p><h1>总结规划</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="效果总结">效果总结<a href="#效果总结" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ol><li>Doris 完美覆盖了原本需要多个技术栈才能实现的业务场景，极大地简化了大数据的架构体系。</li><li>Doris 对 Join 支持较好，因此我们选择了将维度表放到 Doris 中进行维护，相较于之前在离线数仓中进行维护，明显地提高了开发的效率，并降低了数据出错的可能性，满足了业务上对维度表近实时更新的需求。</li><li>Doris 支持 MySQL 协议和标准 SQL，开发人员学习成本低，上手简单，可以快速将原先的业务迁移至 Doris 上来。</li><li>Doris 社区活跃，版本迭代速度快。在遇到任何问题时，都可以向社区求助，<a href="https://cn.selectdb.com/" target="_blank" rel="noopener noreferrer">SelectDB</a> 为 Apache Doris 组建了一支全职专业的技术支持入队，24H 内即可得到社区的响应回复。</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="未来规划">未来规划<a href="#未来规划" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>到目前为止，基于 Doris 的实时数仓搭建已基本完成，但我们对 Doris 的进一步尝试才刚刚开始，比如 BE 的多磁盘部署，物化视图的使用，Doris-On-ES，Doris 多租户和资源划分等。</p><p><strong>此外，我们也希望 Doris 未来能对以下功能进行进一步优化：</strong></p><ol><li>Flink-Doris-Connector 能支持单 Sink 同时写入多张表，不需要再通过分流后多个 Sink 写入。</li><li>物化视图能够支持多表 Join，不再局限于单表。</li><li>进一步优化数据的底层 Compaction，在保证数据可见性的同时能够尽可能降低资源消耗。</li><li>Doris-Manager 提供对慢 SQL 的优化建议以及表信息收集，对于不合理的建表进行一定的提示。</li></ol><p>从 3 月份使用 Doris 以来，我们一直都和 Doris 社区保持着密切的联系，后续我们也将继续围绕 Doris 作为实时数仓应用到更多的业务使用场景中，对于使用中遇到的问题和优化的方案，我们也会持续和社区保持热切联系，为社区进步贡献我们的一份力量。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/linkedcare">并发提升 10 倍，运算延时降低 70%，领健从 ClickHouse 和 Kudu 到 Apache Doris 数仓升级实践</a></h2><div class="blog-info"><time datetime="2023-01-28T00:00:00.000Z" itemprop="datePublished">2023年1月28日</time><span class="split-line"></span><span class="authors"><span class="s-author">杨鷖</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p>作者｜杨鷖 资深大数据开发工程师</p><p>领健是健康科技行业 SaaS 软件的引领者，专注于消费医疗口腔和医美行业，为口腔诊所、医美机构、生美机构提供经营管理一体化系统，提供了覆盖单店管理、连锁管理、健康档案/电子病历、客户关系管理、智能营销、B2B交易平台、进销存、保险支付、影像集成、BI商业智能等覆盖机构业务全流程的一体化SaaS软件。 同时通过开放平台连接产业上下游，与优质的第三方平台合作，为机构提供完整配套的一站式服务。 截止当前，领健已经在全国设立了 20 余个分支机构，超过 30000 多家中高端以及连锁机构正在使用其服务。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0a0e8025d5fa49d185b2d074a4290a54~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h1>Doris 在领健的演进历程</h1><p>在进入正文之前，简单了解一下 Doris 在领健的发展历程。最初 Doris 替代 ClickHouse 被应用到数据服务项目中，该项目是领健为旗下客户提供的增值报表服务；后在项目服务中发掘出 Doris 查询性能优异、简单易用、部署成本低等诸多优势，在 2021 年10月，我们决定扩大 Doris 应用范围，将 Doris 引入到公司的数仓中，在 Doris 社区及 SelectDB 专业技术团队的支持下，业务逐步从 Kudu 迁移到 Doris，并在最近升级到 1.1.4 向量化版本。我们将通过本文为大家详细介绍领健基于 Doris 的演进实践及数仓构建的经验。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a8f81591dede4eafa811bfa4cbb36456~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h1>数据服务架构演进</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="项目需求">项目需求<a href="#项目需求" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>领健致力于为医疗行业客户提供精细化门店运营平台，为客户提供了数据报表工具，该工具可实现自助式拖拽设计图表、支持多种自带函数自建、数据实时更新等功能，可以支持门店订单查询、客户管理、收入分析等，以推动门店数字化转型，辅助门店科学决策。为更好实现以上功能，数据报表工具需满足以下特点：</p><ul><li><p>支持复杂查询：客户进行自助拖拽设计图表时，将生成一段复杂的 SQL 查询语句直查数据库，且语句复杂度未知，这将对数据库带来不小的压力，从而影响查询性能。</p></li><li><p>高并发低延时：至少可以支撑 <strong>100 个</strong>并发，并在<strong>1 秒内</strong>得到查询结果；</p></li><li><p>数据实时同步： 报表数据源自于 SaaS 系统，当客户对系统中的历史数据进行修改后，报表数据也要进行同步更改，保持一致，这就要求报表数据要与系统实现实时同步。</p></li><li><p>低成本易部署：SaaS 业务存在私有云客户，为降低私有化部署的人员及成本投入，这要求<strong>架构部署及运维要足够简单。</strong></p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="clickhouse-遭遇并发宕机"><strong>ClickHouse 遭遇并发宕机</strong><a href="#clickhouse-遭遇并发宕机" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>最初项目选用 ClickHouse 来提供数据查询服务，但在运行过程中 ClickHouse 遭遇了严重的并发问题，即 10 个并发就会导致 ClickHouse 宕机，这使其无法正常为客户提供服务，这是迫使我们寻找可以替代 ClickHouse 产品的关键因素。</p><p><strong>除此之外还有几个较为棘手的问题：</strong></p><ol><li>云上 ClickHouse 服务成本非常高，且 ClickHouse 组件依赖性较高，数据同步时 ClickHouse 和 Zookeeper 的频繁交互，会对稳定性产生较大的压力。</li><li>如何进行无缝迁移，不影响客户正常使用。</li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="技术选型">技术选型<a href="#技术选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>针对存在的问题及需求，我们决定进行技术选型，分别对 Doris（0.14）、Clickhous、Kudu 这 3 个产品展开的调研测试。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3eff0da8d60d4865898ed629a75baeb7~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>如上表所示，我们对这 3 个产品进行了横向比较，可以看出 Doris 在多方面表现优异：</p><ul><li><p>高并发：Doris 并发性好，可支持上百甚至上千并发，轻松解决 10 并发导致 ClickHouse 宕机问题。</p></li><li><p>查询性能：Doris 可实现毫秒级查询响应，在单表查询中，虽 Doris 与 ClickHouse 查询性能基本持平，但在多表查询中，Doris 远胜于 ClickHouse ，Doris 可以实现在较高的并发下，QPS 不下降。</p></li><li><p>数据更新：Doris 的数据模型可以满足我们对数据更新的需求，以保障系统数据和业务数据的一致性，下文将详细介绍。</p></li><li><p>使用成本：Doris 架构简单，整体部署简单快速，具有完备的导入功能，很好的弹性伸缩能力；同时， Doris 内部可以自动做副本平衡，运维成本极低。而 ClickHouse 及 Kudu 对组件依赖较高，在使用上需要做许多准备工作，这就要求具备一支专业的运维支持团队来处理大量的日常运维工作。</p></li><li><p>标准 SQL：Doris 兼容 MySQL 协议，使用标准 SQL，开发人员上手简单，不需要付出额外的学习成本。</p></li><li><p>分布式 Join ：Doris 支持分布式 Join，而 ClickHouse 由于 Join 查询限制、函数局限性、以及可维护性较差等原因，不满足我们当前的业务需求。</p></li><li><p>社区活跃：Apache Doris 是国内自研数据库，开源社区相当活跃，同时 SelectDB 为 Doris 社区提供了专业且全职团队做技术支持，遇到问题可以直接与社区联系沟通，并能得到快速解决，这对于国外的项目，很大的降低与社区沟通的语言与时间成本。</p></li></ul><p>从以上调研中可以发现，Doris 各方面能力优秀，十分符合我们对选型产品的需求，因此我们使用 Doris 替代了 ClickHouse ，很好解决了ClickHouse 并发性能差、宕机等问题，很好的支撑了数据报表查询服务。</p><h1>数仓架构演进</h1><p>在数据报表的使用过程中，我们逐渐发掘出 Doris 诸多优势，因此决定扩大 Doris 应用范围，将 Doris 引入到公司的数仓中来。<strong>接下来将为大家介绍公司数仓从 Kudu 到 Doris 的演进历程，以及在搭建过程中的优化实践分享。</strong></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="早期公司数仓架构">早期公司数仓架构<a href="#早期公司数仓架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>早期的公司数仓架构使用 Kudu、Impala 来作为运算存储引擎，整体架构如下图所示。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/08384d4c88a44df2b2043839b4275db3~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>从上图可知，数据通过 Kafka Consumer 进入 ODS 层；通过 Kudu 层满足数据更新需要；运用 Impala 来执行数据运算和查询；通过自研平台 DMEP 进行任务调度。 在 ETL 代码中会使用大量的 Upsert 对数据进行 Merge 操作，那么引入 Doris 的首要问题就是要如何实现 Merge 操作，支持业务数据更新，下文中将进行介绍。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="基于-doris-的新数仓架构设计">基于 Doris 的新数仓架构设计<a href="#基于-doris-的新数仓架构设计" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/71287cc9fa804e51a8eaccfa84a0bb94~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>如上图所示，在新架构设计中使用 Apache Doris 负责数仓存储及数据运算；实时数据、 ODS数据的同步从 Kafka Consumter 改为 Flink ；流计算平台使用团队自研的 Duckula；任务调度则引入最新 的 DolphinSchedular，Dolphin schedule 几乎涵盖了自研 DMEP 的大部分功能，同时可以很方便拓展 ETL 的方式，可调度很多不同的任务。</p><h1>优化实践</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据模型选择"><strong>数据模型选择</strong><a href="#数据模型选择" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>上文中提到，当客户对系统中的历史数据修改后，报表数据也要进行同步更改，同时，客户有时只更改某一列的数值，这要求我们需要选择合适的 Doris 模型来满足这些需求。我们在测试中发现，<strong>通过 Aggregate 聚合模型<code>Replace_if_not_null</code> 方式进行数据更新时，可以实现单独更新一列，代码如下：</strong></p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">drop table test.expamle_tbl2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">CREATE TABLE IF NOT EXISTS test.expamle_tbl2</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">(</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `user_id` LARGEINT NOT NULL COMMENT &quot;用户id&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `date` DATE NOT NULL COMMENT &quot;数据灌入日期时间&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `city` VARCHAR(20) COMMENT &quot;用户所在城市&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `age` SMALLINT COMMENT &quot;用户年龄&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `sex` TINYINT COMMENT &quot;用户性别&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `last_visit_date` DATETIME REPLACE_IF_NOT_NULL COMMENT &quot;用户最后一次访问时间&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `cost` BIGINT REPLACE_IF_NOT_NULL COMMENT &quot;用户总消费&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `max_dwell_time` INT REPLACE_IF_NOT_NULL COMMENT &quot;用户最大停留时间&quot;,</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">    `min_dwell_time` INT REPLACE_IF_NOT_NULL COMMENT &quot;用户最小停留时间&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">AGGREGATE KEY(`user_id`, `date`, `city`, `age`, `sex`)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">DISTRIBUTED BY HASH(`user_id`) BUCKETS 1</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">PROPERTIES (</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">&quot;replication_allocation&quot; = &quot;tag.location.default: 1&quot;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">insert into test.expamle_tbl2 </span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">values(10000,&#x27;2017-10-01&#x27;,&#x27;北京&#x27;,20,0,&#x27;017-10-01 06:00:00&#x27;,20,10,10);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select * from test.expamle_tbl ;</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain" style="display:inline-block"></span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">insert into test.expamle_tbl2 (user_id,date,city,age,sex,cost)</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">values(10000,&#x27;2017-10-01&#x27;,&#x27;北京&#x27;,20,0,50);</span><br></span><span class="token-line" style="color:#393A34"><span class="token plain">select * from test.expamle_tbl ;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><p>如下图所示，当写 50 进去，可以实现只覆盖<code>Cost</code>列，其他列保持不变。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/100bb75cf1e44d068661ebe55b6f2de2~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/210265345caf48c29b128c9c8f46704b~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris-compaction-优化">Doris Compaction 优化<a href="#doris-compaction-优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>当 Flink 抽取业务库全量数据、持续不断高频写入 Doris 时，将产生了大量数据版本，Doris 的 Compaction 合并版本速度跟不上新版本生成速度，从而造成数据版本堆积。从下图可看出，BE Compaction Score 分数很高，最高可以达到 400，而健康状态分数应在 100 以下。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2dd483fab25e4f768039a24e71504646~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>针对以上我们做了以下调整：</p><ul><li><p>全量数据不使用实时写入的方式，先导出到 CSV，再通过 Stream Load 写入 Doris；</p></li><li><p>降低 Flink 写入频率，增大 Flink 单一批次数据量；该调整会降低数据的实时性，需与业务侧进行沟通，根据业务方对实时性的要求调整相关数值，最大程度的降低写入压力。</p></li><li><p>调节 Doris BE 参数，使更多 CPU 资源参与到 Compaction 操作中；</p><ul><li><code>compaction_task_num_per_disk</code> 单磁盘 Compaction 任务线程数默认值 2，提升后会大量占用CPU资源，阿里云 16 核，提升 1 个线程多占用 6% 左右 CPU。</li><li><code>max_compaction_threads compaction</code>线程总数默认为10。</li><li><code>max_cumulative_compaction_num_singleton_deltas</code> 参数控制一个 CC 任务最多合并 1000 个数据版本，适当改小后单个 Compaction 任务的执行时间变短，执行频率变高，集群整体版本数会更加稳定。</li></ul></li></ul><p>通过调整集群， Compaction Score 稳定在了<strong>50-100，有效解决了版本堆积问题</strong>。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/032dbec263cc42c18a4bfbe7c89ee32b~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>值得关注的是，在 Doris 1.1 版本中对 Compaction 进行了一系列的优化，<strong>在任务调度层面</strong>，增加了主动触发式的 Compaction 任务调度，结合原有的被动式扫描，高效的感知数据版本的变化，主动触发Compaction。<strong>在任务执行层面</strong>，对资源进行隔离，将重量级的 Base Compaction 和轻量级的Cumulative Compaction 进行了物理分离，防止任务的互相影响。同时，<strong>针对高频小文件的导入</strong>，优化文件合并策略，采用梯度合并的方式，保证单次合并的文件都属于同一数量级，逐渐有层次的进行数据合并，减少单个文件参与合并的次数，大幅节省系统消耗。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="负载隔离">负载隔离<a href="#负载隔离" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>最初我们只有 1 个 Doris 集群，Doris 集群要同时支持高频实时写、 高并发查询、ETL 处理以及Adhoc 查询等功能。其中高频实时写对 CPU 的占用很高，而 CPU 的上限决定高并发查询的能力，另外 Adhoc 查询无法预知 SQL 的复杂度，当复杂度过高时也会占用较高的内存资源。这就导致了资源竞争，业务之前互相影响的问题。为解决这些问题，我们进行了以下探索优化。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/6657aca763b64629af22fe809322fe5e~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><ol><li><strong>Doris 集群拆分</strong></li></ol><p>最初我们尝试对 Doris 集群进行拆分，我们把 1 个集群拆分为 3 个集群，分别为 ODS 集群、DW 集群、ADS 集群。我们将 CPU 负载最高的 ODS 层分离出去， ETL 时，通过 Doris 外表连接另一个 Doris 集群抽取数据；同时也将 BI 应用访问的集群分离出去，独立为业务提供数据查询。如下所示为各集群负责的任务：</p><ul><li><p>ODS 集群：数仓 ODS 层，Flink 写数据集中在此层进行。</p></li><li><p>DW 集群：数仓 DW 层，DIM 层，主要负责 ETL 处理，Adhoc 查询任务。</p></li><li><p>ADS 集群：数仓 ADS 层，主要支持 Web 应用的数据查询</p></li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3be48754afb749d89f74fff965df965f~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>通过集群拆分，有效降低各个资源间的相互影响，保证每个业务运转都有较充足的资源。但是集群的拆分也存在集群之间数据同步 ETL 时间较长、从 ADS 到 ODS 跨3个集群的数据校验复杂度较高等问题。直到 Doris 0.15 发布后，这些问题也得到了相对有效的解决。</p><ol start="2"><li><strong>资源隔离优化集群资源</strong></li></ol><p>Doris 0.15 版本新增了资源标签功能以及查询 Block 功能，资源标签功能允许 Doris 集群实现资源隔离，该功能有效减少集群之间同步数据的时间，降低了跨集群数据校验复杂度。其次查询 Block 功能的上线，可以对 SQL 进行查询审计，阻塞简单/不合规的查询语句，降低资源占用率，提升查询性能。除此之外，通过资源隔离的方式，我们将 3 个集群合并成 1 个集群，被合并的 6 个入口节点 FE 被释放掉，将节省的资源加到核心的运算节点上来。</p><p>升级到 Doris 0.15 后，我们将 ODS 表的副本修改为 <code>group_ods</code>3份，<code>default</code>3份。 Flink 写入时只写 <code>group_ods</code> 资源组的节点，数据写入后，得益于 Doris 内部的副本同步机制，数据会自动实时同步到 <code>default</code>资源组。ETL 则可以使用 <code>default</code> 资源组的节点资源取用 ODS 数据，进行查询和数据处理。同理 ADS 也做了相同处理，原先需要通过外表进行数据抽取同步的表，均被做成了副本跨资源组的形式。此方式有效缩短了跨集群数据同步的 ETL 时长 。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/2805d319336a475b8923bb7651ccafaa~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="离线-etl-内存高">离线 ETL 内存高<a href="#离线-etl-内存高" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们使用的是离线 ETL 方式直接在 Doris 上做 ETL 操作，在 Join 时，如果右表数据量比较大的情况下会消耗大量的内存，从而造成 OOM。在 1.0 版本之前内存跟踪能力较弱，容易造成 BE 节点超出 Linux 限制，导致进程被关闭 ，这时候会收到以下报错信息：<code>Host is down</code>或者 <code>Fail to initialize storage reader</code>。在1.0 及更高版本中， Doris 由于优化了内存跟踪，则容易见到以下报错：<code>Memory exceed limit. Used: XXXX ,Limit XXXX.</code></p><p><strong>针对内存受限问题，我们开始寻找优化方案，另外由于公司内部资源受限，优化方案必须在不增加集群成本的情况下把超出集群负荷的任务跑通。这里为大家介绍 2 个解决方法：</strong></p><ol><li>优化调整 Join 的方式：</li></ol><p>Doris 内部 Join 分为 4 种，其内存开销以及优先级如下图所示：</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/0009e1b36db3405c8351e56e25a6635a~tplv-k3u1fbpfcp-zoom-1.image" alt="img" class="img_ev3q"></p><p>从上图可知，Join 类型优先级从左往右依次变低，Shuffle 的优先级最低，排在 Broadcast 之后。值得注意的是， Broadcast 内存开销非常大，它将右表广播到所有 BE 节点，这相当于每个 BE 节点会消耗一个右表的内存，这将造成很大的内存开销。<strong>针对 Broadcast 比较大的内存开销，我们通过 Hint 条件强制 Join 类型的方式，使 Join 语句跳过 Broadcast 到 Shuffle Join ，从而降低内存消耗</strong>。</p><div class="codeBlockContainer_Ckt0 theme-code-block" style="--prism-color:#393A34;--prism-background-color:#f6f8fa"><div class="codeBlockContent_biex"><pre tabindex="0" class="prism-code language-text codeBlock_bY9V thin-scrollbar"><code class="codeBlockLines_e6Vv"><span class="token-line" style="color:#393A34"><span class="token plain">select * from a join [shuffle] b on a.k1 = b.k1;</span><br></span></code></pre><div class="buttonGroup__atx"><button type="button" aria-label="复制代码到剪贴板" title="复制" class="clean-btn"><span class="copyButtonIcons_eSgA" aria-hidden="true"><svg viewBox="0 0 24 24" class="copyButtonIcon_y97N"><path fill="currentColor" d="M19,21H8V7H19M19,5H8A2,2 0 0,0 6,7V21A2,2 0 0,0 8,23H19A2,2 0 0,0 21,21V7A2,2 0 0,0 19,5M16,1H4A2,2 0 0,0 2,3V17H4V3H16V1Z"></path></svg><svg viewBox="0 0 24 24" class="copyButtonSuccessIcon_LjdS"><path fill="currentColor" d="M21,7L9,19L3.5,13.5L4.91,12.09L9,16.17L19.59,5.59L21,7Z"></path></svg></span></button></div></div></div><ol start="2"><li><strong>数据分批处理</strong></li></ol><p>我们尝试将数据按照时间分批，每批涵盖某一个或某几个时间段的数据，分批进行 ETL，有效降低内存消耗，避免 OOM 。分批须知：需要将分批的标记列放在主键中，最大程度提升搜索数据的效率；注意分桶和分区的设置方式，保证每个分区的数据量都比较均衡，避免个别分区内存占用较高的问题。</p><h1>总结</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="新架构收益"><strong>新架构收益：</strong><a href="#新架构收益" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ul><li><p>基于 Doris 的新数仓架构不再依赖 Hadoop 生态组件，运维简单，维护成本低。</p></li><li><p>具有更高性能，使用更少的服务器资源，提供更强的数据处理能力。</p></li><li><p>支持高并发，能直接支持 WebApp 的查询服务。</p></li><li><p>支持外表，可以很方便的进行数据发布，将数据推送其他数据库中。</p></li><li><p>支持动态扩容，数据自动平衡。</p></li><li><p>支持多种联邦查询方式，支持 Hive、ES、MySQL 等</p></li></ul><p>得益于新架构的优异能力，我们所用集群从 <strong>18 台 16C128G 减少到 12 台 16C128G</strong>，集群资源较之前<strong>节省了33%</strong> ，大大降低了投入成本；并且运算性能得到大幅提升，在 Kudu 上 <strong>3 小时</strong>即可完成的 ETL 任务， Doris 只需要 <strong>1 小时</strong>即可完成 。除此之外，高频更新的场景下，Kudu 内部数据碎片文件不能进行自动合并，表的性能会越来越差，需要定期重建；而Doris 内部的 Compaction 机制可以有效避免此问题。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="社区寄语">社区寄语：<a href="#社区寄语" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>首先，Doris 的使用成本很低，仅需要 3 台低配服务器、甚至是台式机，就能相对容易的部署一套基于 Doris 的数仓作为数据中台基础；我认为对于想要进行数字化，但介于资源投入有限而又不想落后于市场的企业来说，非常建议尝试使用 Apache Doris ，Doris 可以助力企业低成本跑通整个数据中台。</p><p>其次，Doris 是一款国人自研的的 MPP 架构分析型数据库，这令我感到很自豪，同时其社区十分活跃、便于沟通，Doris 背后的商业化公司 SelectDB 为社区组建了一支专职技术团队，任何问题都能在 1 小时内得到响应，近 1 年社区更是在 SelectDB 的持续推动下，推出了一系列十分抗打的新特性。另外社区在版本迭代时会认真考虑中国人的使用习惯，这些会为我们的使用带来很多便利。</p><p>最后，感谢 Doris 社区和 SelectDB 团队的全力支持，也欢迎开发者以及各企业多多了解 Doris、使用 Doris，支持国产数据！</p><h1>1.2.0 版本传送门</h1><p>Apache Doris 于 2022 年 12 月 7 日迎来 1.2.0 Release 版本的正式发布！新版本中<strong>实现了全面的向量化、实现多场景查询性能 3-11 倍的提升</strong>，在 Unique Key 模型上<strong>实现了 Merge-on-Write 的数据更新模式、数据高频更新时查询性能提升达 3-6 倍</strong>，增加了 Multi-Catalog 多源数据目录、<strong>提供了无缝接入</strong> <strong>Hive</strong>、ES、Hudi、Iceberg 等外部数据源的能力 <strong>，引入了 Light Schema Change 轻量表结构变更、</strong> 实现毫秒级的 Schema Change 操作<strong>并可以借助 Flink CDC</strong> 自动同步上游数据库的 DML 和 <strong>DDL</strong> <strong>操作</strong>，以 JDBC 外部表替换了过去的 ODBC 外部表，支持了 Java UDF 和 Romote UDF 以及 Array 数组类型和 JSONB 类型，修复了诸多之前版本的性能和稳定性问题，推荐大家下载和使用！</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="下载安装">下载安装<a href="#下载安装" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>GitHub下载：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fdoris%2Freleases" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris/releases</a></p><p>官网下载页：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fdoris.apache.org%2Fdownload" target="_blank" rel="noopener noreferrer">https://doris.apache.org/download</a></p><p>源码地址：<a href="https://www.oschina.net/action/GoToLink?url=https%3A%2F%2Fgithub.com%2Fapache%2Fdoris%2Freleases%2Ftag%2F1.1.3-rc024" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris/releases/tag/1.2.0-rc04</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/LY">应用实践：数仓体系效率全面提升！同程数科基于 Apache Doris 的数据仓库建设</a></h2><div class="blog-info"><time datetime="2022-12-19T00:00:00.000Z" itemprop="datePublished">2022年12月19日</time><span class="split-line"></span><span class="authors"><span class="s-author">王星</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>导读：同程数科成立于 2015 年，是同程集团旗下的旅游产业金融服务平台。2020 年，同程数科由于看到了 Apache Doris 丰富的数据接入方式、优异的并行运算能力和极简运维的特性，引入了Apache Doris 进行数仓架构改造。本文详细讲述了同程数科数仓架构从1.0 到 2.0 的演进过程及使用Doris过程中的应用实践。希望对大家有所帮助。</p></blockquote><blockquote><p>作者｜同程数科大数据高级工程师 王星</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/zh-CN/assets/images/kv-fb77e142257a98bea6656a33a626b310.png" width="900" height="383" class="img_ev3q"></p><h1>业务背景</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务介绍">业务介绍<a href="#业务介绍" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>同程数科是同程集团旗下的旅游金融服务平台，其前身是同程金服。正式成立于 2015 年，同程数科以“数字科技引领旅游产业”为愿景，坚持以科技的力量，赋能我国旅游产业。
目前，同程数科的业务涵盖金融服务、消费金融服务、金融科技及数字科技等板块，累计服务覆盖超过千万用户和 76 座城市。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="业务需求">业务需求<a href="#业务需求" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>包含四大类：</p><ul><li>看板类：包括实时驾驶舱以及 T+1 业务看板等。</li><li>预警类：包括风控熔断、资金异常以及流量监控等。</li><li>分析类：包括及时性数据查询分析以及临时取数等。</li><li>财务类：包括清算以及支付对账需求。</li></ul><h1>架构演进之 1.0</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="工作流程">工作流程<a href="#工作流程" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="1" src="https://cdnd.selectdb.com/zh-CN/assets/images/1-bb0a3cc8027796049893fcf4d60dd5ab.png" width="1080" height="608" class="img_ev3q"></p><p>我们最初的数仓架构沿袭了前几年非常流行的SteamSets 和 Apache Kudu 组合的第一代架构。该架构中，Binlog 通过StreamSets后，通过实时采集后写入 Apache Kudu 中，最后通过 Apache Impala 和可视化工具进行查询和使用。</p><p>不足：</p><ul><li>组件引入过多，维护成本随之增加</li><li>多种技术架构和过长的开发链路，提高了数仓研发人员的学习成本，数仓人员需要在不同组件之间进行开发，导致开发效率降低。</li><li>Apache Kudu 在大表关联 Join 方面性能差强人意。</li><li>由于数仓使用了 CDH组件搭建，离线和实时集群并未进行分离，形成资源之间的相互竞争；在离线数据批量处理时对 IO 或磁盘消耗较大，无法保证实时数据的及时性。</li><li>虽然 SteamSets 配备了预警能力，但作业恢复能力仍相对欠缺。在配置多个任务时， JVM 的消耗较大，导致恢复速度较慢。</li></ul><h1>架构演进之 2.0</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="调研过程">调研过程<a href="#调研过程" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>由于缺点众多，我们不得不放弃了数仓1.0的架构。在 2020年中，我们对市面上流行的数仓进行了深度调研。</p><p>在调研过程中，我们集中对比了Click house和Apache Doris。ClickHouse 对 CPU 的利用率较高，所以在单表查询时表现比较优秀，但是在多查询高 QPS 的情况下则表现欠佳。反观Doris不仅单节点最高可支持上千QPS，而且得益于分区分桶裁剪的功能，可以支持QPS万级别的高并发查询；再者，ClickHouse的扩容缩容复杂且繁琐，目前做不到自动在线操作，需要自研工具支持。Doris支持集群的在线动态扩缩容，且可以随着业务的发展水平扩展。</p><p>在调研中，ApacheDoris脱颖而出。Doris高并发的查询能力非常吸引我们，而且灵活的扩缩容能力也也更适合我们灵活多变的广告业务。因此我们选择了 Apache Doris。</p><p><img loading="lazy" alt="2" src="https://cdnd.selectdb.com/zh-CN/assets/images/2-11b7311ef2c06a545dbdb54e01787f6d.png" width="1080" height="608" class="img_ev3q"></p><p>引入 Apache Doris 后，我们对整个数仓进行了改造：</p><ul><li>通过Canal 采集MySQL Binlog 进入 Kafka中。因为Apache Doris 与 Kafka 的契合度较高，可以便捷地使用 Routine Load 对数据加载和导入。</li><li>我们对原有离线计算的数据链路进行了细微调整。对于存储在 Hive 中的数据，Apahce Doris 可以通过 Broker Load 将 Hive 中的数据导入。这样一来离线集群的数据就可以直接加载到 Doris。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="选择-doris">选择 Doris<a href="#选择-doris" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="3" src="https://cdnd.selectdb.com/zh-CN/assets/images/3-1a638414ccc0a8decbd99b24160973a8.png" width="1080" height="608" class="img_ev3q"></p><p>Apache Doris 整体表现令人深刻：</p><ul><li>数据接入：提供了丰富的数据导入方式，能够支持众多数据源的接入。</li><li>数据连接：Doris 支持 JDBC 与 ODBC 等方式连接。Doris对 BI 工具的可视化展示比较友好，能够便捷地与 BI 工具进行连接。另外Doris 采用MySQL 协议进行通信，用户可以通过各类 Client 工具直接访问 Doris。</li><li>SQL 语法：Doris 采用MySQL 协议，高度兼容MySQL 语法，支持标准SQL，对于数仓开发人员来说学习成本较低；</li><li>MPP 并行计算：Doris 基于 MPP 架构，提供了非常优秀的并行计算能力。在复杂Join和大表Join的场景下Doris优势非常明显；</li><li>文档健全：Doris 官方文档非常健全，对于新用户上手非常友好。（我们最看重的一点）</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris-实时系统架构">Doris 实时系统架构<a href="#doris-实时系统架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="4" src="https://cdnd.selectdb.com/zh-CN/assets/images/4-eeefe38e4d58d7a253a72ca3c98ecace.png" width="1080" height="608" class="img_ev3q"></p><ul><li><p>数据源：在实时系统架构中，数据源来自产业金融、消费金融、风控数据等业务线，通过 Canal 和 API 接口进行采集。</p></li><li><p>数据采集：通过 Canal- Admin 进行数据采集后，Canal将数据发送到 Kafka 消息队列之中。之后，数据再通过 Routine Load 接入到 Doris 集群。</p></li><li><p>Doris 数仓：由Doris 集群组成了了数据仓库的三级分层，分别是：使用了 Unique 模型的 DWD 明细层 、 Aggregate 模型的 DWS 汇总层以及 ADS 应用层。</p></li><li><p>数据应用：数据应用于实时看板、数据及时性分析以及数据服务三方面。</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris-新数仓特点">Doris 新数仓特点<a href="#doris-新数仓特点" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>数据导入方式简便，根据不同场景采用 3 种不同的导入方式：</p><ul><li>Routine Load：当我们提交 Rountine Load 任务时，Doris 内部会有一个常驻进程实时消费 Kafka ，不断从 Kafka 中读取数据并导入进 Doris中。</li><li>Broker Load：维度表及历史数据等离线数据有序地导入Doris。</li><li>Insert Into：用于定时批式计算任务，负责处理DWD 层数据，从而形成 DWS 层以及 ADS 层。
Doris 的良好数据模型，提升了我们的开发效率：</li><li>Unique 模型在 DWD 层接入时使用，可以有效防止重复消费数据。</li><li>Aggregate 模型用作聚合。在 Doris 中，Aggregate 支持如 Sum、Replace、Min 、Max 4 种方式的聚合模型，聚合的过程中使用 Aggregate 底层模型可以减少很大部分 SQL 代码量，不再人工手写Sum、Min、Max 等代码。
Doris 查询效率高：</li><li>支持物化视图与 Rollup 物化索引。物化视图底层类似 Cube 的概念与预计算的过程，与 Kylin 中以空间换时间的方式类似，均是在底层生成特殊的表，在查询中命中物化视图并快速响应。</li></ul><h1>新架构的收益</h1><ul><li>数据接入：在最初的架构中，通过 SteamSets 进行数据接入的过程中需要手动建立 Kudu 表。由于缺乏工具，整个建表和创建任务的过程需要 20-30 分钟。如今可以通过平台与快速构建语句实现数据快速接入，每张表的接入过程从之前的20-30分钟缩短到现在的 3-5 分钟，性能提升了 5-6 倍。</li><li>数据开发：使用 Doris之后，我们可以直接使用 Doris 中自带的 Unique、Aggregate 等数据模型及可以很好支持日志类场景的 Duplicate 模型，在 ETL 过程中大幅度加快开发过程。</li><li>查询分析：Doris 底层带有物化视图及 Rollup 物化索引等功能。物化视图底层类似 Cube 的概念与预计算的过程，与 Kylin 中以空间换时间的方式类似，均是在底层生成特殊的表，在查询中命中物化视图并快速响应。同时 Doris 底层对于大表关联进行了诸多优化，如 Runtime Filter 以及其他 Join 和自定义优化。相较于 Doris，Apache Kudu 则需要经过更为深入和复杂的优化才能更好地使用。</li><li>数据报表：我们最初使用 Kudu 报表查询需要 1-2 分钟才能够完成渲染，而 Doris 则是秒级甚至是毫秒级的响应速度。</li><li>便捷运维：Doris 没有 Hadoop 生态系统的复杂度，维护成本远低于 Hadoop。尤其是在集群迁移过程中，Doris 的运维便捷性尤为突出。3 月份，我们的机房进行了搬迁，12 台 Doris 节点机器在三天内全部迁移完成。整体操作较为简单，除了机器上架下架和搬移外，FE 扩容与缩容时只运用了 Add 与 Drop 等简单指令，并未消耗太长时间。</li></ul><h1>未来展望</h1><ul><li>实现基于 Flink CDC 的数据接入：当前，优化后的数据库架构中并没有没有引入 Flink CDC ，而是继续沿用了 数据经Canal 采集到 Kafka 后再采集到 Doris 中的模式，链路相对来说较长。使用Flink CDC 虽然可以继续精简整体架构，但是还需要写一定量的代码，这对于数据分析师直接使用感受并不友好。我们希望数据分析师只需要写简单SQL 或在页面上直接操作。在未来的规划中，我们计划引入 Flink CDC 功能并对上层应用进行扩充。</li><li>紧跟社区迭代计划：我们正在使用的 Doris 版本相对较老，现在的最新版本 Apache Doris V1.2.0在全面向量化、multi-catalog多元数据目录、light schema change轻量表结构变更方面有了较大幅度的提升。我们将紧跟社区迭代节奏对集群进行升级并充分利用新特性。</li><li>强化建设相关体系：我们现在的指标体系管理如报表元数据、业务元数据等维护与管理水平依旧有待提高。在数据质量监控方面，虽然目前包含了数据质量监控功能，但对于整个平台监控与数据自动化监控方面还需要强化与改善。</li></ul></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/BestPractice_Kwai">Doris on Es在快手商业化的最佳实践</a></h2><div class="blog-info"><time datetime="2022-12-14T00:00:00.000Z" itemprop="datePublished">2022年12月14日</time><span class="split-line"></span><span class="authors"><span class="s-author">贺祥</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>作者：贺祥，数据架构高级工程师，快手商业化团队</p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/zh-CN/assets/images/kv-846e4e39fd88e1e34d2474b23690d9b2.png" width="900" height="383" class="img_ev3q"></p><h1>1 关于快手</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="11-快手">1.1 快手<a href="#11-快手" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>快手（HKG: 1024）是一个短视频和潮流社交网络。发现有趣的短片，通过生活中的录音、视频、玩日常挑战或喜欢最好的动效模版和视频来为虚拟社区做出贡献。用短视频分享生活，并从数十种神奇的效果和滤镜中选择喜欢的方式。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="12-快手商业化报表引擎">1.2 快手商业化报表引擎<a href="#12-快手商业化报表引擎" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>快手商业化报表引擎为外部广告主提供广告投放效果的实时多维分析报表在线查询服务，以及为内部各商业化系统提供多维分析报表查询服务，致力于解决多维分析报表场景的高性能、高并发、高稳定的查询问题。</p><h1>2 初期架构</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="21-需求背景">2.1 需求背景<a href="#21-需求背景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>传统 OLAP 引擎应对多维分析时更多是以预建模的方式，通过构建数据立方体（Cube）对事实数据进行下钻、上卷、切片、切块等操作。现代 OLAP 分析引入了关系模型的理念，在二维关系表中描绘数据。而在建模过程中，往往有两种建模方式，一是采用宽表模型、将多张表的数据通过 Join 写入进一张宽表中，另一种方式是采用星型模型、将数据表区分为事实表和维度表、查询时对事实表与维度表进行 Join 。
以上两种方案各有部分优缺点：</p><p>宽表模型：</p><p>采取空间换时间的思路，理论上都是维表主键为唯一 ID 来填充所有维度，冗余存储了多条维度数据。其优势在于查询时非常方便，无需关联额外维表，性能表现更佳。其弊端在于如果有维度数据变化，需要对全表数据进行重刷，无法支撑高频的 Update。</p><p>星型模型：</p><p>维度数据与事实数据完全分离，维度数据往往用专门的引擎存储 (如 MySQL、Elasticsearch 等)，查询时通过主键关联查询维度数据，其优势在于维度数据变化不影响事实数据、可支持高频 Update 操作。其弊端在于查询逻辑相对更复杂，且多表 Join 可能导致性能受损。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="22-业务需求">2.2 业务需求<a href="#22-业务需求" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>在快手的业务场景中，商业化报表引擎承载了外部广告主实时查询广告投放效果的需求，在构建报表引擎时，我们期望可以满足如下要求：</p><ul><li>超大数据量：单表原始数据每天增量百亿</li><li>查询高 QPS：平均 QPS千级别</li><li>高稳定性要求：在线服务要求稳定性4个9
最为重要的是，由于维度数据经常发生变更，维度表需要支持高达上千 QPS 的 Update 操作，同时还要进一步支持模糊匹配、分词检索等需求。
基于以上需求，我们选择了星型模型来建模，并以 Apache Druid 和 Elasticsearch 为核心构建了早期的报表引擎架构。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="23-初期架构基于apache-druid的架构">2.3 初期架构：基于Apache Druid的架构<a href="#23-初期架构基于apache-druid的架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们选择了引擎结合的方式，用Elasticsearch适配Druid引擎来实现。在数据写入阶段，我们通过Flink对数据进行分钟级预聚合，利用Kafka对数据进行小时级别的数据预聚合。在数据查询中，App端发起查询需求，对RE Front统一接口进行查询，Re Query根据引擎适配，向维表引擎（Elasticsearch和MySQL）及扩展引擎分别发起查询。</p><p>Druid则是一款基于时序的查询引擎，支持数据实时摄入，用来存储和查询大量的事实数据。而选用Elasticsearch作为维度数据存储引擎，主要是因为如下原因：</p><ul><li>支持高频实时更新，可以支撑上千 QPS的 Update操作</li><li>支持分词模糊检索，适用于快手的业务</li><li>支持量级较高的维表数据，不用像MySQL数据库一样做分库分表才能满足</li><li>支持数据同步监控，同时拥有检查和恢复的服务</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="24-报表引擎">2.4 报表引擎<a href="#24-报表引擎" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>报表引擎架构整体分为REFront 和 REQuery两层，REMeta为独立的元数据管理模块。报表引擎在REQuery内部实现MEM Join。支持Druid引擎中的事实数据与ES引擎中的维度数据做关联查询。为上层业务提供虚拟的cube表查询。屏蔽复杂的跨引擎管理查询逻辑。</p><p><img loading="lazy" alt="1" src="https://cdnd.selectdb.com/zh-CN/assets/images/1-ca39b5dcdd3d69d8fee822dc1b74a2e5.png" width="864" height="885" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="3-基于apache-doris的架构">3 基于Apache Doris的架构<a href="#3-基于apache-doris的架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><h2 class="anchor anchorWithStickyNavbar_LWe7" id="31-架构遗留的问题">3.1 架构遗留的问题<a href="#31-架构遗留的问题" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>首先，我们在使用报表引擎时，发现了这样的一个问题。Mem Join是单机实现与串行执行，到单次从ES中拉取的数据量超过10W时，响应时间已经接近10s，用户体验差。而且单节点实现大规模数据Join处理，内存消耗大，有Full GC风险。</p><p>其次，Druid的Lookup Join了功能不够完善是一个较大的问题，不能完全满足真实业务需求。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="32-选型调研">3.2 选型调研<a href="#32-选型调研" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>于是我们对业界常见的 OLAP 数据库进行了调研，其中最具代表性的为 Apache Doris和 Clickhouse。在进一步的调研中我们发现，Apache Doris在大宽表Join的能力更强。ClickHouse能够支持 Broadcast 基于内存的Join，但是对于大数据量千万级以上大宽表的Join，ClickHouse 的性能表现不好。Doris 和 Clickhouse 都支持明细数据存储，但Clickhouse支持的并发度较低，相反Doris支持高并发低延时的查询服务，单机最高支持上千QPS。在并发增加时，线性扩充FE和BE即可支持。而Clickhouse的数据导入没有事务支持功能，无法实现exactly once语义，对标准sql的支持也是有限的。相比之下，Doris提供了数据导入的事务支持和原子性，Doris 自身能够保证不丢不重的订阅 Kafka 中的消息，即 Exactly-Once 消费语义。ClickHouse使用门槛高、运维成本高和分布式能力弱，需要较多的定制化和较深的技术实力也是另一个难题，Doris则不同，只有FE、BE两个核心组件，外部依赖也比较少，运维快捷简单。我们还发现，由于Doris 更加接近 MySQL协议，比起Clickhouse更加便捷，在迁移时的成本并不大。在横向扩容方面，Doris 的扩缩容也能够做到自平衡，大大优于Clickhouse。</p><p>由此看来Doris可以比较好的提升Join的性能，在迁移成本、横向扩容、并发程度等其他方面也比较优秀。不过在高频Update上，Elasticsearch具有先天的优势。</p><p>通过 Doris 创建 ES 外表的方式来同时应对高频Upate和Join性能问题，会是比较理想的解决方案。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="33-dorisdoris-on-es完美配合">3.3 Doris+Doris on ES完美配合<a href="#33-dorisdoris-on-es完美配合" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>Doris on ES 的查询性能究竟如何呢？</p><p>首先，Apache Doris 是一个基于MPP 架构的实时分析型数据库，性能强劲、横向扩展能力能力强。Doris on ES构建在这个能力之上，并且对查询做了大量的优化。其次，在这些之上，融合Elasticsearch的能力之后，我们还对查询功能做出了大量的优化：</p><ul><li>Shard级别并发</li><li>行列扫描自动适配，优先列式扫描</li><li>顺序读取，提前终止</li><li>两阶段查询变为一阶段查询</li><li>Join场景使用Broadcast Join，对于小批量数据Join特别友好</li></ul><p><img loading="lazy" alt="2" src="https://cdnd.selectdb.com/zh-CN/assets/images/2-b0e578721df866bb977d80072c559f32.png" width="864" height="800" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="34-基于doris-on-elasticsearch的架构实现">3.4 基于Doris on Elasticsearch的架构实现<a href="#34-基于doris-on-elasticsearch的架构实现" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="341-数据链路升级">3.4.1 数据链路升级<a href="#341-数据链路升级" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>数据链路的升级适配比较简单。第一步，由Doris构建新的Olap表，配置好物化视图。第二步，基于之前事实数据的kafka topic启动routine load，导入实时数据。第三步，从Hive中通broker load导入离线数据。最后一步，通过Doris创建Es外表。</p><p><img loading="lazy" alt="3" src="https://cdnd.selectdb.com/zh-CN/assets/images/3-1c0a381d13453a0e975d97ffab096981.png" width="864" height="629" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="342-报表引擎适配升级">3.4.2 报表引擎适配升级<a href="#342-报表引擎适配升级" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" alt="4" src="https://cdnd.selectdb.com/zh-CN/assets/images/4-3fca61e78e95bd16fad48de37ee7124f.png" width="864" height="722" class="img_ev3q"></p><p>注：上图关联的mysql维表是基于未来规划，目前主要是ES做维表引擎</p><p>报表引擎适配</p><ul><li>抽象基于Doris的星型模型虚拟cube表</li><li>适配cube表查询解析，智能下推</li><li>支持灰度上线</li></ul><h1>4  线上表现</h1><h2 class="anchor anchorWithStickyNavbar_LWe7" id="41-查询响应时间">4.1 查询响应时间<a href="#41-查询响应时间" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="411-事实表查询表现对比">4.1.1 事实表查询表现对比<a href="#411-事实表查询表现对比" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Druid</p><p><img loading="lazy" alt="5" src="https://cdnd.selectdb.com/zh-CN/assets/images/5-4ce2b0396e8e14ac9e536befcf11cfd0.png" width="864" height="200" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="6" src="https://cdnd.selectdb.com/zh-CN/assets/images/6-80fe6a32cf89065b0323afca7038f0ba.png" width="864" height="195" class="img_ev3q"></p><p>99分位耗时Druid大概为270ms，Doris为150ms，延时下降45%</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="412-join场景下cube表查询表现对比">4.1.2 Join场景下cube表查询表现对比<a href="#412-join场景下cube表查询表现对比" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Druid</p><p><img loading="lazy" alt="7" src="https://cdnd.selectdb.com/zh-CN/assets/images/7-30f72edbee75326a65e006652a846e63.png" width="864" height="197" class="img_ev3q"></p><p>Doris</p><p><img loading="lazy" alt="8" src="https://cdnd.selectdb.com/zh-CN/assets/images/8-b9485387d33cb96f36fdcaf09999ce2a.png" width="864" height="193" class="img_ev3q"></p><p>99分位耗时Druid大概为660ms，Doris为440ms，延时下降33%</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="413-收益总结">4.1.3 收益总结<a href="#413-收益总结" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ul><li>P99整体耗时下降35%左右</li><li>资源节省50%左右</li><li>去除报表引擎内部Mem Join的复杂逻辑，下沉至Doris通过DOE实现，在大查询场景下(维表结果超过10W，性能提升超过10倍，10s-&gt;1s)</li><li>更丰富的查询语义(原本Mem Join实现比较简单，不支持复杂的查询)</li></ul><h1>5  总结与未来规划</h1><p>在快手商业化业务里面，维度数据与事实数据Join查询是非常普遍的。使用Doris 之后，查询变得简单。我们仅需要按天同步事实表和维表，在查询的同时 Join即可。通过Doris替代Druid、Clickhouse的方案，基本覆盖了我们使用Druid 时的所有场景，大大提高了海量数据的聚合分析能力。在Apache Doris的使用过程中，我们还发现了一些意想不到的收益：例如，Routine Load和 Broker Load的导入方式较为简单，提升了查询速度；数据占用空间大幅降低；Doris支持MySQL协议，方便了数据分析师自助取数绘图等。</p><p>尽管Doris on ES的解决方案比较成功的满足了我们的报表业务，ES外表映射仍然需要手工建表。但Apache Doris于近日完成了最新版本V1.2.0的发布，新版本功能新增了Multi-Catlog，提供了无缝接入Hive、ES、Hudi、Iceberg 等外部数据源的能力。用户可以通过 CREATE CATALOG 命令连接到外部数据源，Doris 会自动映射外部数据源的库、表信息。如此一来，以后我们就不需要再手动创建Es外表完成映射，大大节省了开发的时间成本，提升了研发效率。而全面向量化、Ligt Schema Change、Merge-on-Write、Java UDF等其他新功能的实现，也让我们对Apache Doris有了全新的期待。祝福Apache Doris！</p><h1>联系我们</h1><p>官网：<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Github：<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>dev邮件组：<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/xiaomi_vector">最佳实践: Apache Doris 在小米数据场景的应用实践与优化</a></h2><div class="blog-info"><time datetime="2022-12-08T00:00:00.000Z" itemprop="datePublished">2022年12月8日</time><span class="split-line"></span><span class="authors"><span class="s-author">魏祚</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>导读：小米集团于 2019 年首次引入了 Apache Doris ，目前 Apache Doris 已经在小米内部数十个业务中得到广泛应用，并且在小米内部已经形成一套以 Apache Doris 为核心的数据生态。本篇文章转录自 Doris 社区线上 Meetup 主题演讲，旨在分享 Apache Doris 在小米数据场景的落地实践与优化实践。</p></blockquote><blockquote><p>作者｜魏祚 小米 OLAP 引擎研发工程师 </p></blockquote><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/zh-CN/assets/images/kv-b27d71e34981d9850785329cea2cb610.png" width="900" height="383" class="img_ev3q"></p><h1>关于小米</h1><p>小米公司（“小米”或“集团”；HKG：1810），一家消费电子和智能制造公司，其智能手机和智能硬件通过物联网 (IoT) 平台连接。 2021年，小米总收入达到人民币3283亿元（4722.3131.62亿美元），同比增长33.5%；调整后净利润为人民币 220 亿元（316,451.08 万美元），同比增长 69.5%。</p><p>因分析业务的增长，小米集团于 2019 年首次引入了 Apache Doris 。经过三年时间的发展，目前 Apache Doris 已经在广告投放、新零售、增长分析、数据看板、用户画像、天星数科、小米有品、等小米内部数十个业务和品牌中得到广泛应用，并且在小米内部已经围绕 Apache Doris 为核心建设了数据生态。</p><p><img loading="lazy" alt="1" src="https://cdnd.selectdb.com/zh-CN/assets/images/1-1ba7f77a03c987c9397cedee505fe819.png" width="1080" height="600" class="img_ev3q"></p><p>当前 Apache Doris 在小米内部已经具有数十个集群、总体达到数百台 BE 节点的规模，其中单集群最大规模达到近百台节点，拥有数十个实时数据同步任务，每日单表最大增量 120 亿、支持 PB 级别存储，单集群每天可以支持 2W 次以上的多维分析查询。</p><h1>架构演进</h1><p>小米引入 Apache Doris 的初衷是为了解决内部的用户行为分析中所遇到的问题。随着小米互联网业务的发展，利用用户行为数据进行增长分析的需求越来越强烈。如果每个业务产品线都自己搭建一套增长分析系统，不仅成本高昂，效率也不高。因此如果能有一款产品能够帮助他们不用关心底层的复杂技术细节，让相关业务人员能够专注于自己的技术工作，可以极大提高工作效率。所以，小米大数据和云平台联合开发了增长分析系统 Growing Analytics（下文中简称 GA)，旨在提供一个灵活的多维实时查询和分析平台，可以统一管理数据接入和查询方案，帮助业务线做好精细化运营。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="历史架构">历史架构<a href="#历史架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>增长分析平台立项于 2018 年年中，当时基于开发时间和成本，技术栈等因素的考虑，小米复用了现有各种大数据基础组件（HDFS, Kudu, SparkSQL 等），搭建了一套基于 Lamda 架构的增长分析查询系统。GA 系统初代版本的架构如下图所示，包含了以下几个方面：</p><ul><li>数据源：数据源是前端的埋点数据以及用户行为数据。</li><li>数据接入层：对埋点数据进行统一的清洗后写入小米内部自研的消息队列中，并通过 Spark Streaming 将数据导入Kudu 中。</li><li>存储层：在存储层中进行冷热数据分离。热数据存放在 Kudu 中，冷数据则会存放在 HDFS 上。同时在存储层中进行分区，当分区单位为天时，每晚会将一部分数据转冷并存储到 HDFS 上。</li><li>计算层/查询层：在查询层中，使用 SparkSQL 对 Kudu 与 HDFS 上数据进行联邦查询，最终把查询结果显示在前端页面。</li></ul><p><img loading="lazy" alt="2" src="https://cdnd.selectdb.com/zh-CN/assets/images/2-f6f2fe0acf61bc2e3aefb9f853931c27.png" width="1080" height="603" class="img_ev3q"></p><p>在当时的历史背景下，初代版本的增长分析平台帮助我们解决了一系列用户运营过程中的问题，但同时在历史架构中也存在了两个问题：</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第一个问题组件分散">第一个问题：组件分散<a href="#第一个问题组件分散" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>由于历史架构是基于 SparkSQL + Kudu + HDFS 的组合，依赖的组件过多导致运维成本较高。原本的设计是各个组件都使用公共集群的资源，但是实践过程中发现执行查询作业的过程中，查询性能容易受到公共集群其他作业的影响，容易发生查询抖动，尤其在读取 HDFS 公共集群的数据时，有时较为缓慢。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="第二个问题资源占用高">第二个问题：资源占用高<a href="#第二个问题资源占用高" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>通过 SparkSQL 进行查询时，延迟相对较高。SparkSQL 是基于批处理系统设计的查询引擎，在每个 Stage 之间交换数据 Shuffle 的过程中依然需要进行落盘，完成 SQL 查询的时延较高。为了保证 SQL 查询不受资源的影响，我们通过添加机器来保证查询性能，但是实践过程中发现，性能提升的空间有限，这套解决方案并不能充分地利用机器资源来达到高效查询的目的，存在一定的资源浪费。</p><p>针对上述两个问题，我们的目标是寻求一款计算、存储一体的 MPP 数据库来替代我们目前的存储计算层的组件，在通过技术选型后，最终我们决定使用 Apache Doris 替换老一代历史架构。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="重新选型">重新选型<a href="#重新选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>MPP架构的查询引擎，如Impala,Presto等能够高效地支持SQL查询，但是仍然需要依赖Kudu, HDFS, Hive Metastore等组件, 运维成本依然比较高。同时，由于计算存储分离，查询引擎不能很好地及时感知存储层的数据变化，就无法做更细致的查询优化。如想在SQL层做缓存就无法保证查询的结果是最新的。</p><p>Doris是Apache基金会顶级项目，主要定位是高性能的、支持实时的分析型数据库， 主要用于解决报表和多维分析。它主要集成了 Google Mesa 和 Cloudera Impala 技术。我们对Doris进行了内部的性能测试并多次和社区沟通交流，确定了用Doris替换原来的计算存储组件的解决方案。我们新的架构如下图所示：</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="基于-apache-doris-的新版架构">基于 Apache Doris 的新版架构<a href="#基于-apache-doris-的新版架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>新版架构从数据源获取埋点数据后，数据接入后写入 Apache Doris 后可以直接查询结果并在前端进行显示。真正实现了通过Doris统一了计算、存储，和资源管理yarn相关工具。</p><p><img loading="lazy" alt="3" src="https://cdnd.selectdb.com/zh-CN/assets/images/3-266579e567d5c09c8931d7044813c707.png" width="1080" height="598" class="img_ev3q"></p><p>我们选择 Doris 原因：</p><ul><li>Doris 具有优秀的查询性能，能够满足业务需求。</li><li>Doris 支持标准 SQL ，用户使用与学习成本较低。</li><li>Doris 不依赖于其他的外部系统，运维简单。</li><li>Doris 社区拥有很高活跃度，版本迭代快。开发者规模大，有利于后续系统的维护升级。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="新旧架构性能对比">新旧架构性能对比<a href="#新旧架构性能对比" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="4" src="https://cdnd.selectdb.com/zh-CN/assets/images/4-c98f04af8754b651217aa474e7178d39.png" width="1061" height="546" class="img_ev3q"></p><p>我们选取了日均数据量大约 10 亿的业务，分别在不同场景下对Doris进行了性能测试，其中包含 6 个事件分析场景，3 个留存分析场景以及 3 个漏斗分析场景。经过与【SparkSQL+Kudu+HDFS】的旧方案对比后，我们发现：</p><ul><li>在事件分析的场景下，平均查询所耗时间降低了 85%。</li><li>在留存分析和漏斗分析场景下，平均查询所耗时间降低了 50%。</li></ul><h1>应用实践</h1><p>下面将介绍我们在Apache Doris应用中数据导入、数据查询、A/B测试的经验。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据导入">数据导入<a href="#数据导入" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="5" src="https://cdnd.selectdb.com/zh-CN/assets/images/5-adfeb633824992e5692635b13cfdfb78.png" width="1080" height="607" class="img_ev3q"></p><p>小米内部主要通过 Stream Load 与 Broker Load 以及少量 Insert 方式来导入数据到Doris。数据一般会先写入到消息队列中，分为实时数据和离线数据两个部分。
实时数据如何写入到Apache Doris 中：一部分实时数据通过 Flink数据处理 后， 并通过 Doris 社区提供的 Flink Doris Connector 组件写入到 Doris 中。另一部分数据通过 Spark Streaming 组件写入。这两种写入方式的底层都依赖的是社区提供的 Stream Load。
离线数据如何写入到Apache Doris 中：离线数据部分写入 Hive 后，通过小米的数据工场将数据导入到 Doris 中。用户可以直接在数据工场提交 Broker Load 任务并将数据直接导入 Doris 中，也可以通过 Spark SQL 将数据导入 。Spark SQL 方式则是依赖了 Doris 社区提供的 Spark Doris Connector 组件，其底层为 Doris 的 Stream Load 的封装。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="数据查询">数据查询<a href="#数据查询" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="6" src="https://cdnd.selectdb.com/zh-CN/assets/images/6-5ec3ef0ad093ee68dd7297622a064096.png" width="1080" height="596" class="img_ev3q"></p><p>用户通过数据工场将数据导入至 Doris 后即可进行查询。在小米内部，可以通过自研的数鲸平台进行查询的。用户可以通过数鲸平台对 Doris 进行可视化的查询，并展开用户行为分析和用户画像分析。其中，为帮助业务进行事件分析、留存分析、漏斗分析、路径分析等行为分析，我们为 Doris 添加了相应的 UDF （User Defined Function）和 UDAF (User Defined Aggregate Function)。
在即将发布的 1.2 版本中，Doris添加了外表元数据同步的功能，支持 Hive/Hudi/Iceberg 外表并增加了 Multi Catalog。查询外部表提升了性能，接入外表大幅增加了易用性。在未来，我们考虑直接通过 Doris 查询 Hive 与 Iceberg 数据，构建湖仓一体的架构。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="ab测试">A/B测试<a href="#ab测试" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>小米的 A/B 实验平台对 Apache Doris 查询性能的提升有着迫切的需求，因此我们选择优先在小米的 A/B 实验平台上线 Apache Doris 向量化版本，也就是 1.1.2 版本。</p><p>小米的 A/B 实验平台是一款通过 A/B 测试的方式，借助实验分组、流量拆分与科学评估等手段来辅助完成科学的业务决策，最终实现业务增长的一款运营工具产品。在实际业务中，为了验证一个新策略的效果，通常需要准备原策略 A 和新策略 B 两种方案。随后在总体用户中取出一小部分，将这部分用户完全随机地分在两个组中，使两组用户在统计角度无差别。将原策略 A 和新策略 B 分别展示给不同的用户组，一段时间后，结合统计方法分析数据，得到两种策略生效后指标的变化结果，并以此来判断新策略 B 是否符合预期。</p><p>小米的 A/B 实验平台有几类典型的查询应用：用户去重、指标求和、实验协方差计算等，查询类型会涉及较多的 Count(distinct)、Bitmap计算、Like语句等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="上线前验证">上线前验证<a href="#上线前验证" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们基于 Apache Doris 1.1.2 版本搭建了一个和小米线上 Apache Doris 0.13 版本在机器配置和机器规模上完全相同的测试集群，用于向量化版本上线前的验证。验证测试分为两个方面：单 SQL 串行查询测试和批量 SQL 并发查询测试。在这两种测试中，我们在保证两个集群数据完全相同的条件下，分别在 Doris 1.1.2 测试集群和小米线上 Doris 0.13 集群执行相同的查询 SQL 来做性能对比。我们的目标是，Doris 1.1.2 版本在小米线上 Doris 0.13 版本的基础上有 1 倍的查询性能提升。
两个集群配置完全相同，具体配置信息如下：</p><ul><li>集群规模：3 FE + 89 BE</li><li>BE节点CPU:  Intel(R) Xeon(R) Silver 4216 CPU @ 2.10GHz 16核 32线程 × 2</li><li>BE节点内存：256GB</li><li>BE节点磁盘：7.3TB × 12 HDD</li></ul><h4 class="anchor anchorWithStickyNavbar_LWe7" id="单-sql-串行查询测试">单 SQL 串行查询测试<a href="#单-sql-串行查询测试" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>在该测试场景中，我们选取了小米A/B 实验场景中 7 个典型的查询 Case，针对每一个查询 Case，我们将扫描的数据时间范围分别限制为 1 天、7 天和 20 天进行查询测试，其中单日分区数据量级大约为 31 亿（数据量大约 2 TB），测试结果如图所示：</p><p><img loading="lazy" alt="7" src="https://cdnd.selectdb.com/zh-CN/assets/images/7-8f6f2d02c9688f713ef48c8221c25158.png" width="750" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="8" src="https://cdnd.selectdb.com/zh-CN/assets/images/8-0ee361fa5acabc282382a20b61f5baaa.png" width="749" height="450" class="img_ev3q"></p><p><img loading="lazy" alt="9" src="https://cdnd.selectdb.com/zh-CN/assets/images/9-f28d9b3dc18ad2f8314faaf514c5dc69.png" width="750" height="450" class="img_ev3q"></p><p>根据以上小米 A/B 实验场景下的单 SQL 串行查询测试结果所示，Doris 1.1.2 版本相比小米线上 Doris 0.13 版本至少有 3~5 倍的性能提升，效果显著。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="调优测试结果">调优测试结果<a href="#调优测试结果" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们基于小米的 A/B实验场景对 Apache Doris 1.1.2 版本进行了一系列调优，并将调优后的 Doris 1.1.2 版本与小米线上 Doris 0.13 版本分别进行了并发查询测试。测试情况如下：</p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-1">测试 1<a href="#测试-1" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>我们选择了 A/B 实验场景中一批典型的用户去重、指标求和以及协方差计算的查询 Case（SQL 总数量为 3245）对两个版本进行并发查询测试，测试表的单日分区数据大约为 31 亿（数据量大约 2 TB），查询的数据范围会覆盖最近一周的分区。测试结果如图所示，Doris 1.1.2 版本相比 Doris0.13版本，总体的平均延迟降低了大约 48%，P95 延迟降低了大约 49%。在该测试中，Doris 1.1.2 版本相比 Doris0.13 版本的查询性能提升了接近 1 倍。</p><p><img loading="lazy" alt="10" src="https://cdnd.selectdb.com/zh-CN/assets/images/10-c9499045fecce0f0eae927ba3e0ac883.png" width="1080" height="338" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-2">测试 2<a href="#测试-2" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>我们选择了 A/B实验场景下的 7 份 A/B 实验报告对两个版本进行测试，每份 A/B 实验报告对应小米 A/B实验平台页面的两个模块，每个模块对应数百或数千条查询 SQL。每一份实验报告都以相同的并发向两个版本所在的集群提交查询任务。测试结果如图所示，Doris 1.1.2 版本相比 Doris 0.13 版本，总体的平均延迟降低了大约 52%。在该测试中，Doris 1.1.2 版本相比 Doris 0.13 版本的查询性能提升了超过 1 倍。</p><p><img loading="lazy" alt="11" src="https://cdnd.selectdb.com/zh-CN/assets/images/11-164d7e9bca3b81ccb6bae88a0048be41.png" width="750" height="450" class="img_ev3q"></p><h4 class="anchor anchorWithStickyNavbar_LWe7" id="测试-3">测试 3<a href="#测试-3" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h4><p>为了验证调优后的 Apache Doris 1.1.2 版本在小米 A/B 实验场景之外的性能表现，我们选取了小米用户行为分析场景进行了 Doris 1.1.2 版本和 Doris 0.13 版本的并发查询性能测试。我们选取了 2022年10月24日、25日、26日和 27日这 4 天的小米线上真实的行为分析查询 Case 进行对比查询，测试结果如图所示，Doris 1.1.2 版本相比 Doris 0.13 版本，总体的平均延迟降低了大约7 7%，P95 延迟降低了大约 83%。在该测试中，Doris 1.1.2 版本相比 Doris 0.13 版本的查询性能有 4~6 倍的提升。</p><p><img loading="lazy" alt="12" src="https://cdnd.selectdb.com/zh-CN/assets/images/12-125f05fc3d7544d5f15edf2ab41184e8.png" width="1080" height="338" class="img_ev3q"></p><h1>总结</h1><p>自从 Apache Doris 从 2019 年上线第一个业务至今，目前 Apache Doris 已经在小米内部服务了数十个业务及子品牌、集群数量达到数十个、节点规模达到数百台。每天完成数万次用户在线分析查询，承担了包括增长分析和报表查询等绝大多数在线分析的需求。</p><p>经过一个多月的性能调优和测试，Apache Doris 1.1.2 版本在查询性能和稳定性方面已经达到了小米 A/B实验平台的上线要求，在某些场景下的查询性能甚至超过了我们的预期，希望本次分享可以给有需要的朋友一些可借鉴的经验参考。</p><p>与此同时，在以上小米的实践中，已有部分功能在 Apache Doris 1.0 或 1.1 版本中发布，部分 PR 已经合入社区 Master，将在不久后发布的 1.2 新版本中与大家见面。随着社区的快速发展，有越来越多小伙伴参与到社区建设中，社区活跃度有了极大的提升。Apache Doris 已经变得越来越成熟，并开始从单一计算存储一体的分析型 MPP 数据库走向湖仓一体的道路，相信在未来，还会有更多的数据分析场景被探索和实现。</p><h1>联系我们</h1><p>官网：<a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Github：<a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>dev邮件组：<a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/JD_OLAP">Apache Doris 在京东搜索实时 OLAP 探索与实践</a></h2><div class="blog-info"><time datetime="2022-12-02T00:00:00.000Z" itemprop="datePublished">2022年12月2日</time><span class="split-line"></span><span class="authors"><span class="s-author">李哲</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>京东搜索实时 OLAP 探索与实践</h1><p><img loading="lazy" alt="kv" src="https://cdnd.selectdb.com/zh-CN/assets/images/kv-e94fd46c1522a3383d161daec2249d18.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>前言
本文讨论了京东搜索在实时流量数据分析方面，利用Apache Flink和Apache Doris进行的探索和实践。流式计算在近些年的热度与日俱增：从Google Dataflow论文的发表，到Apache Flink计算引擎逐渐站到舞台中央，再到Apache Druid等实时分析型数据库的广泛应用，流式计算引擎百花齐放。但不同的业务场景，面临着不同的问题，没有哪一种引擎是万能的。我们希望京东搜索业务在流计算的应用实践，能够给到大家一些启发，也欢迎大家多多交流，给我们提出宝贵的建议。</p></blockquote><blockquote><p>作者：李哲，京东搜推数据开发工程师，曾就职于美团点评，主要从事离线数据开发、流计算开发以及OLAP多维查询引擎的应用开发。</p></blockquote><h2 class="anchor anchorWithStickyNavbar_LWe7" id="京东与京东搜索">京东与京东搜索<a href="#京东与京东搜索" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>京东集团（NASDAQ：JD）中国领先的电商企业，2021年全年净收入达到9516亿元人民币。京东集团旗下设有京东零售、京东国际、京东科技、京东物流、京东云等。 京东集团于2014年5月在美国纳斯达克证券交易所正式挂牌上市。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="搜索业务对实时数据分析的需求">搜索业务对实时数据分析的需求<a href="#搜索业务对实时数据分析的需求" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>京东搜索作为电商平台的入口，为众多商家与用户提供连接的纽带。京东搜索发挥着导流的作用，给用户提供表达需求的入口；为了正确理解用户意图，将用户的需求进行高效的转化，线上同时运行着多个AB实验算法，遍及POP形态与自营形态的多个商品。而这些商品所属的品类、所在的组织架构以及品牌店铺等属性，都需要在线进行监控，以衡量转化的效果和承接的能力。目前搜索上层应用业务对实时数据的需求，主要包含三部分内容：
1、 搜索整体数据的实时分析。
2、 AB实验效果的实时监控。
3、 热搜词的Top榜单以反映舆情的变化。
这三部分数据需求，都需要进行深度的下钻，维度细化需要到SKU粒度。同时我们也承担着搜索实时数据平台的建设任务，为下游用户输出不同层次的实时流数据。
我们的用户包括搜索的运营、产品、算法以及采销人员。虽然不同用户关心的数据粒度不同、时间频率不同、维度也不同，但是我们希望能够建立统一的实时OLAP数据仓库，并提供一套安全、可靠的、灵活的实时数据服务。
目前每日新增的曝光日志达到几亿条记录，而拆分到SKU粒度的日志则要翻10倍，再细拆到AB实验的SKU粒度时，数据量则多达上百亿记录，多维数据组合下的聚合查询要求秒级响应时间，这样的数据量也给团队带来了不小的挑战。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时技术架构演进">实时技术架构演进<a href="#实时技术架构演进" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>我们之前的方案是以Apache Storm引擎进行点对点的数据处理，这种方式在业务需求快速增长的阶段，可以快速的满足实时报表的需求。但是随着业务的不断发展、数据量逐渐增加以及需求逐渐多样化，弊端随之产生。例如灵活性差、数据一致性无法满足、开发效率较低、资源成本增加等。</p><p><img loading="lazy" alt="page_2-zh" src="https://cdnd.selectdb.com/zh-CN/assets/images/page_2-zh-3261d16224daf63c18e91b2644108848.png" width="1684" height="801" class="img_ev3q"></p><p>为解决之前架构出现的问题，我们首先进行了架构升级，将storm引擎替换为Apache Flink，用以实现高吞吐、exactly once的处理语义。同时根据搜索数据的特点，将实时数据进行分层处理，构建出PV流明细层、SKU流明细层和AB实验流明细层，期望基于不同明细层的实时流，构建上层的实时OLAP层。
OLAP层的在技术选型时，需要满足以下几点：
1：数据延迟在分钟级，查询响应时间在秒级
2：标准SQL交互引擎，降低使用成本
3：支持join操作，方便以维度级别增加属性信息
4：流量数据可以近似去重，但订单行要精准去重
5：高吞吐，每分钟数据量在千万级记录，每天数百亿条新增记录
6：前端业务较多，查询并发度不能太低
通过对比目前业界广泛使用的支持实时导入的OLAP引擎，我们在druid、ES、clickhouse和doris之间做了横向比较：</p><p><img loading="lazy" alt="page_3-zh" src="https://cdnd.selectdb.com/zh-CN/assets/images/page_3-zh-bb25c0ea2faa03912dea231b8b207d3e.png" width="2315" height="758" class="img_ev3q"></p><p>通过对比开源的几款实时OLAP引擎，我们发现doris和clickhouse能够满足我们的需求，但是clickhouse的并发度太低是个潜在的风险，而且clickhouse的数据导入没有事务支持，无法实现exactly once语义，对标准sql的支持也是有限的。
最终，我们选定doris作为聚合层，用于实时OLAP分析。对于流量数据，使用聚合模型建表；对于订单行，我们使用Uniq模型，保证同一个订单最终只会存储一条记录，从而达到订单行精准去重的目的。在flink处理时，我们也将之前的任务拆解，将反复加工的逻辑封装，每一次处理都生成新的topic流，明细层细分了不同粒度的实时流。新方案如下：</p><p><img loading="lazy" alt="page_4-zh" src="https://cdnd.selectdb.com/zh-CN/assets/images/page_4-zh-87f0c1b0fea8992d98b23ad9a02b3d1e.png" width="3004" height="1571" class="img_ev3q"></p><p>目前的技术架构中，flink的任务是非常轻的。基于生产的数据明细层，我们直接使用了doris来充当聚合层的功能，将原本可以在flink中实现的窗口计算，下沉到doris中完成。利用doris的routine load消费实时数据，虽然数据在导入前是明细粒度，但是基于聚合模型，导入后自动进行异步聚合。而聚合度的高低，完全根据维度的个数与维度的基数决定。通过在base表上建立rollup，在导入时双写或多写并进行预聚合操作，这有点类似于物化视图的功能，可以将数据进行高度的汇总，以提升查询性能。
在明细层采用kafka直接对接到doris，还有一个好处就是这种方式天然的支持数据回溯。数据回溯简单说就是当遇到实时数据的乱序问题时，可以将“迟到”的数据进行重新计算，更新之前的结果。这是因为我们导入的是明细数据，延迟的数据无论何时到达都可以被写入到表中，而查询接口只需要再次进行查询即可获得最新的计算结果。最终方案的数据流图如下：</p><p><img loading="lazy" alt="page_5-zh" src="https://cdnd.selectdb.com/zh-CN/assets/images/page_5-zh-248c2ca88f12afd922abf431162b289c.png" width="1137" height="729" class="img_ev3q"></p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris在大促期间的优化">Doris在大促期间的优化<a href="#doris在大促期间的优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>上文提到我们在doris中建立了不同粒度的聚合模型，包括PV粒度、SKU粒度以及AB实验粒度。我们这里以每日生产数据量最大的曝光AB实验模型为例，阐述在doris中如何支持大促期间每日新增百亿条记录的查询的。
AB实验的效果监控，业务上需要10分钟、30分钟、60分钟以及全天累计等四个时间段，同时需要根据渠道、平台和一二三级品类等维度进行下钻分析，观测的指标则包含曝光PV、UV、曝光SKU件次、点击PV、点击UV等基础指标，以及CTR等衍生指标。
在数据建模阶段，我们将曝光实时数据建立聚合模型，其中K空间包含日期字段、分钟粒度的时间字段、渠道、平台、一二三级品类等，V空间则包含上述的指标列，其中UV和PV进行HLL近似计算，而SKU件次则采用SUM函数，每到来一条新记录则加1。由于AB实验数据都是以AB实验位作为过滤条件，因此将实验位字段设置为分桶字段，查询时能够快速定位tablet分片。值得注意的是，HLL的近似度在目前PV和UV的基数下，实际情况误差在0.8%左右，符合预期。
目前doris的集群共30+台BE，存储采用的是支持NVMe协议的SSD硬盘。AB实验曝光topic的分区数为40+，每日新增百亿条数据。在数据导入阶段，我们主要针对导入任务的三个参数进行优化：最大时间间隔、最大数据量以及最大记录数。当这3个指标中任何一个达到设置的阈值时，任务都会触发导入操作。为了更好的了解任务每次触发的条件，达到10分钟消费6亿条记录的压测目标，我们通过间隔采样的方法，每隔3分钟采样一次任务的情况，获取Statistic信息中的receivedBytes、cimmittedTaskNum、loadedRows以及taskExecuteTimeMs数值。通过对上述数值在前后2个时间段的差值计算，确定每个任务触发的条件，并调整参数，以在吞吐和延迟之间进行平衡，最终达到压测的要求。
为了实现快速的多维数据查询，基于base表建立了不同的rollup，同时每个rollup的字段顺序，也要遵循过滤的字段尽可能放到前面的原则，充分利用前缀索引的特性。这里并不是rollup越多越好，因为每个rollup都会有相应的物理存储，每增加一个rollup，在写入时就会增加一份IO。最终我们在此表上建立了2个rollup，在要求的响应时间内尽可能多的满足查询需求。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="总结与展望">总结与展望<a href="#总结与展望" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>京东搜索是在2020年5月份引入doris的，规模是30+台BE，线上同时运行着10+个routine load任务，每日新增数据条数在200亿+，已经成为京东体量最大的doris用户。从结果看，用doris替换flink的窗口计算，既可以提高开发效率，适应维度的变化，同时也可以降低计算资源，用doris充当实时数据仓库的聚合层，并提供统一的接口服务，保证了数据的一致性和安全性。
我们在使用中也遇到了查询相关的、任务调度相关的bug，也在推动京东OLAP平台升级到最新版本。接下来待版本升级后，我们计划使用bitmap功能来支持UV等指标的精准去重操作，并将推荐实时业务应用doris实现。除此之外，为了完善实时数仓的分层结构，为更多业务提供数据输入，我们也计划使用适当的flink窗口开发聚合层的实时流，增加数据的丰富度和完整度。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/Netease">Apache Doris 助力网易严选打造精细化运营 DMP 标签系统</a></h2><div class="blog-info"><time datetime="2022-11-30T00:00:00.000Z" itemprop="datePublished">2022年11月30日</time><span class="split-line"></span><span class="authors"><span class="s-author">刘晓东</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>应用实践｜Apache Doris 助力网易严选打造精细化运营 DMP 标签系统</h1><p><img loading="lazy" alt="1280X1280" src="https://cdnd.selectdb.com/zh-CN/assets/images/kv-a63c2e8908df91d10704f971aa636fa6.png" width="900" height="383" class="img_ev3q"></p><p><strong>导读</strong>:如果说互联网的上半场是粗狂运营，那么在下半场，精细化运营将是长久的主题，有数据分析能力才能让用户得到更好的体验。当下比较典型的分析方式是构建用户标签系统，本文将由网易严选分享 DMP 标签系统的建设以及 Apache Doris 在其中的应用实践。</p><p>作者<strong>｜</strong>刘晓东 网易严选资深开发工程师</p><p>如果说互联网的上半场是粗狂运营，因为有流量红利不需要考虑细节。那么在下半场，精细化运营将是长久的主题，有数据分析能力才能让用户得到更好的体验。当下比较典型的分析方式是构建用户标签系统，从而精准地生成用户画像，提升用户体验。今天分享的主题是网易严选 DMP 标签系统建设实践，<strong>主要围绕下面五点展开：</strong></p><ul><li>平台总览</li><li>标签生产 ：标签圈选&amp;生产链路</li><li>标签存储：存储方式&amp;存储架构演进</li><li>高性能查询</li><li>未来规划</li></ul><h1>平台总览</h1><p>DMP 作为网易严选的数据中台，向下连接数据，向上赋能业务，承担着非常重要的基石角色。</p><p><strong>DMP 的数据来源主要包括三大部分：</strong></p><ul><li>自营平台的 APP、小程序、PC 端等各端的业务日志</li><li>网易集团内部共建的一些基础数据</li><li>京东、淘宝、抖音等第三方渠道店铺的数据</li></ul><p>通过收集、清洗，将以上数据形成数据资产沉淀下来。DMP 在数据资产基础上形成了一套自己的标签产出、人群圈选和用户画像分析体系，从而为业务提供支撑，包括：智能化的选品、精准触达以及用户洞察等。总的来说，<strong>DMP 系统就是构建以数据为核心的标签体系和画像体系，从而辅助业务做一系列精细化的运营。</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/c4/c4213e903762cfadc42e038e5de71d1a.png" alt="img" class="img_ev3q"></p><p>了解 DMP 系统，先从以下几个概念开始。</p><ul><li><strong>标签</strong>: 对于实体（用户、设备、手机号等）特征的描述，是一种面向业务的数据组织形式，比如使用：年龄段、地址、偏好类目等对用户实体进行刻画。</li><li><strong>人群圈选</strong>: 通过条件组合从全体用户中圈选出一部分用户，具体就是指定一组用户标签和其对应的标签值，得到符合条件的用户人群。</li><li><strong>画像分析</strong>: 对于人群圈选结果，查看该人群的行为情况、标签分布。例如查看【城市为杭州，且性别为女性】的用户在严选 APP 上的行为路径、消费模型等。</li></ul><p><img loading="lazy" src="https://static001.geekbang.org/infoq/77/7727372da8b4d2e591a2563bd061425d.png" alt="img" class="img_ev3q"></p><p>严选标签系统对外主要提供两大核心能力：</p><ol><li><p>标签查询：查询特定实体指定标签的能力，常用于基本信息的展示。</p></li><li><p>人群圈选：分为实时和离线圈选。<strong>圈选结果主要用于：</strong></p></li></ol><ul><li>分组判断：判读用户是否在指定的一个或多个分组，资源投放、触点营销等场景使用较多。</li><li>结果集拉取：拉取指定的人群数据到业务方系统中，进行定制化开发。</li><li>画像分析：分析特定人群的行为数据，消费模型等，进行更精细的运营。</li></ul><p><strong>整体的业务流程如下：</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/fe/fe5591eaeb384e52cad6c3940ffa8ff2.png" alt="img" class="img_ev3q"></p><ul><li>首先定义标签和人群圈选的规则；</li><li>定义出描述业务的 DSL 之后，便可以将任务提交到 Spark 进行计算；</li><li>计算完成之后，<strong>将计算结果存储到 Hive 和 Doris</strong>；</li><li>之后业务方便可以根据实际业务需求<strong>从 Hive 或</strong> <strong>Doris</strong> <strong>中查询使用数据</strong>。</li></ul><p><img loading="lazy" src="https://static001.geekbang.org/infoq/1a/1ad0a3fa4efc6cc4dee9603a7c0f73aa.png" alt="img" class="img_ev3q"></p><p><strong>DMP 平台整体分为计算存储层、调度层、服务层、和元数据管理四大模块。</strong></p><p>所有的标签元信息存储在源数据表中；调度层对业务的整个流程进行任务调度：数据处理、聚合转化为基础标签，基础标签和源表中的数据通过 DSL 规则转化为可用于数据查询的 SQL 语义，由调度层将任务调度到计算存储层的 Spark 进行计算，<strong>并将计算结果存储到 Hive 和 Doris 中。</strong>服务层由标签服务、实体分组服务、基础标签数据服务、画像分析服务四部分组成。</p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/f8/f8bfd13ee1135cfad025022a2e48ae2a.png" alt="img" class="img_ev3q"></p><p><strong>标签的生命周期包含5个阶段：</strong></p><ul><li><strong>标签需求</strong>: 在此阶段，运营提出标签的需求和价值预期，产品评估需求合理性以及紧迫性。</li><li><strong>排期生产</strong>: 此阶段需要数据开发梳理数据，从 ods 到 dwd 到 dm 层整个链路，根据数据建立模型，同时数据开发需要做好质量监控。 </li><li><strong>人群圈选</strong>: 标签生产出来之后进行应用，圈选出标签对应的人群。</li><li><strong>精准营销</strong>: 对圈选出来的人群进行精准化营销。</li><li><strong>效果评估</strong>: 最后产品、数据开发和运营对标签使用率、使用效果进行效果评估来决定后续对标签进行改进或降级。</li></ul><p>总的来说，就是以业务增长为目标，围绕标签的生命周期，投入合理的资源，最大化运营效果。</p><h1>标签生产</h1><p><strong>接下来介绍标签生产的整个过程。</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/c5/c50e3d5747bf300b9ffb708b0a7c8cb8.png" alt="img" class="img_ev3q"></p><p><strong>标签的数据分层：</strong></p><ul><li>最下层是 ods 层，包括用户登录日志、埋点记录日志、交易数据以及各种数据库的 Binlog 数据。</li><li>对 ods 层处理后的数据到达 dwd 明细层，包括用户登录表、用户活动表、订单信息表等。</li><li>dwd 层数据聚合后到 dm 层，标签全部基于 dm 层数据实现。</li></ul><p>目前我们从原始数据库到 ods 层数据产出已经完全自动化，从 ods 层到 dwd 层实现了部分自动化，从 dwd 到 dm 层有一部分自动化操作，但自动化程度还不高，这部分的自动化操作是我们接下来的工作重点。</p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/00/007eea83ec5ab995bf6ed52b9720508f.png" alt="img" class="img_ev3q"></p><p><strong>标签根据时效性分为</strong>：离线标签、近实时标签和实时标签。</p><p><strong>根据聚合粒度分为</strong>：聚合标签和明细标签。</p><p>通过类别维度可将标签分为：账号属性标签、消费行为标签、活跃行为标签、用户偏好标签、资产信息标签等。</p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/45/450809e28afcf4bf76f3ad6f0eb89415.png" alt="img" class="img_ev3q"></p><p><strong>直接将 dm 层的数据不太方便拿来用，原因在于：</strong></p><p>基础数据比较原始，抽象层次有所欠缺、使用相对繁琐。通过对基础数据进行与、或、非的组合，形成业务标签供业务方使用，可以降低运营的理解成本，降低使用难度。</p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/45/4551dbb64f0412ab305028ef42ebb730.png" alt="img" class="img_ev3q"></p><p>标签组合之后需要对标签进行具体业务场景应用，如人群圈选。配置如上图左侧所示，支持离线人群包和实时行为（需要分开配置）。</p><p>配置完后，生成上图右侧所示的 DSL 规则，以 Json 格式表达，对前端比较友好，也可以转成存储引擎的查询语句。</p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/68/686c29c2c6929e626d0707efe8e40d1a.png" alt="img" class="img_ev3q"></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/c5/c58e6ea678716a50d60df23144247833.png" alt="img" class="img_ev3q"></p><p>标签有一部分实现了自动化。在人群圈选部分自动化程度比较高。比如分组刷新，每天定时刷新；高级计算，如分组与分组间的交/并/差集；数据清理，及时清理过期失效的实体集。</p><h1>标签存储</h1><p><strong>下面介绍一下我们在标签存储方面的实践。</strong></p><p>严选 DMP 标签系统需要承载比较大的 C端流量，对实时性要求也比较高。</p><p>我们对存储的要求包括：</p><ul><li>支持高性能查询，以应对大规模 C端流量</li><li>支持 SQL，便于应对数据分析场景</li><li>支持数据更新机制</li><li>可存储大数据量</li><li>支持扩展函数，以便处理自定义数据结构</li><li>和大数据生态结合紧密</li></ul><p>目前还没有一款存储能够完全满足要求。</p><p><strong>我们第一版的存储架构如下图所示：</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/8c/8cc165d0cc9a119540f72b642f63172e.png" alt="img" class="img_ev3q"></p><p>离线数据大部分存储在 Hive 中，小部分存储在 Hbase（主要用于基础标签的查询）。实时数据一部分存储在 Hbase 中用于基础标签的查询，部分双写到 KUDU 和 ES 中，用于实时分组圈选和数据查询。离线圈选的数据通过 impala 计算出来缓存在 Redis 中。</p><p><strong>这一版本的缺点包括：</strong></p><ul><li>存储引擎过多。</li><li>双写有数据质量隐患，可能一方成功一方失败，导致数据不一致。</li><li>项目复杂，可维护性较差。</li></ul><p>为了减少引擎和存储的使用量，提高项目可维护性，在版本一的基础上改进实现了版本二。</p><p><strong>我们第二版的存储架构如下图所示：</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/dd/dd013f1d6d64cb258edfcd1fd9e826c5.png" alt="img" class="img_ev3q"></p><p><strong>存储架构版本二引入了 Apache Doris</strong>，离线数据主要存储在 Hive 中，同时将基础标签导入到 Doris，实时数据也存储在 Doris，基于 Spark 做 Hive 加 Doris 的联合查询，并将计算出来的结果存储在 Redis 中。经过此版改进后，实时离线引擎存储得到了统一，性能损失在可容忍范围内（Hbase 的查询性能比 Doris 好一些，能控制在 10ms 以内，Doris 目前是 1.0 版本，p99，查询性能能控制在 20ms 以内，p999，能控制在 50ms 以内）；<strong>项目简化，降低了运维成本。</strong></p><p><strong>在大数据领域，各种存储计算引擎有各自的适用场景，如下表所示：</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/71/71be41d6c04b28ae16b11db0796719f0.png" alt="img" class="img_ev3q"></p><h1>高性能查询</h1><p><img loading="lazy" src="https://static001.geekbang.org/infoq/23/23f6cb52a12b10335f07a6e57e4b207c.png" alt="img" class="img_ev3q"></p><p>分组存在性判断：判断用户是否在指定的一个分组或者多个分组。包括两大部分：</p><ul><li>第一部分为静态人群包，提前进行预计算，存入 Redis 中（Key 为实体的 ID，Value 为结果集 ID），采用 Lua 脚本进行批量判断，提升性能；</li><li>第二部分为实时行为人群，需要从上下文、API 和 Apache Doris 中提取数据进行规则判断。性能提升方案包括，异步化查询、快速短路、查询语句优化、控制 Join表数量等。</li></ul><p><img loading="lazy" src="https://static001.geekbang.org/infoq/c8/c8e6312891a1e4a3255debbebdd53f5a.png" alt="img" class="img_ev3q"></p><p>还有一个场景是人群分析：人群分析需要将人群包数据同多个表进行联合查询，分析行为路径。目前 Doris 还不支持路径分析函数，因此我们开发了 DorisUDF 来支持此业务。<strong>Doris 的计算模型对自定义函数的开发还是很友好的，能够比较好地满足我们的性能需要。</strong></p><p><img loading="lazy" src="https://static001.geekbang.org/infoq/12/123ca7535e6fb03b1337060f20d2fe6d.png" alt="img" class="img_ev3q"></p><p><strong>Apache Doris 在网易严选中已应用于点查、批量查询、路径分析、人群圈选等场景。在实践中具备以下优势：</strong></p><ul><li>在点查和少量表的联合查询性能 QPS 超过万级，RT99&lt;50MS。</li><li>水平扩展能力很强，运维成本相对比较低。</li><li>离线数据和实时数据相统一，降低标签模型复杂度。</li></ul><p>不足之处在于大量小数据量的导入任务资源占用较多，待 Doris 1.1.2 版本正式发布后我们也会及时同步升级。不过此问题已经在 Doris 1.1 版本中进行了优化，<strong>Doris 在 1.1 中大幅增强了数据 Compaction 能力，对于新增数据能够快速完成聚合，避免分片数据中的版本过多导致的 -235 错误以及带来的查询效率问题。</strong></p><p><strong>具体可以参考：</strong><a href="http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247500848&amp;idx=1&amp;sn=a667665ed4ccf4cf807a47be7c264f69&amp;chksm=cf2fca37f85843219e2f74d856478d4aa24d381c1d6e7f9f6a64b65f3344ce8451ad91c5af97&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">Apache Doris 1.1 特性揭秘：Flink 实时写入如何兼顾高吞吐和低延时</a></p><h1>未来规划</h1><p><img loading="lazy" src="https://static001.geekbang.org/infoq/1c/1c9088b38860dddf5f794a71a2b62023.png" alt="img" class="img_ev3q"></p><p><strong>提升存储&amp;计算性能</strong>: Hive 和 Spark 逐渐全部转向 Apache Doris。</p><p><strong>优化标签体系：</strong></p><ul><li>建立丰富准确的标签评价体系</li><li>提升标签质量和产出速度</li><li>提升标签覆盖率</li></ul><p><strong>更精准的运营</strong></p><ul><li>建立丰富的用户分析模型</li><li>从使用频次和用户价值两个方面提升用户洞察模型评价体系</li><li>建立通用化画像分析能力，辅助运营智能化决策</li></ul><h1>资料下载</h1><p>关注公众号「<strong>SelectDB</strong>」，后台回复【<strong>网易严选</strong>】获取本次演讲 <strong>PPT 资料</strong>！</p><p><strong>相关链接：</strong></p><p>Apache Doris 官方网站：</p><p><a href="http://doris.apache.org" target="_blank" rel="noopener noreferrer">http://doris.apache.org</a></p><p>Apache Doris Github：</p><p><a href="https://github.com/apache/doris" target="_blank" rel="noopener noreferrer">https://github.com/apache/doris</a></p><p>Apache Doris 开发者邮件组：</p><p><a href="mailto:dev@doris.apache.org" target="_blank" rel="noopener noreferrer">dev@doris.apache.org</a></p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/NIO">Apache Doris 在蔚来汽车的应用</a></h2><div class="blog-info"><time datetime="2022-11-28T00:00:00.000Z" itemprop="datePublished">2022年11月28日</time><span class="split-line"></span><span class="authors"><span class="s-author">唐怀东</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>Apache Doris 在蔚来汽车的应用</h1><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/zh-CN/assets/images/NIO_kv-7601d71a49c7ecd7fb42f03de600ae6c.png" width="900" height="383" class="img_ev3q"></p><blockquote><p>导读：本次分享的题目是Apache Doris在蔚来汽车的应用，主要包括以下几大部分：</p><ol><li>蔚来</li><li>OLAP在蔚来的发展</li><li>Doris作为统一OLAP数仓</li><li>Doris在运营平台上的实践</li><li>经验总结</li></ol></blockquote><p>作者：唐怀东，蔚来汽车 数据团队负责人</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="蔚来">蔚来<a href="#蔚来" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>蔚来（纽约证券交易所代码：NIO）是设计高端智能电动汽车市场的领先公司。 NIO 成立于 2014 年 11 月，设计、开发、联合制造和销售高端智能电动汽车，并不断推动自动驾驶、数字技术、电动动力总成和电池领域的创新。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="olap在蔚来的发展">OLAP在蔚来的发展<a href="#olap在蔚来的发展" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>首先，让我们来一起回顾OLAP在蔚来汽车的发展。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-2017年引入apache-druid">1. 2017年引入Apache Druid<a href="#1-2017年引入apache-druid" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在当时可选择的OLAP存储和查询引擎并不多，比较常见的有Apache Druid、Apache Kylin。我们优先引入Druid的原因是以前有使用经验，而Kylin预计算虽然具有极高的查询效率优势，但是：</p><ul><li><p>Kylin底层最合适和最优的存储是HBase，之前公司并未引入，会额外增加运维的工作。</p></li><li><p>Kylin对各种维度和指标进行预计算，如果维度和维度取值非常多，会有维度爆炸的问题，对存储造成非常大的压力。</p></li></ul><p>Druid的优势很明显，支持实时和离线数据接入，列式存储，高并发，查询效率非常高。其缺点也比较明显：</p><ul><li>未使用标准协议例如JDBC，使用门槛高</li><li>Join的支持较弱</li><li>精确去重的效率低，性能会随之下降。整体性能要分场景去考虑，这也是我们后期去选型其他OLAP的原因</li><li>运维成本高，不同的组件有不同的安装方式和不同的依赖；数据导入还要考虑和Hadoop集成以及JAR包的依赖</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-2019年引入tidb">2. 2019年引入TiDB<a href="#2-2019年引入tidb" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><strong>TiDB是一个OLTP+OLAP的成熟引擎，同样是优点、缺点分明：</strong></p><p>优势：</p><ul><li>OLTP数据库，更新友好。</li><li>支持明细和聚合，有指标计算和数据看板展示，还支持明细数据查询</li><li>支持标准SQL，使用成本低</li><li>运维成本低</li></ul><p>劣势：</p><ul><li>它不是一个独立的OLAP。TiFlash依赖于OLTP，会增加存储。其OLAP能力稍显不足</li><li>整体性能要分场景去衡量</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-2021年引入doris">3. 2021年引入Doris<a href="#3-2021年引入doris" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>自2021年起，我们正式引入了Apache Doris。在系统选型过程中，产品的性能、SQL语法、系统兼容性、学习以及运维成本等多方面因素是我们最为关心的部分。经过深入调研、层层对比以下几个系统后，我们得出了如下结论：</p><p><strong>我们重点关注的Doris，其优点完全满足我们的诉求：</strong></p><ul><li>支持高并发查询（我们最关心的一点）</li><li>同时支持实时和离线数据</li><li>支持明细和聚合</li><li>Uniq模型支持更新</li><li>物化视图的能力能极大的加速查询效率</li><li>兼容MySQL协议，所以开发和使用成本比较低</li><li>性能完全满足我们的要求</li><li>运维成本比较低</li></ul><p><strong>Clickhouse，我们之前也调研过，也尝试想去使用它。其单机性能极强，但是缺点明显:</strong></p><ul><li>我们明确需要的场景下，它的多表join支持的稍微差一些</li><li>并发度比较低</li><li>运维成本极高</li></ul><p>凭借多种性能优势，Apache Doris比较理想地替代了Druid和TiDB。而Clickhouse在我们的业务上并不能很好的适配，让我们最终走向了Apache Doris。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris作为统一olap数仓">Doris作为统一OLAP数仓<a href="#doris作为统一olap数仓" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/zh-CN/assets/images/olap-96ad3bb86cebd92a200a0581f0418d3c.png" width="1018" height="669" class="img_ev3q"></p><p>这张图基本上就是从数据源到数据接入、数据计算、数据仓库、数据服务以及应用。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-数据源">1. 数据源<a href="#1-数据源" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>蔚来的场景下，数据源不仅仅指业务系统的数据，还有埋点数据、设备数据、车辆数据等等。数据会通过一种接入方式接入到大数据平台。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-数据接入">2. 数据接入<a href="#2-数据接入" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>对于一些业务系统的数据，可以开启CDC捕捉变化的数据，然后转换成一个数据流存储到Kafka，接续再进行流式的计算。某些只能通过批量的方式的数据会直接进入到我们的分布式存储。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3-数据计算">3. 数据计算<a href="#3-数据计算" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们没有采用流批一体，采用的是Lambda架构。
我们本身的业务决定了我们的Lambda架构是离线和实时分成了两条路径：</p><ul><li>部分数据是流式的。</li><li>部分数据能够存储到数据流里，一些历史数据不会存储到Kafka。</li><li>有些场景数据要求高精准度。为了保证数据的准确性，一个离线的pipeline将会把整个数据重新计算和刷新。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-数据仓库">4. 数据仓库<a href="#4-数据仓库" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>数据计算到数仓，这两条线路我们没有采用Flink或Spark Doris Connector。我们用Routine Load来连接Apache Doris和Flink，用Broker Load连接Doris和Spark。 由Spark批量生成的数据，会备份到Hive供其他场景使用。这样每计算一次，就同时供多个场景去使用，大大提升了效率。Flink的情况也诸如此类。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="5-数据服务">5. 数据服务<a href="#5-数据服务" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Doris后面是One Service。通过注册数据源或灵活配置的方式，自动生成API，对API进行流量的控制和权限的控制，灵活性大大提高。并借助于k8s serverless方案，整个服务非常灵活和丰富。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="6-数据应用">6. 数据应用<a href="#6-数据应用" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>应用层中我们主要是部署一些报表应用和其他的一些服务。</p><p>我们主要有两类使用场景：</p><ul><li>面向用户，类似于互联网，我们有很多用户的场景，包括看板和指标</li><li>面向车，车的数据通过这种方式进入到Doris，通过一定的聚合之后，Doris数据体量在几十亿级别。但总体性能仍然可以满足我们的要求。</li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="doris在运营平台上的实践">Doris在运营平台上的实践<a href="#doris在运营平台上的实践" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-cdp-architecture">1. CDP Architecture<a href="#1-cdp-architecture" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" alt="NIO" src="https://cdnd.selectdb.com/zh-CN/assets/images/cdp-3d65926e741a2837759b07514e914bbf.png" width="1471" height="422" class="img_ev3q"></p><p>接下来我来介绍Doris在运营平台上的实践。这是我们的真实使用场景。如今互联网公司普遍会做自己的CDP，它一般包括几个模块：</p><ul><li>标签，是最基础的部分。</li><li>圈人，基于标签，按照一定逻辑将人圈选出来。</li><li>洞察，针对圈定的人群，了解人群分布、特点。</li><li>触达，利用例如短信、电话、声音、APP通知、IM等方式触达到用户，并配合流量控制。</li><li>效果分析，提升运营平台的完整性，有动作、有效果、有反馈。</li></ul><p>Doris在这里面起到了最重要的作用，包括：标签存储、人群存储、效果分析。
标签分为基础标签和用户行为的基础数据，在此基础之上，我们可以灵活自定义其他标签。从实效性来看，标签还分为实时的标签和离线的标签。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-cdp存储选型的考量点">2. CDP存储选型的考量点<a href="#2-cdp存储选型的考量点" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>我们从5个维度去考量CDP存储的选型。</p><p>**(1) 离线和实时统一
如前所述标签有离线标签，有实时标签。目前我们是准实时的场景。对于有些数据，准实时已足够满足我们的需求，大量的标签还是离线的标签，采用的方式就是Doris的Routine Load和Broker Load。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>实时标签</td><td>数据实时更新</td><td>Routine Load</td></tr><tr><td>离线标签</td><td>高效大批量导入</td><td>Broker Load</td></tr><tr><td>流批统一</td><td>实时历险数据存储统一</td><td>Routine Load 和 Broker Load 更新同一张表的不同列</td></tr></tbody></table><p>另外同一张表上，不同列更新的频率也是不一样的。例如用户的基础标签，我们对用户的身份需要实时的更新，因为用户的身份是时刻变化的。T+1的更新不能满足我们的需求。有些标签离线，例如用户的性别、年龄等基础标签，T+1更新足以满足我们的标准。基础用户的原子标签放在一张表中带来的维护成本很低。当后期自定义标签时，表的数量会大大减少，这样对于整体性能的提升有极大好处。</p><p><strong>(2) 高效圈选</strong></p><p>用户运营有了标签，第二步就是圈人，圈选就是根据标签的不同组合，把符合标签条件的所有人筛选出来，这时会有不同标签条件组合的查询、这个查询在Doris引入向量化之后有比较明显的提升。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>复杂条件圈选</td><td>高效的支持多条件圈选</td><td>SIMD的优化</td></tr></tbody></table><p><strong>(3) 高效聚合</strong></p><p>前面提到的用户洞察或群体洞察以及效果分析统计，需要对数据做统计分析，并不是单一的按用户ID获取标签的这种简单场景。其读取的数据量和查询效率，对我们这个标签的分布、群体的分布、效果分析的统计都有很大的影响。在这里，体现到的Doris的功能特点是：</p><ul><li>第一是数据分片，我们按时间把数据分片，分析统计就会极大的减少数据量，可以极大的加速查询和分析的效率。</li><li>第二是节点聚合，然后再收集做统一的聚合。</li><li>第三是向量化加速，向量化引擎对性能提升非常显著。</li></ul><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>标签值的分布</td><td>每天都需要更新所有标签，需要快速高效统计</td><td>数据分片，减少数据传输和计算</td></tr><tr><td>群体的分布</td><td>同上</td><td>存算统一，每个节点先聚合</td></tr><tr><td>效果分析的统计值</td><td>同上</td><td>SIMD提速</td></tr></tbody></table><p><strong>(4) 多表关联</strong></p><p>我们的CDP可能和业内常见的CDP场景不太一样，因为有些场景的CDP标签是提前预估完成的，不存在自定义标签。只做原子标签，或者说用户基础行为数据的统计，这样可以把灵活性留给使用CDP的用户，根据自己的业务场景去自定义标签。底层的数据是分散在不同的数据库表里，如果做自定义的标签的建设，势必需要做表的关联。
我们选择Doris一个非常重要的原因，就是多表关联的能力。通过性能测试，Doris目前能够满足我们的要求。而且Doris为用户提供了非常强大的能力。因为标签是动态的。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>群体的特征分布</td><td>统计群体在某个特征下的分布</td><td>多表关联</td></tr><tr><td>Single Tag</td><td>Display tags</td><td></td></tr></tbody></table><p><strong>(5) 联邦查询</strong></p><p>用户触达成功与否我们会记录到TiDB。用户运营中的通知，可能只影响用户体验，如果涉及到钱例如发放积分或优惠券，任务执行就要做到不重不漏，这种OLTP场景用TiDB比较合适。
做效果分析，需要了解运营计划执行到什么程度，是否达成目标，其分布情况等等。需要把任务执行情况和人群圈选相结合才能进行分析，就会用到Doris和TiDB的关联，外表关联进行查询。
我们设想标签体量比较小，保存到es可能比较合适，然而ES不能满足我们的需求，后面会解释其原因。</p><table><thead><tr><th><strong>场景</strong></th><th><strong>需求</strong></th><th><strong>Apache Doris功能点</strong></th></tr></thead><tbody><tr><td>效果分析关联任务执行明细</td><td>Doris数据关联TiDB数据</td><td>关联外表进行查询</td></tr><tr><td>人群标签关联行为聚合数据</td><td>Doris数据关联Elasticsearch数据</td><td></td></tr></tbody></table><h2 class="anchor anchorWithStickyNavbar_LWe7" id="经验和总结">经验和总结<a href="#经验和总结" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><ol><li><p><strong>bitmap</strong>. 我们的体量无法充分发挥其效率。如果体量达到一定程度，用bitmap会有很好的性能提升。例如计算UV场景，Id全集大于5000万，可以考虑bitmap聚合。</p></li><li><p><strong>ES外表。单表查询下效率比较理想。</strong> </p></li><li><p><strong>分批更新列</strong>. 为了减少表的数量和提升join表的性能，设计表尽量精简尽量聚合，相同类型的事实都放在一起。但相同类型的字段可能更新频率不同，有些字段需要天级更新，有些字段可能需要小时级更新，单独更新某一列就是一个明显的诉求。Doris聚合模型单独更新某些列的解决方案是使用REPLACE_IF_NOT_NULL。注意:用null替换原来的非null值是做不到的,可以把所有的null替换成有意义的默认值，例如unknown。</p></li><li><p><strong>在线服务</strong>. Doris同一份数据同时服务在线离线场景，对资源隔离的要求比较高，目前还存在进一步优化的空间。</p></li></ol></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/xiaomi">Apache Doris 在小米数据场景的应用实践与优化</a></h2><div class="blog-info"><time datetime="2022-08-15T00:00:00.000Z" itemprop="datePublished">2022年8月15日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1>背景</h1><p>因增长分析业务需要，小米集团于 2019 年首次引入了 Apache Doris 。经过三年时间的发展，目前 Apache Doris 已经在广告投放、新零售、增长分析、数据看板、天星数科、小米有品、用户画像等小米内部数十个业务中得到广泛应用 <strong>，并且在小米内部已经形成一套以 Apache Doris 为核心的数据生态。</strong>
<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/25d7c2c45acd4e1c8c1a1742016fc6b9~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">
当前 Apache Doris 在小米内部已经具有<strong>数十个</strong>集群、总体达到<strong>数百台</strong> BE 节点的规模，其中单集群最大规模达到<strong>近百台节点</strong>，拥有<strong>数十个</strong>流式数据导入产品线，每日单表最大增量 <strong>120 亿</strong>、支持 <strong>PB 级别</strong>存储，单集群每天可以支持 <strong>2W 次以上</strong>的多维分析查询。</p><h1>架构演进</h1><p>小米引入 Apache Doris 的初衷是为了解决内部进行用户行为分析时所遇到的问题。随着小米互联网业务的发展，各个产品线利用用户行为数据对业务进行增长分析的需求越来越迫切。让每个业务产品线都自己搭建一套增长分析系统，不仅成本高昂，也会导致效率低下。因此能有一款产品能够帮助他们屏蔽底层复杂的技术细节，让相关业务人员能够专注于自己的技术领域，可以极大提高工作效率。基于此，小米大数据和云平台联合开发了增长分析系统 Growing Analytics（下文中简称 GA )，旨在提供一个灵活的多维实时查询和分析平台，统一数据接入和查询方案，帮助业务线做精细化运营。（此处内容引用自：<a href="https://mp.weixin.qq.com/s?__biz=MzUxMDQxMDMyNg==&amp;mid=2247486817&amp;idx=1&amp;sn=99fbef15b4d6f6059c3affbc77517e6e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">基于Apache Doris的小米增长分析平台实践</a>）</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/897a0453e1a540ae88cdf05ee9188b56~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>分析、决策、执行是一个循环迭代的过程，在对用户进行行为分析后，针对营销策略是否还有提升空间、是否需要在前端对用户进行个性化推送等问题进行决策，帮助小米实现业务的持续增长。这个过程是对用户行为进行<strong>分析-决策-优化执行-再分析-再决策-再优化执行</strong>的迭代过程。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="历史架构">历史架构<a href="#历史架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>增长分析平台立项于 2018 年年中，当时基于开发时间和成本，技术栈等因素的考虑，小米复用了现有各种大数据基础组件（HDFS, Kudu, SparkSQL 等），搭建了一套基于 Lamda 架构的增长分析查询系统。<strong>GA 系统初代版本的架构如下图所示，包含了以下几个方面：</strong></p><ul><li>数据源：数据源是前端的埋点数据以及可能获取到的用户行为数据。</li><li>数据接入层：对埋点数据进行统一的清洗后打到小米内部自研的消息队列 Talos 中，并通过 Spark Streaming 将数据导入存储层 Kudu 中。</li><li>存储层：在存储层中进行冷热数据分离。热数据存放在 Kudu 中，冷数据则会存放在 HDFS 上。同时在存储层中进行分区，当分区单位为天时，每晚会将一部分数据转冷并存储到 HDFS 上。</li><li>计算层/查询层：在查询层中，使用 SparkSQL 对 Kudu 与 HDFS 上数据进行联合视图查询，最终把查询结果在前端页面上进行显示。</li></ul><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/9039c4f9ef8a4a3cbfd092b21233e831~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>在当时的历史背景下，初代版本的增长分析平台帮助我们解决了一系列用户运营过程中的问题，但同时在历史架构中也存在了两个问题：</strong></p><p><strong>第一个问题：</strong> 由于历史架构是基于 SparkSQL + Kudu + HDFS 的组合，依赖的组件过多导致运维成本较高。原本的设计是各个组件都使用公共集群的资源，但是实践过程中发现执行查询作业的过程中，查询性能容易受到公共集群其他作业的影响，容易抖动，尤其在读取 HDFS 公共集群的数据时，有时较为缓慢。</p><p><strong>第二个问题：</strong> 通过 SparkSQL 进行查询时，延迟相对较高。SparkSQL 是基于批处理系统设计的查询引擎，在每个 Stage 之间交换数据 Shuffle 的过程中依然需要落盘操作，完成 SQL 查询的时延较高。为了保证 SQL 查询不受资源的影响，我们通过添加机器来保证查询性能，但是实践过程中发现，性能提升的空间有限，这套解决方案并不能充分地利用机器资源来达到高效查询的目的，存在一定的资源浪费。 <strong>（此处内容引用自：<a href="https://mp.weixin.qq.com/s?__biz=MzUxMDQxMDMyNg==&amp;mid=2247486817&amp;idx=1&amp;sn=99fbef15b4d6f6059c3affbc77517e6e&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">基于Apache Doris的小米增长分析平台实践</a>）</strong></p><p>针对上述两个问题，我们的目标是寻求一款计算存储一体的 MPP 数据库来替代我们目前的存储计算层的组件，<strong>在通过技术选型后，最终我们决定使用 Apache Doris 替换老一代历史架构。</strong></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="基于-apache-doris-的新版架构">基于 Apache Doris 的新版架构<a href="#基于-apache-doris-的新版架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>当前架构从数据源获取前端埋点数据后，通过数据接入层打入 Apache Doris 后可以直接查询结果并在前端进行显示。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/540f5fa779af4b629869e54b793ea273~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>选择 Doris 原因：</strong></p><ul><li><p>Doris 具有优秀的查询性能，能够满足业务需求。</p></li><li><p>Doris 支持标准 SQL ，用户使用与学习成本较低。</p></li><li><p>Doris 不依赖于其他的外部系统，运维简单。</p></li><li><p>Doris 社区拥有很高活跃度，有利于后续系统的维护升级。</p></li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新旧架构性能对比">新旧架构性能对比<a href="#新旧架构性能对比" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ada8246b409a4cb6b11ffd2454aa2b06~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>我们选取了日均数据量大约 10 亿的业务，分别在不同场景下进行了性能测试，其中包含 6 个事件分析场景，3 个留存分析场景以及 3 个漏斗分析场景。<strong>经过对比后，得出以下结论：</strong></p><ul><li>在事件分析的场景下，平均查询所耗时间<strong>降低了 85%</strong> 。</li><li>在留存分析和漏斗分析场景下，平均查询所耗时间<strong>降低了 50%</strong> <strong>。</strong></li></ul><h1>应用实践</h1><p>随着接入业务的增多和数据规模的增长，让我们也遇到不少问题和挑战，下面我们将介绍在<strong>使用 Apache Doris 过程中沉淀出来的一些实践经验</strong>。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据导入">数据导入<a href="#数据导入" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8afce198933f4ca4b2c97d4cf85b27de~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">小米内部主要通过 Stream Load 与 Broker Load 以及少量 Insert 方式来进行 Doris 的数据导入。数据一般会先打到 Talos 消息队列中，并分为实时数据和离线数据两个部分。</p><p><strong>实时数据写入 Apache Doris 中：</strong></p><p> 一部分业务在通过 Flink 对数据进行处理后，会通过 Doris 社区提供的 Flink Doris Connector 组件写入到 Doris 中，底层依赖于 Doris Stream Load 数据导入方式。也有一部分会通过 Spark Streaming 封装的 Stream Load 将数据导入到 Doris 中。</p><p><strong>离线数据写入</strong> <strong>Apache Doris 中：</strong></p><p>离线数据部分则会先写到 Hive 中，再通过小米的数据工场将数据导入到 Doris 中。用户可以直接在数据工场提交 Broker Load 任务并将数据直接导入 Doris 中，也可以通过 Spark SQL 将数据导入 Doris 中。Spark SQL 方式则是依赖了 Doris 社区提供的 Spark Doris Connector 组件，底层也是对 Doris 的 Stream Load 数据导入方式进行的封装。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数据查询">数据查询<a href="#数据查询" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/8c1cd3554e854dbe99aba27499e28118~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>用户通过数据工场将数据导入至 Doris 后即可进行查询，在小米内部是通过小米自研的数鲸平台来做查询的。用户可以通过数鲸平台对 Doris 进行查询可视化，并实现用户行为分析（为满足业务的事件分析、留存分析、漏斗分析、路径分析等行为分析需求，我们为 Doris 添加了相应的 UDF 和 UDAF ）和用户画像分析。</p><p>虽然目前依然需要将 Hive 的数据导过来，但 Doris 社区也正在支持湖仓一体能力，在后续实现湖仓一体能力后，我们会考虑直接通过 Doris 查询 Hive 与 Iceberg 外表。<strong>值得一提的是，Doris 1.1 版本已经实现支持查询 Iceberg 外表能力。</strong> 同时在即将发布的 <strong>1.2 版本</strong>中，还将支持 Hudi 外表并增加了 Multi Catalog ，可以实现外部表元数据的同步，无论是查询外部表的性能还是接入外表的易用性都有了很大的提升。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="compaction-调优">Compaction 调优<a href="#compaction-调优" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/92ad4ea90c564af2b720080b449c6edf~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>Doris 底层采用类似 LSM-Tree 方式，支持快速的数据写入。每一次的数据导入都会在底层的 Tablet 下生成一个新的数据版本，每个数据版本内都是一个个小的数据文件。单个文件内部是有序的，但是不同的文件之间又是无序的。为了使数据有序，在 Doris 底层就会存在 Compaction 机制，异步将底层小的数据版本合并成大的文件。Compaction 不及时就会造成版本累积，增加元数据的压力，并影响查询性能。由于 Compaction 任务本身又比较耗费机器CPU、内存与磁盘资源，如果 Compaction 开得太大就会占用过多的机器资源并影响到查询性能，同时也可能会造成 OOM。<strong>针对以上问题，我们一方面从业务侧着手，通过以下方面引导用户：</strong></p><ul><li>通过引导业务侧进行合理优化，对表设置<strong>合理的分区和分桶</strong>，避免生成过多的数据分片。</li><li>引导用户尽量<strong>降低数据的导入频率</strong> <strong>，</strong> <strong>增大单次数据导入的量</strong>，降低 Compaction 压力。</li><li>引导用户<strong>避免过多使用会在底层生成 Delete 版本的 Delete 操作</strong>。在 Doris 中 Compaction 分为 Base Compaction 与 Cumulative Compaction。Cumulative Compaction 会快速的把大量新导入的小版本进行快速的合并，在执行过程中若遇到 Delete 操作就会终止并将当前 Delete 操作版本之前的所有版本进行合并。由于 Cumulative Compaction 无法处理 Delete 版本，在合并完之后的版本会和当前版本一起放到 Base Compaction 中进行。当 Delete 版本特别多时， Cumulative Compaction 的步长也会相应变短，只能合并少量的文件，导致 Cumulative Compaction 不能很好的发挥小文件合并效果。</li></ul><p><strong>另一方面我们从运维侧着手：</strong></p><ul><li><strong>针对不同的业务集群配置不同的 Compaction 参数。</strong> 部分业务是实时写入数据的，需要的查询次数很多，我们就会将 Compaction 开的大一点以达到快速合并目的。而另外一部分业务只写今天的分区，但是只对之前的分区进行查询，在这种情况下，我们会适当的将 Compaction 放的小一点，避免 Compaction 占用过大内存或 CPU 资源。到晚上导入量变少时，之前导入的小版本能够被及时合并，对第二天查询效率不会有很大影响。</li><li><strong>适当降低 Base Compaction 任务优先级并增加 Cumulative Compaction 优先级。</strong> 根据上文提到的内容，Cumulative Compaction 能够快速合并大量生成的小文件，而 Base Compaction 由于合并的文件较大，执行的时间也会相应变长，读写放大也会比较严重。所以我们希望 Cumulative Compaction 优先、快速的进行。</li><li><strong>增加版本积压报警。</strong> 当我们收到版本积压报警时，动态调大 Compaction 参数，尽快消耗积压版本。</li><li><strong>支持手动触发指定表与分区下数据分片的 Compaction 任务。</strong> 由于 Compaction 不及时，部分表在查询时版本累积较多并需要能够快速进行合并。所以，我们支持对单个表或单个表下的某个分区提高 Compaction 优先级。</li></ul><p><strong>目前 Doris 社区针对以上问题已经做了</strong> <strong>一系列的优化</strong> <strong>，在 1.1 版本中</strong> <strong>大幅增强了数据 Compaction 能力，对于新增数据能够快速完成聚合，避免分片数据中的版本过多导致的 -235 错误以及带来的查询效率问题。</strong>\
<strong>首先</strong>，在 Doris 1.1 版本中，引入了 QuickCompaction，增加了主动触发式的 Compaction 检查，在数据版本增加的时候主动触发 Compaction。同时通过提升分片元信息扫描的能力，快速的发现数据版本多的分片，触发 Compaction。通过主动式触发加被动式扫描的方式，彻底解决数据合并的实时性问题。</p><p><strong>同时</strong>，针对高频的小文件 Cumulative Compaction，实现了 Compaction 任务的调度隔离，防止重量级的 Base Compaction 对新增数据的合并造成影响。</p><p><strong>最后</strong>，针对小文件合并，优化了小文件合并的策略，采用梯度合并的方式，每次参与合并的文件都属于同一个数据量级，防止大小差别很大的版本进行合并，逐渐有层次的合并，减少单个文件参与合并的次数，能够大幅的节省系统的 CPU 消耗。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cd2f0a547d6e4ddcb027715c4a544c5a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"><strong>在社区 1.1 新版本的测试结果中，不论是Compaction 的效率、CPU 的资源消耗，还是高频导入时的查询抖动，效果都有了大幅的提升。</strong></p><p><strong>具体可以参考：</strong> <a href="http://mp.weixin.qq.com/s?__biz=Mzg3Njc2NDAwOA==&amp;mid=2247500848&amp;idx=1&amp;sn=a667665ed4ccf4cf807a47be7c264f69&amp;chksm=cf2fca37f85843219e2f74d856478d4aa24d381c1d6e7f9f6a64b65f3344ce8451ad91c5af97&amp;scene=21#wechat_redirect" target="_blank" rel="noopener noreferrer">Apache Doris 1.1 特性揭秘：Flink 实时写入如何兼顾高吞吐和低延时</a></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="监控报警">监控报警<a href="#监控报警" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>Doris 的监控主要是通过 Prometheus 以及 Grafana 进行。对于 Doris 的报警则是通过 Falcon 进行。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/3fbe6b44f1124a91bf5ee17608f302d5~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">小米内部使用 Minos 进行集群部署。Minos 是小米内部自研并开源的大数据服务进程管理工具。在完成 Doris 集群部署后会更新至小米内部的轻舟数仓中。在轻舟数仓中的节点注册到 ZooKeeper 后，Prometheus 会监听 ZooKeeper 注册的节点，同时访问对应端口，拉取对应 Metrics 。在这之后，Grafana 会在面板上对监控信息进行显示，若有指标超过预设的报警阈值，Falcon 报警系统就会在报警群内报警，同时针对报警级别较高或某些无法及时响应的警告，可直接通过电话呼叫值班同学进行报警。</p><p>另外，小米内部针对每一个 Doris 集群都有 Cloud - Doris 的守护进程。Could - Doris 最大功能是可以对 Doris 进行可用性探测。比如我们每一分钟对 Doris 发送一次 select current timestamp(); 查询，若本次查询 20 秒没有返回，我们就会判断本次探测不可用。小米内部对每一个集群的可用性进行保证，通过上述探测方法，可以在小米内部输出 Doris可用性指标。</p><h1>小米对Apache Doris的优化实践</h1><p>在应用 Apache Doris 解决业务问题的同时，我们也发现了 Apache Doris 存在的一些优化项，因此在与社区进行沟通后我们开始深度参与社区开发，解决自身问题的同时也及时将开发的重要 Feature 回馈给社区，具体包括 Stream Load 两阶段提交（2PC）、单副本数据导入、Compaction 内存限制等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="stream-load-两阶段提交2pc">Stream Load 两阶段提交（2PC)<a href="#stream-load-两阶段提交2pc" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><strong>遇到的问题</strong></p><p>在 Flink 和 Spark 导入数据进 Doris 的过程中，当某些异常状况发生时可能会导致如下问题：</p><p><strong>Flink 数据重复导入</strong> <strong>：</strong> Flink 通过周期性 Checkpoint 机制处理容错并实现 EOS，通过主键或者两阶段提交实现包含外部存储的端到端 EOS。Doris-Flink-Connector 1.1 之前 UNIQUE KEY 表通过唯一键实现了EOS，非 UNIQUE KEY 表不支持 EOS。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e7750384cac44a569c8edf6c5de61744~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p> <strong>Spark SQL 数据部分导入</strong> <strong>：</strong> 通过 SparkSQL 从 Hive 表中查出的数据并写入 Doris 表中的过程需要使用到 Spark Doris Connector 组件，会将 Hive 中查询的数据通过多个 Stream Load 任务写入 Doris 中，出现异常时会导致部分数据导入成功，部分导入失败。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/936ffd500f364f838a9976584727ed42~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Stream Load 两阶段提交设计</strong></p><p>以上两个问题可以通过导入支持两阶段提交解决，第一阶段完成后确保数据不丢且数据不可见，这就能保证第二阶段发起提交时一定能成功，也能够保证第二阶段发起取消时一定能成功。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/50e59f3a78f74ba6a8dd2d7960497adb~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Doris 中的写入事务分为三步：</strong></p><ol><li>在  FE 上开始事务，状态为 Prepare ；</li><li>数据写入 BE；</li><li>多数副本写入成功的情况下，提交事务，状态变成 Committed，并且 FE 向 BE 下发 Publish Version 任务，让数据立即可见。</li></ol><p>引入两阶段提交之后，第 3 步变为状态修改为 Pre Commit，Publish Version 在第二阶段完成。用户在第一阶段完成后（事务状态为 Pre Commit ），可以选择在第二阶段放弃或者提交事务。</p><p><strong>支持 Flink Exactly-Once 语义</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ef5e0a81b441487ba7c3b3fa22e8c85d~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q">Doris-Flink-Connector 1.1 使用两阶段 Stream Load 并支持 Flink 两阶段提交实现了 EOS，只有全局的 Checkpoint 完成时，才会发起 Sream Load 的第二阶段提交，否则发起第二阶段放弃。</p><p><strong>解决 SparkSQL 数据部分导入</strong></p><p>Doris-Spark-Connector 使用两阶段 Stream Load 之后，成功的 Task 通过 Stream Load 第一阶段将写入数据到 Doris （Pre Commit 状态，不可见），当作业成功后，发起所有 Stream Load 第二阶段提交，作业失败时，发起所有 Stream Load 第二阶段取消。这就确保了不会有数据部分导入的问题。<img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/26b11a29566946c99b53ef90e01665ef~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="单副本数据导入优化">单副本数据导入优化<a href="#单副本数据导入优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><strong>单副本数据导入设计</strong></p><p><strong>Doris 通过多副本机制确保数据的高可靠以及系统高可用。</strong> 写入任务可以按照使用的资源分为计算和存储两类：排序、聚合、编码、压缩等使用的是 CPU 和内存的计算资源，最后的文件存储使用存储资源，三副本写入时计算和存储资源会占用三份。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/a0012b34b7404e5482700c281f6c206f~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>那能否只写一份副本数据在内存中，待到单副本写入完成并生成存储文件后，将文件同步到另外两份副本呢？答案是可行的，因此针对三副本写入的场景，我们做了单副本写入设计。<strong>单副本数据在内存中做完排序、聚合、编码以及压缩后，将文件同步至其他两个副本，这样很大程度上可以节省出 CPU 和内存资源。</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/e3528e0d75184068aa3b50384cb548d1~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>性能对比测试</strong></p><p><strong>Broker Load 导入 62G 数据性能对比</strong>
<strong>导入时间：</strong> 三副本导入耗时 33 分钟，单副本导入耗时 31 分钟。</p><p><strong>内存使用：</strong> 内存使用上优化效果十分明显，三副本数据导入的内存使用是单副本导入的三倍。单副本导入时只需要写一份内存，但是三副本导入时需要写三份内存，内存优化达到了 3 倍。</p><p><strong>CPU 消耗对比：</strong> 三副本导入的 CPU 消耗差不多是单副本的三倍。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/cbe6bb648e8d47d09c556eed4ffcdfa9~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>并发场景性能对比</strong></p><p>测试中向  100 个表并发导入数据，每个表有 50 个导入任务，任务总数为 5000 个。单个 Stream Load 任务导入的数据行是 200 万行，约为 90M 的数据。测试中开了 128 个并发，<strong>将</strong> <strong>单副本导入和三副本导入进行了对比：</strong></p><p><strong>导入时间：</strong> 3 副本导入耗时 67 分钟，而后单副本耗时 27 分钟完成。导入效率相当提升两倍以上。</p><p><strong>内存使用：</strong> 单副本的导入会更低。</p><p><strong>CPU消耗对比：</strong> 由于都已经是开了并发在导入，CPU开销都比较高，但是单副本导入吞吐提升明显。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/5a4f5533c4184f8caab39c38d951e410~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>Compaction 内存限制</strong></p><p>之前 Doris 在单机磁盘一次导入超过 2000 个 Segment 的情况下，Compaction 有内存 OOM 的问题。对于当天写入但不查当天数据而是查询之前的数据业务场景，我们会把 Compaction 稍微放的小一点，避免占用太大的内存，导致进程 OOM。Doris 之前每个磁盘有固定的线程做存储在这个盘上的数据的 Compaction，没有办法在全局进行管控。因为我们要限制单个节点上面内存的使用，<strong>所以我们将该模式改成了生产者-消费者模式：</strong></p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/ede14473f9104bdc89213e82398ba32a~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p>生产者不停的从所有的磁盘上面生产任务，之后将生产任务提交到线程池中。我们可以很好的把控线程池的入口，达到对 Compaction 的限制。我们在合并时会把底层的小文件进行归并排序，之后在内存里给每一个文件开辟 Block，所以我们可以近似认为占用的内存量与文件的数量是相关的，从而可以通过对单节点上同时执行合并的文件数量做限制，来达到控制内存的效果。</p><p><img loading="lazy" src="https://p3-juejin.byteimg.com/tos-cn-i-k3u1fbpfcp/00803f23d5a0427fb57abde4a2b1ec2d~tplv-k3u1fbpfcp-zoom-1.image" class="img_ev3q"></p><p><strong>我们增加了对单个 BE Compaction 合并的文件数量的限制。</strong> 若正在进行的 Compaction 的文件数量超过或等于当前限制时，后续提交上来的任务就需要等待，等到前面的 Compaction 任务做完并将指标释放出来后，后边提交进来的那些任务才可以进行。</p><p>通过这种方式，我们对某些业务场景做了内存的限制，很好的避免集群负载高时占用过多内存导致 OOM 的问题。</p><h1>总结</h1><p>自从 Apache Doris 从 2019 年上线第一个业务至今，<strong>目前 Apache Doris 已经在小米内部服务了数十个业务、集群数量达到数十个、节点规模达到数百台、每天完成数万次用户在线分析查询，承担了包括增长分析和报表查询等场景绝大多数在线分析的需求。</strong></p><p>与此同时，以上所列小米对于 Apache Doris 的优化实践，已经有部分功能已经在 Apache Doris 1.0 或 1.1 版本中发布，有部分 PR 已经合入社区 Master，在不久后发布的 1.2 新版本中应该就会与大家见面。随着社区的快速发展，有越来越多小伙伴参与到社区建设中，社区活跃度有了极大的提升。Apache Doris 已经变得越来越成熟，并开始从单一计算存储一体的分析型 MPP 数据库走向湖仓一体的道路，相信在未来还有更多的数据分析场景等待去探索和实现。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/jd">Apache Doris 在京东客服 OLAP 中的应用实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><h1><strong>引言</strong></h1><p>Apache Doris 是一款开源的 MPP 分析型数据库产品，不仅能够在亚秒级响应时间即可获得查询结果，有效的支持实时数据分析，而且支持 10PB 以上的超大的数据集。相较于其他业界比较火的 OLAP 数据库系统，Doris 的分布式架构非常简洁，支持弹性伸缩，易于运维，节省大量人力和时间成本。目前国内社区火热，也有美团、小米等大厂在使用。</p><p>本文主要讨论京东客服在人工咨询、客户事件单、售后服务单等专题的实时大屏，在实时和离线数据多维分析方面，如何利用 Doris 进行业务探索与实践。近些年来，随着数据量爆炸式的增长，以及海量数据联机分析需求的出现，MySQL、Oracle 等传统的关系型数据库在大数据量下遇到瓶颈，而 Hive、Kylin 等数据库缺乏时效性。于是 Apache Doris、Apache Druid、ClickHouse 等实时分析型数据库开始出现，不仅可以应对海量数据的秒级查询，更满足实时、准实时的分析需求。离线、实时计算引擎百花齐放。但是针对不同的场景，面临不同的问题，没有哪一种引擎是万能的。我们希望通过本文，对京东客服业务在离线与实时分析的应用与实践，能够给到大家一些启发，也希望大家多多交流，给我们提出宝贵的建议。</p><h1><strong>京东客服业务形态</strong></h1><p>京东客服作为集团服务的入口，为用户和商家提供了高效、可靠的保障。京东客服肩负着及时解决用户问题的重任，给用户提供详细易懂的说明与解释；为更好的了解用户的反馈以及产品的状况，需要实时的监控咨询量、接起率、投诉量等一系列指标，通过环比和同比，及时发现存在的问题，以更好的适应用户的购物方式，提高服务质量与效率，进而提高京东品牌的影响力。</p><h1><strong>Easy OLAP 设计</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-easyolap-doris-数据导入链路"><strong>01 EasyOLAP Doris 数据导入链路</strong><a href="#01-easyolap-doris-数据导入链路" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>EasyOLAP Doris 数据源主要是实时 Kafka 和离线 HDFS 文件。实时数据的导入依赖于 Routine Load 的方式；离线数据主要使用 Broker Load 和 Stream Load 的方式导入。</p><p><img loading="lazy" alt="EasyOLAP Doris 数据导入链路" src="https://cdnd.selectdb.com/zh-CN/assets/images/jd03-00bd471f0fab2d98798f5e3148b35fce.png" width="1080" height="604" class="img_ev3q"></p><p>EasyOLAP Doris 数据导入链路</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-easyolap-doris-全链路监控"><strong>02 EasyOLAP Doris 全链路监控</strong><a href="#02-easyolap-doris-全链路监控" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>目前 EasyOLAP Doris 项目的监控，使用的是 Prometheus + Grafana 框架。其中 node_exporter 负责采集机器层面的指标，Doris 也会自动以 Prometheus 格式吐出 FE、BE 的服务层面的指标。另外，部署了 OLAP Exporter 服务用于采集 Routine Load 相关的指标，旨在第一时间发现实时数据流导入的情况，确保实时数据的时效性。</p><p><img loading="lazy" alt="EasyOLAP Doris monitoring link" src="https://cdnd.selectdb.com/zh-CN/assets/images/jd04-8770adfb04ffe977f931d9eaff4cb534.png" width="1080" height="594" class="img_ev3q"></p><p>EasyOLAP Doris 监控链路</p><p><img loading="lazy" alt="640" src="https://cdnd.selectdb.com/zh-CN/assets/images/jd01-47257e8bb0b14785f854db959cdfd931.png" width="871" height="600" class="img_ev3q"></p><p>EasyOLAP Doris 监控面板展示</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-easyolap-doris-主备双流设计"><strong>03 EasyOLAP Doris 主备双流设计</strong><a href="#03-easyolap-doris-主备双流设计" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>EasyOLAP Doris 为了保障 0 级业务在大促期间服务的稳定性，采取了主备集群双写的方式。当其中一个集群出现抖动或者数据存在延迟的情况，用户可以自主地快速切换到另一个集群，尽可能的减少集群抖动给业务带来的影响。</p><p><img loading="lazy" alt="03 EasyOLAP Doris Primary-Secondary Dual Stream Design" src="https://cdnd.selectdb.com/zh-CN/assets/images/jd02-a6a4279c0c33a25862e89b56e7c986a7.png" width="1080" height="669" class="img_ev3q"></p><p>EasyOLAP Doris 主备双流设计</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="04-easyolap-doris-动态分区管理"><strong>04 EasyOLAP Doris 动态分区管理</strong><a href="#04-easyolap-doris-动态分区管理" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>京东 OLAP 团队分析需求之后，对 Doris 做了一定的定制化开发，其中就涉及到动态分区管理功能。尽管社区版本已经拥有动态分区的功能，但是该功能无法保留指定时间的分区。针对京东集团的特点，我们对指定时间的历史数据进行了留存，比如 618 和 11.11 期间的数据，不会因为动态分区而被删除。</p><p>动态分区管理功能能够控制集群中存储的数据量，而且方便了业务方的使用，无需手动或使用额外代码来管理分区信息。</p><h1><strong>Doris 缓存机制</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-需求场景"><strong>01 需求场景</strong><a href="#01-需求场景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>致力于不断提升用户体验，京东客服的数据分析追求极致的时效性。离线数据分析场景是写少读多，数据写入一次，多次频繁读取；实时数据分析场景，一部分数据是不更新的历史分区，一部分数据是处于更新的分区。在大部分的分析应用中，存在下述几种场景：</p><ul><li>高并发场景：Doris 较好的支持高并发，但是过高的 QPS 会引起集群抖动，且单个节点无法承载太高的 QPS ；</li><li>复杂查询：京东客服实时运营平台监控根据业务场景需展示多维复杂指标，丰富指标展示对应多种不同的查询，且数据源来自于多张表，虽然单个查询的响应时间在毫秒级别，但是整体的响应时间可能会到秒级别；</li><li>重复查询：如果没有防重刷机制，由于延迟或手误，重复刷新页面会导致提交大量重复的查询；</li></ul><p>针对上述场景，在应用层有解决方案——将查询结果放入到 Redis 中，缓存会周期性的刷新或者由用户手动刷新，但是也会存在一些问题：</p><ul><li>数据不一致：无法立即对数据的更新作出响应，用户接收到的结果可能是旧数据；</li><li>命中率低：如果数据实时性强，缓存频繁失效，则缓存的命中率低且系统的负载无法得缓解；</li><li>额外成本：引入外部组件，增加系统复杂度，增加额外成本。</li></ul><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-缓存机制简介"><strong>02 缓存机制简介</strong><a href="#02-缓存机制简介" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在 EasyOLAP Doris 中，一共有三种不同类型 Cache。根据适用场景的不同，分别为 Result Cache、SQL Cache 和 Partition Cache 。三种缓存都可以通过 MySQL 客户端指令控制开关。</p><p>这三种缓存机制是可以共存的，即可以同时开启。查询时，查询分析器首先会判断是否开启了 Result Cache ，在 Result Cache 开启的情况下先从 Result Cache 中查找该查询是否存在缓存，如果存在缓存，直接取缓存的值返回给客户端；如果缓存失效或者不存在，则直接进行查询并将结果写入到缓存。缓存放在各个 FE 节点的内存中，以便快速读取。</p><p>SQL Cache 按照 SQL 的签名、查询的表的分区的 ID 和分区最新版本号来存储和获取缓存。这三者一起作为缓存的条件，其中一者发生变化，如 SQL 语句变化、数据更新之后分区版本号变化，都会无法命中缓存。在多表 Join 的情况下，其中一张表的分区更新，也会导致无法命中缓存。SQL Cache 更适合 T+1 更新的场景。</p><p>Partition Cache 是更细粒度的缓存机制。Partition Cache 主要是将一个查询根据分区并行拆分，拆分为只读分区和可更新分区，只读分区缓存，更新分区不缓存，相应的结果集也会生成 n 个，然后再将各个拆分后的子查询的结果合并。因此，如果查询 N 天的数据，数据更新最近的 D 天，每天只是日期范围不一样但相似的查询，就可以利用 Partition Cache ，只需要查询 D 个分区即可，其他部分都来自缓存，可以有效降低集群负载，缩短查询响应时间。</p><p>一个查询进入到 Doris，系统先会处理查询语句，并将该查询语句作为 Key，在执行查询语句之前，查询分析器能够自动选择最适合的缓存机制，以确保在最优的情况下，利用缓存机制来缩短查询相应时间。然后检查 Cache 中是否存在该查询结果，如果存在就获取缓存中的数据返回给客户端；如果没有缓存，则正常查询，并将该查询结果以 Value 的形式和该查询语句 Key 存储到缓存中。Result Cache 可以在高并发场景下发挥其作用，也可以保护集群资源不受重复的大查询的侵占。SQL Cache 更加适合 T+1 的场景，在分区更新不频繁以及 SQL 语句重复的情况下，效果很好。Partition Cache 是粒度最小的缓存。在查询语句查询一个时间段的数据时，查询语句会被拆分成多个子查询。在数据只写一个分区或者部分分区的情况下，能够缩短查询时间，节省集群资源。</p><p>为了更好的观察缓存的效果，相关指标已经加入到 Doris 的服务指标中，通过 Prometheus 和 Grafana 监控系统获取直观的监控数据。指标有不同种类的 Cache 的命中数量、不同种类的 Cache 命中率、 Cache 的内存大小等指标。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-缓存机制效果"><strong>03 缓存机制效果</strong><a href="#03-缓存机制效果" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>京东客服 Doris 主集群，11.11 期间在没有开启缓存时，部分业务就导致 CPU 的使用率达到 100% ；在开启 Result Cache 的情况下，CPU 使用率在 30%-40% 之间。缓存机制确保业务在高并发场景下，能够快速的得到查询结果，并很好的保护了集群资源。</p><h1><strong>Doris 在 2020 年 11.11 大促期间的优化</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="01-导入任务优化"><strong>01 导入任务优化</strong><a href="#01-导入任务优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>实时数据的导入一直是一个挑战。其中，保证数据实时性和导入稳定性是最重要的。为了能够更加直观的观察实时数据导入的情况，京东 OLAP 团队自主开发了 OLAP Exporter ，用于采集实时数据导入相关的指标，如导入速度、导入积压和暂停的任务等。通过导入速度和导入积压，可以判断一个实时导入任务的状态，如发现任务有积压的趋势，可以使用自主开发的采样工具，对实时任务进行采样分析。实时任务主要有三个阈值来控制任务的提交，分别是每批次最大处理时间间隔、每批次最大处理条数和每批次最大处理数据量，一个任务只要达到其中一个阈值，该任务就会被提交。通过增加日志，发现 FE 中的任务队列比较繁忙，所以，参数的调整主要都是将每批次最大处理条数和每批次最大处理数据量调大，然后根据业务的需求，调整每批次最大处理时间间隔，以保证数据的延迟在每批次最大处理时间间隔的两倍之内。通过采样工具，分析任务，不仅保证了数据的实时性，也保证了导入的稳定性。另外，我们也设置了告警，可以及时发现实时导入任务的积压以及导入任务的暂停等异常情况。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="02-监控指标优化"><strong>02 监控指标优化</strong><a href="#02-监控指标优化" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>监控指标主要分为两个部分，一个是机器层面指标部分，一个是业务层面指标部分。在整个监控面板里，详细的指标带来了全面的数据的同时，也增加了获取重要指标的难度。所以，为了更好的观察所有集群的重要指标，单独设立一个板块—— 11.11 重要指标汇总板块。板块中有 BE CPU 使用率、实时任务消费积压行数、TP99、QPS 等指标。指标数量不多，但是可以观测到所有集群的情况，这样可以免去在监控中频繁切换的麻烦。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="03-周边工具支持"><strong>03 周边工具支持</strong><a href="#03-周边工具支持" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>除了上述说到的采样工具和 OLAP Exporter ，京东 OLAP 团队还开发了一系列的 Doris 维护工具。</p><ol><li><p>导入采样工具：导入采样工具不仅可以采集实时导入的数据，而且还支持调整实时导入任务的参数，或者在实时导入任务暂停状态下，生成创建语句（包括最新的位点等信息）用于任务的迁移等操作。</p></li><li><p>大查询工具：大查询不仅会造成集群 BE CPU 使用率的抖动，还会导致其他查询响应时间变长。在有大查询工具之前，发现集群 CPU 出现抖动，需要去检查所有 FE 上的审计日志，然后再做统计，不仅浪费时间，而且不够直观。大查询工具就是为了解决上述的问题。当监控侧发现集群有抖动，就可以使用大查询工具，输入集群名和时间点，就可以得到该时间点下，不同业务的查询总数，时间超过 5 秒、 10 秒、 20 秒的查询个数，扫描量巨大的查询个数等，方便我们从不同的维度分析大查询。大查询的详细情况也将被保存在中间文件中，可以直接获取不同业务的大查询。整个过程只需要几十秒到一分钟就可以定位到正在发生的大查询并获取相应的查询语句，大大节约了时间和运维成本。</p></li><li><p>降级与恢复工具：为了确保 11.11 大促期间， 0 级业务的稳定性，在集群压力超过安全位的时候，需要对其他非 0 级业务做降级处理，待度过高峰期后，再一键恢复到降级前的设置。降级主要是降低业务的最大连接数、暂停非 0 级的实时导入任务等。这大大增加了操作的便捷性，提高了效率。</p></li><li><p>集群巡检工具：在 11.11 期间，集群的健康巡检是极其重要的。常规巡检包括双流业务的主备集群一致性检查，为了确保业务在一个集群出现问题的时候可以快速切换到另一个集群，就需要保证两个集群上的库表一致、数据量差异不大等；检查库表的副本数是否为 3 且检查集群是否存在不健康的 Tablet ；检查机器磁盘使用率、内存等机器层面的指标等。</p></li></ol><h1><strong>总结与展望</strong></h1><p>京东客服是在 2020 年年初开始引入 Doris 的，目前拥有一个独立集群，一个共享集群，是京东 OLAP 的资深用户。</p><p>在业务使用中也遇到了例如任务调度相关的、导入任务配置相关的和查询相关等问题，这也在推动京东 OLAP 团队更深入的了解 Doris 。我们计划推广使用物化视图来进一步提升查询的效率；使用 Bitmap 来支持 UV 等指标的精确去重操作；使用审计日志，更方便的统计大查询、慢查询；解决实时导入任务的调度问题，使导入任务更加高效稳定。除此之外，我们也计划优化建表、创建优质 Rollup 或物化视图以提升应用的流畅性，加速更多业务向 OLAP 平台靠拢，以提升应用的影响力。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/meituan">Apache Doris 在美团外卖实时数仓建设中的实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">Apache Doris</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><p><strong>导读：</strong>本文主要介绍一种通用的实时数仓构建的方法与实践。实时数仓以端到端低延迟、SQL 标准化、快速响应变化、数据统一为目标。在实践中，我们总结的最佳实践是：一个通用的实时生产平台 + 一个通用交互式实时分析引擎相互配合同时满足实时和准实时业务场景。两者合理分工，互相补充，形成易于开发、易于维护、效率最高的流水线，兼顾开发效率与生产成本，以较好的投入产出比满足业务多样需求。</p><h1><strong>实时场景</strong></h1><p>实时数据在美团外卖的场景是非常多的，主要有以下几点：</p><ul><li><p>运营层面：比如实时业务变化，实时营销效果，当日营业情况以及当日实时业务趋势分析等。</p></li><li><p>生产层面：比如实时系统是否可靠，系统是否稳定，实时监控系统的健康状况等。</p></li><li><p>C 端用户：比如搜索推荐排序，需要实时了解用户的想法，行为、特点，给用户推荐更加关注的内容。</p></li><li><p>风控侧：在外卖以及金融科技用的是非常多的，实时风险识别，反欺诈，异常交易等，都是大量应用实时数据的场景</p></li></ul><h1><strong>实时技术及架构</strong></h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1实时计算技术选型"><strong>1.实时计算技术选型</strong><a href="#1实时计算技术选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>目前开源的实时技术比较多，比较通用的是 Storm、Spark Streaming 以及 Flink，具体要根据不同公司的业务情况进行选型。</p><p>美团外卖是依托美团整体的基础数据体系建设，从技术成熟度来讲，前几年用的是 Storm，Storm 当时在性能稳定性、可靠性以及扩展性上是无可替代的，随着 Flink 越来越成熟，从技术性能上以及框架设计优势上已经超越 Storm，从趋势来讲就像 Spark 替代 MR 一样，Storm 也会慢慢被 Flink 替代，当然从 Storm 迁移到 Flink 会有一个过程，我们目前有一些老的任务仍然在 Storm 上，也在不断推进任务迁移。</p><p>具体 Storm 和 Flink 的对比可以参考上图表格。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2实时架构">2.<strong>实时架构</strong><a href="#2实时架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p><strong>① Lambda 架构</strong></p><p>Lambda 架构是比较经典的架构，以前实时的场景不是很多，以离线为主，当附加了实时场景后，由于离线和实时的时效性不同，导致技术生态是不一样的。Lambda 架构相当于附加了一条实时生产链路，在应用层面进行一个整合，双路生产，各自独立。这在业务应用中也是顺理成章采用的一种方式。</p><p>双路生产会存在一些问题，比如加工逻辑 double，开发运维也会 double，资源同样会变成两个资源链路。因为存在以上问题，所以又演进了一个 Kappa 架构。</p><p><strong>② Kappa 架构</strong></p><p>Kappa 架构从架构设计来讲比较简单，生产统一，一套逻辑同时生产离线和实时。但是在实际应用场景有比较大的局限性，在业内直接用 Kappa 架构生产落地的案例不多见，且场景比较单一。这些问题在我们这边同样会遇到，我们也会有自己的一些思考，在后面会讲到。</p><h1><strong>业务痛点</strong></h1><p>在外卖业务上，我们也遇到了一些问题。</p><p>业务早期，为了满足业务需要，一般是拿到需求后 case by case 的先把需求完成，业务对于实时性要求是很高的，从时效性来说，没有进行中间层沉淀的机会，在这种场景下，一般是拿到业务逻辑直接嵌入，这是能想到的简单有效的方法，在业务发展初期这种开发模式比较常见。</p><p>如上图所示，拿到数据源后，会经过数据清洗，扩维，通过 Storm 或 Flink 进行业务逻辑处理，最后直接进行业务输出。把这个环节拆开来看，数据源端会重复引用相同的数据源，后面进行清洗、过滤、扩维等操作，都要重复做一遍，唯一不同的是业务的代码逻辑是不一样的，如果业务较少，这种模式还可以接受，但当后续业务量上去后，会出现谁开发谁运维的情况，维护工作量会越来越大，作业无法形成统一管理。而且所有人都在申请资源，导致资源成本急速膨胀，资源不能集约有效利用，因此要思考如何从整体来进行实时数据的建设。</p><h1><strong>数据特点与应用场景</strong></h1><p>那么如何来构建实时数仓呢？</p><p>首先要进行拆解，有哪些数据，有哪些场景，这些场景有哪些共同特点，对于外卖场景来说一共有两大类，日志类和业务类。</p><ul><li><p>日志类：数据量特别大，半结构化，嵌套比较深。日志类的数据有个很大的特点，日志流一旦形成是不会变的，通过埋点的方式收集平台所有的日志，统一进行采集分发，就像一颗树，树根非常大，推到前端应用的时候，相当于从树根到树枝分叉的过程（从 1 到 n 的分解过程），如果所有的业务都从根上找数据，看起来路径最短，但包袱太重，数据检索效率低。日志类数据一般用于生产监控和用户行为分析，时效性要求比较高，时间窗口一般是 5min 或 10min 或截止到当前的一个状态，主要的应用是实时大屏和实时特征，例如用户每一次点击行为都能够立刻感知到等需求。</p></li><li><p>业务类：主要是业务交易数据，业务系统一般是自成体系的，以 Binlog 日志的形式往下分发，业务系统都是事务型的，主要采用范式建模方式，特点是结构化的，主体非常清晰，但数据表较多，需要多表关联才能表达完整业务，因此是一个 n 到 1 的集成加工过程。</p></li></ul><p>业务类实时处理面临的几个难点：</p><ul><li><p>业务的多状态性：业务过程从开始到结束是不断变化的，比如从下单-&gt;支付-&gt;配送，业务库是在原始基础上进行变更的，Binlog 会产生很多变化的日志。而业务分析更加关注最终状态，由此产生数据回撤计算的问题，例如 10 点下单，13 点取消，但希望在 10 点减掉取消单。</p></li><li><p>业务集成：业务分析数据一般无法通过单一主体表达，往往是很多表进行关联，才能得到想要的信息，在实时流中进行数据的合流对齐，往往需要较大的缓存处理且复杂。</p></li><li><p>分析是批量的，处理过程是流式的：对单一数据，无法形成分析，因此分析对象一定是批量的，而数据加工是逐条的。</p></li></ul><p>日志类和业务类的场景一般是同时存在的，交织在一起，无论是 Lambda 架构还是 Kappa 架构，单一的应用都会有一些问题。因此针对场景来选择架构与实践才更有意义。</p><h1><strong>实时</strong>数仓架构设计</h1><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1-实时架构流批结合的探索"><strong>1. 实时架构：流批结合的探索</strong><a href="#1-实时架构流批结合的探索" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>基于以上问题，我们有自己的思考。通过流批结合的方式来应对不同的业务场景。</p><p>如上图所示，数据从日志统一采集到消息队列，再到数据流的 ETL 过程，作为基础数据流的建设是统一的。之后对于日志类实时特征，实时大屏类应用走实时流计算。对于 Binlog 类业务分析走实时 OLAP 批处理。</p><p>流式处理分析业务的痛点？对于范式业务，Storm 和 Flink 都需要很大的外存，来实现数据流之间的业务对齐，需要大量的计算资源。且由于外存的限制，必须进行窗口的限定策略，最终可能放弃一些数据。计算之后，一般是存到 Redis 里做查询支撑，且 KV 存储在应对分析类查询场景中也有较多局限。</p><p>实时 OLAP 怎么实现？有没有一种自带存储的实时计算引擎，当实时数据来了之后，可以灵活的在一定范围内自由计算，并且有一定的数据承载能力，同时支持分析查询响应呢？随着技术的发展，目前 MPP 引擎发展非常迅速，性能也在飞快提升，所以在这种场景下就有了一种新的可能。这里我们使用的是 Doris 引擎。</p><p>这种想法在业内也已经有实践，且成为一个重要探索方向。阿里基于 ADB 的实时 OLAP 方案等。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2-实时数仓架构设计"><strong>2. 实时数仓架构设计</strong><a href="#2-实时数仓架构设计" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>从整个实时数仓架构来看，首先考虑的是如何管理所有的实时数据，资源如何有效整合，数据如何进行建设。</p><p>从方法论来讲，实时和离线是非常相似的，离线数仓早期的时候也是 case by case，当数据规模涨到一定量的时候才会考虑如何治理。分层是一种非常有效的数据治理方式，所以在实时数仓如何进行管理的问题上，首先考虑的也是分层的处理逻辑，具体如下：</p><ul><li><p>数据源：在数据源的层面，离线和实时在数据源是一致的，主要分为日志类和业务类，日志类又包括用户日志，DB 日志以及服务器日志等。</p></li><li><p>实时明细层：在明细层，为了解决重复建设的问题，要进行统一构建，利用离线数仓的模式，建设统一的基础明细数据层，按照主题进行管理，明细层的目的是给下游提供直接可用的数据，因此要对基础层进行统一的加工，比如清洗、过滤、扩维等。</p></li><li><p>汇总层：汇总层通过 Flink 或 Storm 的简洁算子直接可以算出结果，并且形成汇总指标池，所有的指标都统一在汇总层加工，所有人按照统一的规范管理建设，形成可复用的汇总结果。</p></li></ul><p>总结起来，从整个实时数仓的建设角度来讲，首先数据建设的层次化要先建出来，先搭框架，然后定规范，每一层加工到什么程度，每一层用什么样的方式，当规范定义出来后，便于在生产上进行标准化的加工。由于要保证时效性，设计的时候，层次不能太多，对于实时性要求比较高的场景，基本可以走上图左侧的数据流，对于批量处理的需求，可以从实时明细层导入到实时 OLAP 引擎里，基于 OLAP 引擎自身的计算和查询能力进行快速的回撤计算，如上图右侧的数据流。</p><h1><strong>实时平台化建设</strong></h1><p>架构确定之后，后面考虑的是如何进行平台化的建设，实时平台化建设完全附加于实时数仓管理之上进行的。</p><p>首先进行功能的抽象，把功能抽象成组件，这样就可以达到标准化的生产，系统化的保障就可以更深入的建设，对于基础加工层的清洗、过滤、合流、扩维、转换、加密、筛选等功能都可以抽象出来，基础层通过这种组件化的方式构建直接可用的数据结果流。这其中会有一个问题，用户的需求多样，满足了这个用户，如何兼容其他的用户，因此可能会出现冗余加工的情况，从存储来讲，实时数据不存历史，不会消耗过多的存储，这种冗余是可以接受的，通过冗余的方式可以提高生产效率，是一种空间换时间的思想应用。</p><p>通过基础层的加工，数据全部沉淀到 IDL 层，同时写到 OLAP 引擎的基础层，再往上是实时汇总层计算，基于 Storm、Flink 或 Doris，生产多维度的汇总指标，形成统一的汇总层，进行统一的存储分发。</p><p>当这些功能都有了以后，元数据管理，指标管理，数据安全性、SLA、数据质量等系统能力也会逐渐构建起来。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="1实时基础层功能">1.实时基础层功能<a href="#1实时基础层功能" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>实时基础层的建设要解决一些问题。</p><p>首先是一条流重复读的问题，一条 Binlog 打过来，是以 DB 包的形式存在的，用户可能只用其中一张表，如果大家都要用，可能存在所有人都要接这个流的问题。解决方案是可以按照不同的业务解构出来，还原到基础数据流层，根据业务的需要做成范式结构，按照数仓的建模方式进行集成化的主题建设。</p><p>其次要进行组件的封装，比如基础层的清洗、过滤、扩维等功能，通过一个很简单的表达入口，让用户将逻辑写出来。trans 环节是比较灵活的，比如从一个值转换成另外一个值，对于这种自定义逻辑表达，我们也开放了自定义组件，可以通过 Java 或 Python 开发自定义脚本，进行数据加工。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="2实时特征生产功能">2.<strong>实时特征生产功能</strong><a href="#2实时特征生产功能" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>特征生产可以通过 SQL 语法进行逻辑表达，底层进行逻辑的适配，透传到计算引擎，屏蔽用户对计算引擎的依赖。就像对于离线场景，目前大公司很少通过代码的方式开发，除非一些特别的 case，所以基本上可以通过 SQL 化的方式表达。</p><p>在功能层面，把指标管理的思想融合进去，原子指标、派生指标，标准计算口径，维度选择，窗口设置等操作都可以通过配置化的方式，这样可以统一解析生产逻辑，进行统一封装。</p><p>还有一个问题，同一个源，写了很多 SQL，每一次提交都会起一个数据流，比较浪费资源，我们的解决方案是，通过同一条流实现动态指标的生产，在不停服务的情况下可以动态添加指标。</p><p>所以在实时平台建设过程中，更多考虑的是如何更有效的利用资源，在哪些环节更能节约化的使用资源，这是在工程方面更多考虑的事情。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="3sla-建设">3.SLA 建设<a href="#3sla-建设" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>SLA 主要解决两个问题，一个是端到端的 SLA，一个是作业生产效率的 SLA，我们采用埋点+上报的方式，由于实时流比较大，埋点要尽量简单，不能埋太多的东西，能表达业务即可，每个作业的输出统一上报到 SLA 监控平台，通过统一接口的形式，在每一个作业点上报所需要的信息，最后能够统计到端到端的 SLA。</p><p>在实时生产中，由于链路非常长，无法控制所有链路，但是可以控制自己作业的效率，所以作业 SLA 也是必不可少的。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="4-实时-olap-方案">4. 实时 OLAP 方案<a href="#4-实时-olap-方案" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>问题：</p><ul><li><p>Binlog 业务还原复杂：业务变化很多，需要某个时间点的变化，因此需要进行排序，并且数据要存起来，这对于内存和 CPU 的资源消耗都是非常大的。</p></li><li><p>Binlog 业务关联复杂：流式计算里，流和流之间的关联，对于业务逻辑的表达是非常困难的。</p></li></ul><p>解决方案：</p><p>通过带计算能力的 OLAP 引擎来解决，不需要把一个流进行逻辑化映射，只需要解决数据实时稳定的入库问题。</p><p>我们这边采用的是 Doris 作为高性能的 OLAP 引擎，由于业务数据产生的结果和结果之间还需要进行衍生计算，Doris 可以利用 unique 模型或聚合模型快速还原业务，还原业务的同时还可以进行汇总层的聚合，也是为了复用而设计。应用层可以是物理的，也可以是逻辑化视图。</p><p>这种模式重在解决业务回撤计算，比如业务状态改变，需要在历史的某个点将值变更，这种场景用流计算的成本非常大，OLAP 模式可以很好的解决这个问题。</p><h1>实时应用案例</h1><p>最后通过一个案例说明，比如商家要根据用户历史下单数给用户优惠，商家需要看到历史下了多少单，历史 T+1 的数据要有，今天实时的数据也要有，这种场景是典型的 Lambda 架构，可以在 Doris 里设计一个分区表，一个是历史分区，一个是今日分区，历史分区可以通过离线的方式生产，今日指标可以通过实时的方式计算，写到今日分区里，查询的时候进行一个简单的汇总。</p><p>这种场景看起来比较简单，难点在于商家的量上来之后，很多简单的问题都会变的复杂，因此后面我们也会通过更多的业务输入，沉淀出更多的业务场景，抽象出来形成统一的生产方案和功能，以最小化的实时计算资源支撑多样化的业务需求，这也是未来需要达到的目的。</p><p>今天的分享就到这里，谢谢大家。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="嘉宾介绍">嘉宾介绍：<a href="#嘉宾介绍" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>朱良，5 年以上传统行业数仓建设经验，6 年互联网数仓经验，技术方向涉及离线，实时数仓治理，系统化能力建设，OLAP 系统及引擎，大数据相关技术，重点跟进 OLAP，实时技术前沿发展趋势。业务方向涉及即席查询，运营分析，策略报告产品，用户画像，人群推荐，实验评估等。</p></div></article><article class="margin-bottom--xl" itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting"><header><h2 class="blog-post-title" itemprop="headline"><a itemprop="url" href="/zh-CN/blog/scenario">打造自助对话式数据分析场景，Apache Doris 在思必驰的应用实践｜最佳实践</a></h2><div class="blog-info"><time datetime="2022-07-20T00:00:00.000Z" itemprop="datePublished">2022年7月20日</time><span class="split-line"></span><span class="authors"><span class="s-author">赵伟</span></span><span class="split-line"></span><span class="s-tags"><span class="s-tag">最佳实践</span></span></div></header><div class="markdown" itemprop="articleBody"><blockquote><p>作者：赵伟，思必驰大数据高级研发，10年大数据开发和设计经验，负责大数据平台基础技术和OLAP分析技术开发。社区贡献：Doris-spark-connector 的实时读写和优化。</p></blockquote><h1>业务背景</h1><p>思必驰是国内专业的对话式人工智能平台公司，拥有全链路的智能语音语言技术，致力于成为全链路智能语音及语言交互的平台型企业，自主研发了新一代人机交互平台 DUI 和人工智能芯片 TH1520，为车联网、IoT 及政务、金融等众多行业场景合作伙伴提供自然语言交互解决方案。</p><p>思必驰于 2019 年首次引入 Apache Doris ，基于 Apache Doris 构建了实时与离线一体的数仓架构。相对于过去架构，Apache Doris 凭借其灵活的查询模型、极低的运维成本、短平快的开发链路以及优秀的查询性能等诸多方面优势，如今已经在实时业务运营、自助/对话式分析等多个业务场景得到运用，满足了 设备画像/用户标签、业务场景实时运营、数据分析看板、自助 BI、财务对账等多种数据分析需求。在这一过程中我们也积累了诸多使用上的经验，在此分享给大家。</p><h1>架构演进</h1><p>早期业务中离线数据分析是我们的主要需求，近几年，随着业务的不断发展，业务场景对实时数据分析的要求也越来越高，早期数仓架构逐渐力不从心，暴露出很多问题。为了满足业务场景对查询性能、响应时间及并发能力更高的要求，2019年正式引入 Apache Doris 构建实时离线一体的数仓架构。</p><p>以下将为大家介绍思必驰数仓架构的演进之路，早期数仓存在的优缺点，同时分享我们选择 Apache Doris 构建新架构的原因以及面临的新问题与挑战。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="早期数仓架构及痛点">早期数仓架构及痛点<a href="#早期数仓架构及痛点" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="data_wharehouse_architecture_v1_0_git" src="https://cdnd.selectdb.com/zh-CN/assets/images/data_wharehouse_architecture_v1_0_git-006b22817872b04ad8f909e54e8c1411.png" width="1953" height="1106" class="img_ev3q"></p><p>如上图所示，早期架构基于 Hive +Kylin 来构建离线数仓，实时数仓架基于 Spark+MySQL 来构建实时分析数仓。</p><p>我们业务场景的数据源主要分为三类，业务数据库如 MySQL，应用系统如 K8s 容器服务日志，还有车机设备终端的日志。数据源通过 MQTT/HTTP 协议、业务数据库 Binlog 、Filebeat日志采集等多种方式先写入 Kafka 。在早期架构中，数据经 Kafka 后将分为实时和离线两条链路，首先是实时部分，实时部分链路较短，经过 Kafka 缓冲完的数据通过 Spark 计算后放入 MySQL 中进行分析，对于早期的实时分析需求，MySQL 基本可以满足分析需求。而离线部分则由 Spark 进行数据清洗及计算后在 Hive 中构建离线数仓，并使用 Apache Kylin 构建 Cube，在构建 Cube 之前需要提前做好数据模型的的设计，包括关联表、维度表、指标字段、指标需要的聚合函数等，通过调度系统进行定时触发构建，最终使用 HBase 存储构建好的 Cube。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="早期架构的优势"><strong>早期架构的优势：</strong><a href="#早期架构的优势" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>早期架构与 Hive 结合较好，无缝对接 Hadoop 技术体系。</p></li><li><p>离线数仓中基于 Kylin 的预计算、表关联、聚合计算、精确去重等场景，查询性能较高，在并发场景下查询稳定性也较高。</p></li></ol><p>早期架构解决了当时业务中较为紧迫的查询性能问题，但随着业务的发展，对数据分析要求不断升高，早期架构缺点也开始逐渐凸显出来。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="早期架构的痛点"><strong>早期架构的痛点：</strong><a href="#早期架构的痛点" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>依赖组件多。Kylin 在 2.x、3.x 版本中强依赖 Hadoop 和 HBase ，应用组件较多导致开发链路较长，架构稳定性隐患多，维护成本比很高。</p></li><li><p>Kylin 的构建过程复杂，构建任务容易失败。Kylin 构建需要进行打宽表、去重列、生成字典，构建 Cube 等如果每天有 1000-2000 个甚至更多的任务，其中至少会有 10 个甚至更多任务构建失败，导致需要大量时间去写自动运维脚本。</p></li><li><p>维度/字典膨胀严重。维度膨胀指的是在某些业务场景中需要多个分析条件和字段，如果在数据分析模型中选择了很多字段而没有进行剪枝，则会导致 Cube 维度膨胀严重，构建时间变长。而字典膨胀指的是在某些场景中需要长时间做全局精确去重，会使得字典构建越来越大，构建时间也会越来越长，从而导致数据分析性能持续下降。</p></li><li><p>数据分析模型固定，灵活性较低。在实际应用过程中，如果对计算字段或者业务场景进行变更，则要回溯部分甚至全部数据。</p></li><li><p>不支持数据明细查询。早期数仓架构是无法提供明细数据查询的，Kylin 官方给的解决方法是下推给 Presto 做明细查询，这又引入了新的架构，增加了开发和运维成本。</p></li></ol><h2 class="anchor anchorWithStickyNavbar_LWe7" id="架构选型">架构选型<a href="#架构选型" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>为解决以上问题，我们开始探索新的数仓架构优化方案，先后对市面上应用最为广泛的 Apache Doris、Clickhouse 等 OLAP 引擎进行选型调研。相较于 ClickHouse 的繁重运维、各种各样的表类型、不支持关联查询等，结合我们的 OLAP 分析场景中的需求，综合考虑，Apache Doris 表现较为优秀，最终决定引入 Apache Doris 。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="新数仓架构">新数仓架构<a href="#新数仓架构" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="data_wharehouse_architecture_v2_0_git" src="https://cdnd.selectdb.com/zh-CN/assets/images/data_wharehouse_architecture_v2_0_git-825df043f0abf0fda4a92b8dc5d10956.png" width="1993" height="1144" class="img_ev3q"></p><p>如上图所示，我们基于 Apache Doris 构建了实时+离线一体的新数仓架构，与早期架构不同的是，实时和离线的数据分别进行处理后均写入 Apache Doris 中进行分析。</p><p>因历史原因数据迁移难度较大，离线部分基本和早期数仓架构保持一致，在Hive上构建离线数仓，当然完全可以在Apache Doris 上直接构建离线数仓。</p><p>相对早期架构不同的是，离线数据通过 Spark 进行清洗计算后在 Hive 中构建数仓，然后通过 Broker Load 将存储在 Hive 中的数据写入到 Apache Doris 中。这里要说明的， Broker Load 数据导入速度很快，天级别 100-200G 数据导入到 Apache Doris 中仅需要 10-20 分钟。</p><p>实时数据流部分，新架构使用了 Doris-Spark-Connector 来消费 Kafka 中的数据并经过简单计算后写入 Apache Doris 。从架构图所示，实时和离线数据统一在 Apache Doris 进行分析处理，满足了数据应用的业务需求，实现了实时+离线一体的数仓架构。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="新架构的收益"><strong>新架构的收益：</strong><a href="#新架构的收益" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>极简运维，维护成本低，不依赖 Hadoop 生态组件。Apache Doris 的部署简单，只有 FE 和 BE 两个进程， FE 和 BE 进程都是可以横向扩展的，单集群支持到数百台机器，数十 PB 的存储容量，并且这两类进程通过一致性协议来保证服务的高可用和数据的高可靠。这种高度集成的架构设计极大的降低了一款分布式系统的运维成本。在使用 Doris 三年时间中花费的运维时间非常少，相比于基于 Kylin 搭建的早期架构，新架构花费极少的时间去做运维。</p></li><li><p>链路短，开发排查问题难度大大降低。基于 Doris 构建实时和离线统一数仓，支持实时数据服务、交互数据分析和离线数据处理场景，这使得开发链路变的很短，问题排查难度大大降低。</p></li><li><p>支持 Runtime 形式的 Join 查询。Runtime 类似 MySQL 的表关联，这对数据分析模型频繁变更的场景非常友好，解决了早期结构数据模型灵活性较低的问题。</p></li><li><p>同时支持 Join、聚合、明细查询。解决了早期架构中部分场景无法查询数据明细的问题。</p></li><li><p>支持多种加速查询方式。支持上卷索引，物化视图，通过上卷索引实现二级索引来加速查询，极大的提升了查询响应时间。</p></li><li><p>支持多种联邦查询方式。支持对 Hive、Iceberg、Hudi 等数据湖和 MySQL、Elasticsearch 等数据库的联邦查询分析。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="问题和挑战"><strong>问题和挑战：</strong><a href="#问题和挑战" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>在建设新数仓架构过程中，我们遇到了一些问题：</p><ul><li><p>高并发场景对 Apache Doris 查询性能存在一定影响。我们分别在 Doris 0.12 和 Doris 1.1版本上进行测试，同一时间同样的 SQL，10 并发和 50 并发进行访问，性能差别较大。</p></li><li><p>在实时写入场景中，当实时写入的数据量比较大时，会使得 IO 比较密集，导致查询性能下降。</p></li><li><p>大数据量下字符串精确去重较慢。目前使用的是 count distinct 函数、Shuffle 和聚合算子去重，此方式算力比较慢。当前业内常见的解决方法一般是针对去重列构建字典，基于字典构建 Bitmap 索引后使用 Bitmap 函数去重。目前 Apache Doris 只支持数字类型的 Bitmap 索引，具有一定的局限性。</p></li></ul><h1>业务场景的应用</h1><p>Apache Doris 在思必驰最先应用在实时运营业务场景以及自助/对话式分析场景，本章节将介绍两个场景的需求及应用情况。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="实时运营业务场景">实时运营业务场景<a href="#实时运营业务场景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="real-time_operation_git" src="https://cdnd.selectdb.com/zh-CN/assets/images/real-time_operation_git-87d6e8ede096ba1551cb290941741126.png" width="1977" height="1226" class="img_ev3q"></p><p>首先是实时运营业务场景，如上图所示，实时运营业务场景的技术架构和前文所述的新版数仓架构基本一致：</p><ul><li><p>数据源：数据源新版架构图中一致，包括 MySQL 中的业务数据，应用系统埋点数据以及设备和终端日志。</p></li><li><p>数据导入：离线数据导入使用 Broker Load，实时数据导入使用 Doris-Spark-Connector 。</p></li><li><p>数据存储与开发：几乎所有的实时数仓全部在 Apache Doris 构建，有一部分离线数据放在 Airflow 上执行 DAG 跑批任务。</p></li><li><p>数据应用：最上层是业务侧提出的业务分析需求，包括大屏展示，数据运营的实时看板、用户画像、BI 看板等。</p></li></ul><p><strong>在实时运营业务场景中，数据分析的需求主要有两方面：</strong></p><ul><li><p>由于实时导入数据量比较大，因此对实时数据的查询效率要求较高</p></li><li><p>在此场景中，有 20+ 人的团队在运营，需要同时开数据运营的看板，因此对实时写入的性能和查询并发会有比较高的要求。</p></li></ul><h2 class="anchor anchorWithStickyNavbar_LWe7" id="自助对话式分析场景">自助/对话式分析场景<a href="#自助对话式分析场景" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p>除以上之外，Apache Doris 在思必驰第二个应用是自助/对话式分析场景。</p><p><img loading="lazy" alt="ai_chatbots_git" src="https://cdnd.selectdb.com/zh-CN/assets/images/ai_chatbots_git-f094d1221b56b522cb93ba3bc766e659.png" width="1953" height="1118" class="img_ev3q"></p><p>如上图所示，在一般的 BI 场景中，用户方比如商务、财务、销售、运营、项目经理等会提出需求给数据分析人员，数据分析人员在 BI 平台上做数据看板，最终把看板提供给用户，用户从 BI 看板上获取所需信息，但是有时候用户想要查看明细数据、定制化的看板需求，或者在某些场景需做任意维度的上卷或者下钻的分析，一般场景下 BI 看板是不支持的的，基于以上所述用户需求，我们打造了自助对话式 BI 场景来解决用户定制化的需求。</p><p>与一般 BI 场景不同的是，我们将自助/对话式 BI 场景从数据分析人员方下沉到用户方，用户方只需要通过打字，描述数据分析的需求。基于我们公司自然语言处理的能力，自助/对话式 BI 场景会将自然语言转换成SQL，类似 NL2SQL 技术，需要说明的是这里使用的是定制的自然语言解析，相对开源的 NL2SQL 命中率高、解析结果更精确。当自然语言转换成 SQL 后，将 SQL 给到 Apache Doris 查询得到分析结果。由此，用户通过打字就可以随时查看任意场景下的明细数据，或者任意字段的上卷、下钻。</p><p>相比 Apache Kylin、Apache Druid 等预计算的 OLAP 引擎，Apache Doris 符合以下几个特点：</p><ul><li><p>查询灵活，模型不固定，支持自由定制场景。</p></li><li><p>支持表关联、聚合计算、明细查询。</p></li><li><p>响应时间要快速。</p></li></ul><p>因此我们很顺利的运用 Apache Doris 实现了自助/对话式分析场景。同时，自助/对话式分析在我们公司多个数据分析场景应用反馈非常好。</p><h1>实践经验</h1><p>基于上面的两个场景，我们使用过程当中积累了一些经验和心得，分享给大家。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="数仓-表设计"><strong>数仓</strong> <strong>表设计：</strong><a href="#数仓-表设计" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>千万级(量级供参考，跟集群规模有关系)以下的数据表使用 Duplicate 表类型，Duplicate 表类型同时支持聚合、明细查询，不需要额外写明细表。</p></li><li><p>当数据量比较大时，使用 Aggregate 聚合表类型，在聚合表类型上做上卷索引，使用物化视图优化查询、优化聚合字段。由于 Aggregate 表类型是预计算表，会丢失明细数据，如有明细查询需求，需要额外写一张明细表。</p></li><li><p>当数据量又大、关联表又多时，可用 ETL 先写成宽表，然后导入到 Doris，结合 Aggregate 在聚合表类型上面做优化，也可以使用官方推荐Doris 的 Join 优化：<a href="https://doris.apache.org/zh-CN/docs/dev/advanced/join-optimization/doris-join-optimization" target="_blank" rel="noopener noreferrer">https://doris.apache.org/zh-CN/docs/dev/advanced/join-optimization/doris-join-optimization</a></p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="写入"><strong>写入：</strong><a href="#写入" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>通过 Spark Connector 或 Flink Connector 替代 Routine Load： 最早我们使用的是 Routine Load 实时写入 BE 节点， Routine Load 的工作原理是通过 SQL 在 FE 节点起一个类似于 Task Manager 的管理，把任务分发给 BE 节点，在 BE 节点起 Routine Load 任务。在我们实时场景并发很高的情况下，BE 节点 CPU 峰值一般会达到 70% 左右，在这个前提下，Routine Load 也跑到 BE 节点，将严重影响 BE 节点的查询性能，并且查询 CPU 也将影响 Routine Load 导入， Routine Load 就会因为各种资源竞争死掉。面对此问题，目前解决方法是将 Routine Load 从 BE 节点拿出来放到资源调度上，用 Doris-Spark/Flink-Connector 替换 Routine Load。当时 Doris-spark-Connector 还没有实时写入的功能，我们根据业务需求进行了优化，并将方案贡献给社区。</p></li><li><p>通过攒批来控制实时写入频率：当实时写入频率较高时，小文件堆积过多、查询 IO 升高，小文件排序归并的过程将导致查询时间加长，进而出现查询抖动的情况。当前的解决办法是控制导入频次，调整 Compaction 的合并线程、间隔时间等参数，避免 Tablet 下小文件的堆积。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="查询">查询：<a href="#查询" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><ol><li><p>增加 SQL 黑名单，控制异常大查询。个别用户在查询时没有加 where 条件，或者查询时选择的时间范围较长，这种情况下 BE 节点的 SQL 会把磁盘的负载和 CPU 拉高，导致其他节点的 SQL 查询变慢，甚至出现 BE 节点宕机的情况。目前的解决方案是使用 SQL 黑名单禁止全表及大量分区实时表的查询。</p></li><li><p>使用 SQL Cache 和 SQL Proxy 实现高并发访问。同时使用 SQL Cache 和 SQL Proxy 的原因在于，SQL Cache的颗粒度到表的分区，如果数据发生变更， SQL Cache 将失效，因此 SQL Cache 缓存适合数据更新频次较低的场景（离线场景、历史分区等）。对于数据需要持续写到最新分区的场景， SQL Cache 则是不适用的。当 SQL Cache 失效时 Query 将全部发送到 Doris 造成重复的 Runtime 计算，而 SQL Proxy 可以设置一秒左右的缓存，可以避免相同条件的重复计算，有效提高集群的并发。</p></li></ol><h3 class="anchor anchorWithStickyNavbar_LWe7" id="存储">存储：<a href="#存储" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>使用 SSD 和 HDD 做热温数据存储周期的分离，近一年以内的数据存在 SSD，超过一年的数据存在 HDD。Apache Doris 支持对分区设置冷却时间，但只支持创建表分区时设置冷却的时间，目前的解决方案是设置自动同步逻辑，把历史的一些数据从 SSD 迁移到 HDD，确保 1年内的数据都放在 SSD 上。</p><h3 class="anchor anchorWithStickyNavbar_LWe7" id="升级">升级：<a href="#升级" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h3><p>升级前一定要备份元数据，也可以使用新开集群的方式，通过 Broker 将数据文件备份到 S3 或 HDFS 等远端存储系统中，再通过备份恢复的方式将旧集群数据导入到新集群中。</p><h2 class="anchor anchorWithStickyNavbar_LWe7" id="升级前后性能对比"><strong>升级前后性能对比</strong><a href="#升级前后性能对比" class="hash-link" aria-label="标题的直接链接" title="标题的直接链接">​</a></h2><p><img loading="lazy" alt="doris_1_1_performance_test_git" src="https://cdnd.selectdb.com/zh-CN/assets/images/doris_1_1_performance_test_git-ad375d6872f12ab1e3cca76d30caa1f6.png" width="1961" height="1126" class="img_ev3q"></p><p>思必驰最早是从 0.12 版本开始使用 Apache Doris 的，在今年我们也完成了从 0.15 版本到最新 1.1 版本的升级操作，并进行了基于真实业务场景和数据的性能测试。</p><p>从以上测试报告中可以看到，总共 13 个测试 SQL 中，前 3 个 SQL 升级前后性能差异不明显，因为这 3 个场景主要是简单的聚合函数，对 Apache Doris 性能要求不高，0.15 版本即可满足需求。而在 Q4 之后的场景中 ，SQL 较为复杂，Group By 有多个字段、多个字段聚合函数以及复杂函数，因此升级新版本后带来的性能提升非常明显，平均查询性能较 0.15 版本提升 2-3 倍。由此，非常推荐大家去升级到 Apache Doris 最新版本。</p><h1>总结和收益</h1><ol><li><p>Apache Doris 支持构建离线+实时统一数仓，一个 ETL 脚本即可支持实时和离线数仓，大大缩短开发周期，降低存储成本，避免了离线和实时指标不一致等问题。</p></li><li><p>Apache Doris 1.1.x 版本开始全面支持向量化计算，较之前版本查询性能提升 2-3 倍。经测试，Apache Doris 1.1.x 版本在宽表场景的查询性能已基本与 ClickHouse 持平。</p></li><li><p>功能强大，不依赖其他组件。相比 Apache Kylin、Apache Druid、ClickHouse 等，Apache Doris 不需要引入第 2 个组件填补技术空档。Apache Doris 支持聚合计算、明细查询、关联查询，当前思必驰超 90% 的分析需求已移步 Apache Doris实现。 得益于此优势，技术人员需要运维的组件减少，极大降低运维成本。</p></li><li><p>易用性极高，支持 MySQL 协议和标准 SQL，大幅降低用户学习成本。</p></li></ol><h1>未来计划</h1><ol><li><p>Tablet 小文件过多的问题。Tablet 是 Apache Doris 中读写数据最小的逻辑单元，当 Tablet 小文件比较多时会产生 2 个问题，一是 Tablet 小文件增多会导致元数据内存压力变大。二是对查询性能的影响，即使是几百兆的查询，但在小文件有几十万、上百万的情况下，一个小小的查询也会导致 IO 非常高。未来，我们将做一个 Tablet 文件数量/大小比值的监控，当比值在不合理范围内时及时进行表设计的修改，使得文件数量和大小的比值在合理的范围内。</p></li><li><p>支持基于 Bitmap 的字符串精确去重。业务中精确去重的场景较多，特别是基于字符串的 UV 场景，目前 Apache Doris 使用的是 Distinct 函数来实现的。未来我们会尝试的在 Apache Doris 中创建字典，基于字典去构建字符串的 Bitmap 索引。</p></li><li><p>Doris-Spark-Connector 流式写入支持分块传输。Doris-Spark-Connector 底层是复用的 Stream Load，工作机制是攒批，容易出现两个问题，一是攒批可能会会出现内存压力导致 OOM，二是当Doris-Spark-Connector 攒批时，Spark Checkpoint 没有提交，但 Buffer 已满并提交给 Doris，此时 Apacche Doris 中已经有数据，但由于没有提交 Checkpoint，假如此时任务恰巧失败，启动后又会重新消费写入一遍。未来我们将优化此问题，实现 Doris-Spark-Connector 流式写入支持分块传输。</p></li></ol></div></article><nav class="pagination-nav" aria-label="博文列表分页导航"></nav></main></div></div></div></div><div class="footer"><div class="container"><div class="footer-box"><div class="left"><img src="/zh-CN/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--light_HNdA footer__logo"><img src="/zh-CN/images/asf_logo_apache.svg" alt="" class="themedImage_ToTc themedImage--dark_i4oU footer__logo"><div class="row footer__links"><div class="col footer__col"><div class="footer__title">资源</div><ul class="footer__items clean-list"><li class="footer__item"><a class="footer__link-item" href="/zh-CN/download">下载</a></li><li class="footer__item"><a class="footer__link-item" href="/zh-CN/learning">文档</a></li></ul></div><div class="col footer__col"><div class="footer__title">ASF</div><ul class="footer__items clean-list"><li class="footer__item"><a href="https://www.apache.org/" target="_blank" rel="noopener noreferrer" class="footer__link-item">基金会<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/licenses/" target="_blank" rel="noopener noreferrer" class="footer__link-item">版权<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/events/current-event" target="_blank" rel="noopener noreferrer" class="footer__link-item">活动<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/sponsorship.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">捐赠<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://privacy.apache.org/policies/privacy-policy-public.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">隐私<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li><li class="footer__item"><a href="https://www.apache.org/foundation/thanks.html" target="_blank" rel="noopener noreferrer" class="footer__link-item">鸣谢<svg width="13.5" height="13.5" aria-hidden="true" viewBox="0 0 24 24" class="iconExternalLink_nPIU"><path fill="currentColor" d="M21 13v10h-21v-19h12v2h-10v15h17v-8h2zm3-12h-10.988l4.035 4-6.977 7.07 2.828 2.828 6.977-7.07 4.125 4.172v-11z"></path></svg></a></li></ul></div><div class="col footer__col"><div class="footer__title">语言</div><ul class="footer__items clean-list"><li class="footer__item"><a href="/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">English</a></li><li class="footer__item"><a href="/zh-CN/blog/tags/最佳实践" target="_self" rel="noopener noreferrer" class="navbar__item navbar__link footer__link-item">简体中文</a></li></ul></div></div></div><div class="right"><div class="footer__title">分享</div><div class="social-list"><div class="social"><a href="mailto:dev@doris.apache.org" title="mail" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M5.6003 6H26.3997C27.8186 6 28.982 7.10964 29 8.46946L16.0045 15.454L3.01202 8.47829C3.02405 7.11258 4.1784 6 5.6003 6ZM3.01202 11.1508L3 23.5011C3 24.8756 4.16938 26 5.6003 26H26.3997C27.8306 26 29 24.8756 29 23.5011V11.145L16.3111 17.8028C16.1157 17.9058 15.8813 17.9058 15.6889 17.8028L3.01202 11.1508V11.1508Z" fill="currentColor"></path></svg></a><a href="https://github.com/apache/doris" title="github" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M16.0001 2.66675C8.63342 2.66675 2.66675 8.63341 2.66675 16.0001C2.66524 18.7991 3.54517 21.5276 5.1817 23.7983C6.81824 26.0691 9.12828 27.7668 11.7841 28.6508C12.4508 28.7668 12.7001 28.3668 12.7001 28.0161C12.7001 27.7001 12.6828 26.6508 12.6828 25.5334C9.33342 26.1508 8.46675 24.7174 8.20008 23.9668C8.04942 23.5828 7.40008 22.4001 6.83342 22.0828C6.36675 21.8334 5.70008 21.2161 6.81608 21.2001C7.86675 21.1828 8.61608 22.1668 8.86675 22.5668C10.0668 24.5828 11.9841 24.0161 12.7494 23.6668C12.8668 22.8001 13.2161 22.2174 13.6001 21.8841C10.6334 21.5508 7.53342 20.4001 7.53342 15.3001C7.53342 13.8494 8.04942 12.6507 8.90008 11.7161C8.76675 11.3827 8.30008 10.0161 9.03342 8.18275C9.03342 8.18275 10.1494 7.83342 12.7001 9.55075C13.7855 9.2495 14.907 9.09787 16.0334 9.10008C17.1668 9.10008 18.3001 9.24942 19.3668 9.54942C21.9161 7.81608 23.0334 8.18408 23.0334 8.18408C23.7668 10.0174 23.3001 11.3841 23.1668 11.7174C24.0161 12.6507 24.5334 13.8334 24.5334 15.3001C24.5334 20.4174 21.4174 21.5508 18.4508 21.8841C18.9334 22.3001 19.3508 23.1001 19.3508 24.3508C19.3508 26.1334 19.3334 27.5668 19.3334 28.0174C19.3334 28.3668 19.5841 28.7828 20.2508 28.6494C22.8975 27.7558 25.1973 26.0547 26.8266 23.7856C28.4559 21.5165 29.3327 18.7936 29.3334 16.0001C29.3334 8.63341 23.3668 2.66675 16.0001 2.66675V2.66675Z" fill="currentColor"></path></svg></a><a href="https://twitter.com/doris_apache" title="twitter" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M27.1493 10.8313C27.1493 10.5687 27.1442 10.3091 27.1326 10.0512C28.2554 9.21471 29.2295 8.1676 30 6.96938C28.9537 7.44012 27.8403 7.74469 26.7 7.8721C27.8868 7.14188 28.7969 5.97403 29.227 4.57256C28.1158 5.24638 26.8867 5.72811 25.5798 5.97912C24.5326 4.78777 23.0383 4.02895 21.3864 4.00076C18.2137 3.94831 15.6418 6.62904 15.6418 9.98754C15.6418 10.4649 15.6918 10.9279 15.7909 11.3749C11.0133 11.0675 6.77972 8.58103 3.9478 4.8326C3.45367 5.73067 3.17017 6.78025 3.17017 7.90503C3.17017 10.0324 4.18438 11.9232 5.72536 13.039C4.78198 12.9963 3.89827 12.7106 3.12316 12.2422V12.3204C3.12316 15.2937 5.10395 17.7849 7.73223 18.3661C7.2504 18.5032 6.74218 18.5745 6.2195 18.5719C5.85634 18.57 5.49437 18.5304 5.13941 18.4536C5.86973 20.8902 7.99227 22.6703 10.5051 22.7297C8.53846 24.3601 6.06106 25.334 3.37133 25.3272C2.90757 25.3272 2.44929 25.2961 2 25.2397C4.54329 26.9841 7.56224 28 10.8076 28C21.3719 28.0025 27.1493 18.8084 27.1493 10.8313V10.8313Z" fill="currentColor"></path></svg></a></div><div class="social"><a href="https://join.slack.com/t/apachedoriscommunity/shared_invite/zt-1x7x8fger-F7NoshFQn~djlvGdnEtxUQ" title="slack" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><g clip-path="url(#clip0_125_278)"><path d="M12.5875 16.6906C11.0844 16.6906 9.86562 17.9094 9.86562 19.4125V26.2375C9.86562 26.9594 10.1524 27.6517 10.6628 28.1622C11.1733 28.6726 11.8656 28.9594 12.5875 28.9594C13.3094 28.9594 14.0017 28.6726 14.5122 28.1622C15.0226 27.6517 15.3094 26.9594 15.3094 26.2375V19.4531C15.3094 17.9094 14.0906 16.6906 12.5875 16.6906ZM3 19.4531C3 20.175 3.28677 20.8673 3.79722 21.3778C4.30767 21.8882 4.99999 22.175 5.72187 22.175C6.44376 22.175 7.13608 21.8882 7.64653 21.3778C8.15698 20.8673 8.44375 20.175 8.44375 19.4531V16.7312H5.7625C4.25938 16.6906 3 17.9094 3 19.4531ZM12.5875 3C11.8656 3 11.1733 3.28677 10.6628 3.79722C10.1524 4.30767 9.86562 4.99999 9.86562 5.72187C9.86562 6.44376 10.1524 7.13608 10.6628 7.64653C11.1733 8.15698 11.8656 8.44375 12.5875 8.44375H15.3094V5.72187C15.3094 4.21875 14.0906 3 12.5875 3ZM5.72187 15.3094H12.5469C13.2688 15.3094 13.9611 15.0226 14.4715 14.5122C14.982 14.0017 15.2688 13.3094 15.2688 12.5875C15.2688 11.8656 14.982 11.1733 14.4715 10.6628C13.9611 10.1524 13.2688 9.86562 12.5469 9.86562H5.72187C4.99999 9.86562 4.30767 10.1524 3.79722 10.6628C3.28677 11.1733 3 11.8656 3 12.5875C3 13.3094 3.28677 14.0017 3.79722 14.5122C4.30767 15.0226 4.99999 15.3094 5.72187 15.3094ZM26.2375 9.86562C24.7344 9.86562 23.5156 11.0844 23.5156 12.5875V15.3094H26.2375C26.9594 15.3094 27.6517 15.0226 28.1622 14.5122C28.6726 14.0017 28.9594 13.3094 28.9594 12.5875C28.9594 11.8656 28.6726 11.1733 28.1622 10.6628C27.6517 10.1524 26.9594 9.86562 26.2375 9.86562ZM16.6906 5.72187V12.5875C16.6906 13.3094 16.9774 14.0017 17.4878 14.5122C17.9983 15.0226 18.6906 15.3094 19.4125 15.3094C20.1344 15.3094 20.8267 15.0226 21.3372 14.5122C21.8476 14.0017 22.1344 13.3094 22.1344 12.5875V5.72187C22.1344 4.99999 21.8476 4.30767 21.3372 3.79722C20.8267 3.28677 20.1344 3 19.4125 3C18.6906 3 17.9983 3.28677 17.4878 3.79722C16.9774 4.30767 16.6906 4.99999 16.6906 5.72187ZM22.1344 26.2781C22.1344 24.775 20.9156 23.5562 19.4125 23.5562H16.6906V26.2781C16.6906 27 16.9774 27.6923 17.4878 28.2028C17.9983 28.7132 18.6906 29 19.4125 29C20.1344 29 20.8267 28.7132 21.3372 28.2028C21.8476 27.6923 22.1344 27 22.1344 26.2781ZM26.2781 16.6906H19.4125C18.6906 16.6906 17.9983 16.9774 17.4878 17.4878C16.9774 17.9983 16.6906 18.6906 16.6906 19.4125C16.6906 20.1344 16.9774 20.8267 17.4878 21.3372C17.9983 21.8476 18.6906 22.1344 19.4125 22.1344H26.2375C27.7406 22.1344 28.9594 20.9156 28.9594 19.4125C29 17.9094 27.7812 16.6906 26.2781 16.6906Z" fill="currentColor"></path></g><defs><clipPath id="clip0_125_278"><rect width="26" height="26" fill="currentColor" transform="translate(3 3)"></rect></clipPath></defs></svg></a><a href="https://space.bilibili.com/362350065" title="bilibili" class="item"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M23.0533 11.5412H9.14591C8.72353 11.5412 8.36784 11.8633 8.36784 12.296V21.5162C8.36784 21.9489 8.72349 22.2665 9.14591 22.2665H23.0533C23.4757 22.2665 23.7928 21.949 23.7928 21.5162V12.296C23.7928 11.8633 23.4757 11.5412 23.0533 11.5412ZM10.0928 14.9479L14.0122 14.1975L14.3083 15.6685L10.4284 16.4188L10.0928 14.9479ZM16.1348 19.4301C14.9303 20.7431 13.6667 19.0155 13.6667 19.0155L14.3084 18.6008C14.3084 18.6008 15.1673 20.1508 16.125 18.0973C17.0432 20.0916 18.06 18.6206 18.06 18.6304L18.6426 19.0057C18.6426 19.0057 17.5565 20.7431 16.1348 19.4301ZM21.9498 16.4189L18.06 15.6686L18.3661 14.1976L22.2756 14.948L21.9498 16.4189Z" fill="currentColor"></path><path d="M16 2C8.26801 2 2 8.26805 2 16C2 23.7319 8.26806 30 16 30C23.7319 30 30 23.7319 30 16C30 8.26801 23.732 2 16 2ZM23.3727 24.1329C22.3941 24.1019 22.0644 24.1329 22.0644 24.1329C22.0644 24.1329 21.9923 25.2558 21.0343 25.2764C20.0659 25.2867 19.9216 24.4934 19.8907 24.1947C19.3035 24.1947 12.2467 24.2255 12.2467 24.2255C12.2467 24.2255 12.1231 25.266 11.165 25.266C10.1967 25.266 10.1451 24.4006 10.0833 24.2255C9.45486 24.2255 8.6101 24.2049 8.6101 24.2049C8.6101 24.2049 6.48791 23.7621 6.20978 21.0012C6.24067 18.2402 6.20978 12.7801 6.20978 12.7801C6.20978 12.7801 6.01404 10.2356 8.54836 9.50415C9.33118 9.4733 11.0208 9.46293 12.9781 9.46293L11.1753 7.71159C11.1753 7.71159 10.8971 7.36131 11.371 6.96986C11.8553 6.57846 11.8757 6.73797 12.0406 6.85128C12.2055 6.96456 14.7295 9.45229 14.7295 9.45229H14.3895C15.3579 9.45229 16.3572 9.46799 17.3152 9.46799C17.686 9.09711 19.798 7.02903 19.9422 6.92612C20.107 6.82309 20.1378 6.64927 20.6118 7.04068C21.0857 7.43213 20.8075 7.78309 20.8075 7.78309L19.0459 9.48322C21.4668 9.50386 23.3315 9.51423 23.3315 9.51423C23.3315 9.51423 25.7214 10.0398 25.7833 12.78C25.7524 15.5203 25.7936 21.0319 25.7936 21.0319C25.7936 21.0319 25.6598 23.7104 23.3727 24.1329Z" fill="currentColor"></path></svg></a><a class="item wechat"><svg width="2em" height="2em" viewBox="0 0 32 32" fill="none" xmlns="http://www.w3.org/2000/svg"><path d="M20.7578 11.5169C21.0708 11.5169 21.3795 11.5398 21.6851 11.573C20.8524 7.73517 16.7052 4.88306 11.9718 4.88306C6.67951 4.88306 2.34412 8.45283 2.34412 12.9854C2.34412 15.6013 3.78679 17.7498 6.19667 19.4161L5.2339 22.2827L8.59917 20.6122C9.80411 20.8478 10.7698 21.0906 11.9718 21.0906C12.2738 21.0906 12.5728 21.0759 12.8703 21.0523C12.682 20.4159 12.5728 19.7485 12.5728 19.0566C12.5728 14.8947 16.1847 11.5169 20.7578 11.5169ZM15.5822 8.9335C16.3072 8.9335 16.7871 9.40601 16.7871 10.1229C16.7871 10.8369 16.3072 11.3153 15.5822 11.3153C14.8601 11.3153 14.1365 10.8369 14.1365 10.1229C14.1365 9.40601 14.8601 8.9335 15.5822 8.9335ZM8.84429 11.3153C8.12218 11.3153 7.3942 10.8368 7.3942 10.1229C7.3942 9.40597 8.12218 8.93346 8.84429 8.93346C9.56559 8.93346 10.0463 9.40597 10.0463 10.1229C10.0463 10.8369 9.56559 11.3153 8.84429 11.3153ZM29.5453 18.9422C29.5453 15.1332 25.6935 12.0285 21.3677 12.0285C16.7871 12.0285 13.1797 15.1332 13.1797 18.9422C13.1797 22.7567 16.7871 25.8547 21.3677 25.8547C22.326 25.8547 23.2932 25.6169 24.2559 25.3777L26.897 26.8086L26.1726 24.4282C28.1056 22.993 29.5453 21.0906 29.5453 18.9422ZM18.7126 17.7498C18.2335 17.7498 17.7499 17.278 17.7499 16.7966C17.7499 16.3219 18.2335 15.8442 18.7126 15.8442C19.4406 15.8442 19.9176 16.3219 19.9176 16.7966C19.9176 17.278 19.4406 17.7498 18.7126 17.7498ZM24.0079 17.7498C23.5324 17.7498 23.0518 17.278 23.0518 16.7966C23.0518 16.3219 23.5324 15.8442 24.0079 15.8442C24.73 15.8442 25.2128 16.3219 25.2128 16.7966C25.2128 17.278 24.73 17.7498 24.0079 17.7498Z" fill="currentColor"></path></svg><div class="wechat-dropdown"><img src="https://cdnd.selectdb.com/zh-CN/assets/images/wechat-31a2eb3bbefef7171acfae2906dc777c.png" alt=""></div></a></div></div></div></div><div class="footer__copyright">Copyright © 2022 The Apache Software Foundation,Licensed under the <a href="https://www.apache.org/licenses/LICENSE-2.0" target="_blank">Apache License, Version 2.0</a>. Apache, Doris, Apache Doris, the Apache feather logo and the Apache Doris logo are trademarks of The Apache Software Foundation.</div></div></div></div>
<script src="https://cdnd.selectdb.com/zh-CN/assets/js/runtime~main.f82a6f6f.js"></script>
<script src="https://cdnd.selectdb.com/zh-CN/assets/js/main.5ce93bb9.js"></script>
</body>
</html>