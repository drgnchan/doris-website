"use strict";(self.webpackChunkdoris_website=self.webpackChunkdoris_website||[]).push([[66946],{3905:(e,t,a)=>{a.d(t,{Zo:()=>d,kt:()=>y});var n=a(67294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function l(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var o=n.createContext({}),c=function(e){var t=n.useContext(o),a=t;return e&&(a="function"==typeof e?e(t):l(l({},t),e)),a},d=function(e){var t=c(e.components);return n.createElement(o.Provider,{value:t},e.children)},m="mdxType",u={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},p=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,o=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),m=c(a),p=r,y=m["".concat(o,".").concat(p)]||m[p]||u[p]||i;return a?n.createElement(y,l(l({ref:t},d),{},{components:a})):n.createElement(y,l({ref:t},d))}));function y(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,l=new Array(i);l[0]=p;var s={};for(var o in t)hasOwnProperty.call(t,o)&&(s[o]=t[o]);s.originalType=e,s[m]="string"==typeof e?e:r,l[1]=s;for(var c=2;c<i;c++)l[c]=a[c];return n.createElement.apply(null,l)}return n.createElement.apply(null,a)}p.displayName="MDXCreateElement"},12791:(e,t,a)=>{a.r(t),a.d(t,{assets:()=>o,contentTitle:()=>l,default:()=>u,frontMatter:()=>i,metadata:()=>s,toc:()=>c});var n=a(87462),r=(a(67294),a(3905));const i={title:"[Experimental] Dynamie schema table",language:"en"},l=void 0,s={unversionedId:"data-table/dynamic-schema-table",id:"version-1.2/data-table/dynamic-schema-table",title:"[Experimental] Dynamie schema table",description:"\x3c!--",source:"@site/versioned_docs/version-1.2/data-table/dynamic-schema-table.md",sourceDirName:"data-table",slug:"/data-table/dynamic-schema-table",permalink:"/docs/1.2/data-table/dynamic-schema-table",draft:!1,tags:[],version:"1.2",frontMatter:{title:"[Experimental] Dynamie schema table",language:"en"},sidebar:"docs",previous:{title:"Best Practices",permalink:"/docs/1.2/data-table/best-practice"},next:{title:"Index Overview",permalink:"/docs/1.2/data-table/index/prefix-index"}},o={},c=[{value:"Terminology",id:"terminology",level:2},{value:"Create dynamic table",id:"create-dynamic-table",level:2},{value:"Importing data",id:"importing-data",level:2},{value:"Adding Index to Dynamic Columns",id:"adding-index-to-dynamic-columns",level:2},{value:"Type conflict resolution",id:"type-conflict-resolution",level:2}],d={toc:c},m="wrapper";function u(e){let{components:t,...a}=e;return(0,r.kt)(m,(0,n.Z)({},d,a,{components:t,mdxType:"MDXLayout"}),(0,r.kt)("h1",{id:"dynamic-table"},"Dynamic Table"),(0,r.kt)("version",{since:"2.0.0"},(0,r.kt)("p",null,"Dynamic Table")),(0,r.kt)("p",null,"A dynamic schema table is a special kind of table which schema expands automatically with the import procedure. Currently, this feature is mainly used for importing semi-structured data such as JSON. Because JSON is self-describing, we can extract the schema information from the original document and infer the final type information. This special table can reduce manual schema change operations and easily import semi-structured data and automatically expand its schema."),(0,r.kt)("h2",{id:"terminology"},"Terminology"),(0,r.kt)("ul",null,(0,r.kt)("li",{parentName:"ul"},"Schema change, changing the structure of the table, such as adding columns, reducing columns, changing column types"),(0,r.kt)("li",{parentName:"ul"},"Static column, column specified during table creation, such as partition columns, primary key columns"),(0,r.kt)("li",{parentName:"ul"},"Dynamic column, columns automatically recognized and added during import")),(0,r.kt)("h2",{id:"create-dynamic-table"},"Create dynamic table"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},'CREATE DATABASE test_dynamic_table;\n\n-- Create table and specify static column types, import will automatically convert to the type of static column\n-- Choose random bucketing\nCREATE TABLE IF NOT EXISTS test_dynamic_table (\n                qid bigint,\n                `answers.date` array<datetime>,\n                `title` string,\n                ...   -- ... Identifying a table as a dynamic table and its syntax for dynamic tables.\n        )\nDUPLICATE KEY(`qid`)\nDISTRIBUTED BY RANDOM BUCKETS 5 \nproperties("replication_num" = "1");\n\n-- Three Columns are added to the table by default, and their types are specified\nmysql> DESC test_dynamic_table;\n+--------------+-----------------+------+-------+---------+-------+\n| Field        | Type            | Null | Key   | Default | Extra |\n+--------------+-----------------+------+-------+---------+-------+\n| qid          | BIGINT          | Yes  | true  | NULL    |       |\n| answers.date | ARRAY<DATETIME> | Yes  | false | NULL    | NONE  |\n| user         | TEXT            | Yes  | false | NULL    | NONE  |\n+--------------+-----------------+------+-------+---------+-------+\n3 rows in set (0.00 sec)\n')),(0,r.kt)("h2",{id:"importing-data"},"Importing data"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},'-- example1.json\n\'{\n    "title": "Display Progress Bar at the Time of Processing",\n    "qid": "1000000",\n    "answers": [\n        {"date": "2009-06-16T09:55:57.320", "user": "Micha\\u0142 Niklas (22595)"},\n        {"date": "2009-06-17T12:34:22.643", "user": "Jack Njiri (77153)"}\n    ],\n    "tag": ["vb6", "progress-bar"],\n    "user": "Jash",\n    "creationdate": "2009-06-16T07:28:42.770"\n}\'\n\ncurl -X PUT -T example1.json --location-trusted -u root: -H "read_json_by_line:false" -H "format:json"   http://127.0.0.1:8147/api/regression_test_dynamic_table/test_dynamic_table/_stream_load\n\n-- Added five new columns: `title`, `answers.user`, `tag`, `title`, `creationdate`\n-- The types of the three columns: `qid`, `answers.date`, `user` remain the same as with the table was created\n-- The default value of the new array type is an empty array []\nmysql> DESC test_dynamic_table;                                                                                 \n+--------------+-----------------+------+-------+---------+-------+\n| Field        | Type            | Null | Key   | Default | Extra |\n+--------------+-----------------+------+-------+---------+-------+\n| qid          | BIGINT          | Yes  | true  | NULL    |       |\n| answers.date | ARRAY<DATETIME> | Yes  | false | NULL    | NONE  |\n| title        | TEXT            | Yes  | false | NULL    | NONE  |\n| answers.user | ARRAY<TEXT>     | No   | false | []      | NONE  |\n| tag          | ARRAY<TEXT>     | No   | false | []      | NONE  |\n| user         | TEXT            | Yes  | false | NULL    | NONE  |\n| creationdate | TEXT            | Yes  | false | NULL    | NONE  |\n| date         | TEXT            | Yes  | false | NULL    | NONE  |\n+--------------+-----------------+------+-------+---------+-------+\n\n-- Batch import data\n-- Specifying -H "read_json_by_line:true", parsing JSON line by line\ncurl -X PUT -T example_batch.json --location-trusted -u root: -H "read_json_by_line:true" -H "format:json"   http://127.0.0.1:8147/api/regression_test_dynamic_table/test_dynamic_table/_stream_load\n\n-- Specifying -H "strip_outer_array:true", parsing the entire file as a JSON array, each element in the array is the same, more efficient parsing way\ncurl -X PUT -T example_batch_array.json --location-trusted -u root: -H "strip_outer_array:true" -H "format:json"   http://127.0.0.1:8147/api/regression_test_dynamic_table/test_dynamic_table/_stream_load\n')),(0,r.kt)("p",null,"For a dynamic table, you can also use S3load or Routine load, with similar usage."),(0,r.kt)("h2",{id:"adding-index-to-dynamic-columns"},"Adding Index to Dynamic Columns"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre",className:"language-sql"},'-- Create an inverted index on the title column, using English parsing.\nCREATE INDEX title_idx ON test_dynamic_table (`title`) using inverted PROPERTIES("parser"="english")\n')),(0,r.kt)("h2",{id:"type-conflict-resolution"},"Type conflict resolution"),(0,r.kt)("p",null,"In the first batch import, the unified type will be automatically inferred and used as the final Column type, so it is recommended to keep the Column type consistent, for example:"),(0,r.kt)("pre",null,(0,r.kt)("code",{parentName:"pre"},'{"id" : 123}\n{"id" : "123"}\n-- The type will finally be inferred as Text type, and if {"id" : 123} is imported later, the type will automatically be converted to String type\n\nFor types that cannot be unified, such as:\n{"id" : [123]}\n{"id" : 123}\n-- Importing will result in an error."\n')))}u.isMDXComponent=!0}}]);